#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•¸æ“šä¸‹è¼‰é é¢
===========

æä¾›è‚¡ç¥¨æ•¸æ“šä¸‹è¼‰åŠŸèƒ½ï¼Œæ”¯æ´æ—¥æœŸç¯„åœé¸æ“‡ã€æ¸¬è©¦æ¨¡å¼å’Œé€²åº¦é¡¯ç¤ºã€‚
"""

import streamlit as st
import pandas as pd
from datetime import datetime, date, timedelta
import logging
import asyncio
import threading
from typing import List, Optional
import time

# è¨­å®šæ—¥èªŒ
logger = logging.getLogger(__name__)


def _format_duration(duration):
    """æ ¼å¼åŒ–æ™‚é–“é–“éš”é¡¯ç¤º"""
    if isinstance(duration, timedelta):
        total_seconds = int(duration.total_seconds())
    else:
        total_seconds = int(duration)

    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60

    if hours > 0:
        return f"{hours}å°æ™‚{minutes}åˆ†{seconds}ç§’"
    elif minutes > 0:
        return f"{minutes}åˆ†{seconds}ç§’"
    else:
        return f"{seconds}ç§’"


def show_data_download_page():
    """é¡¯ç¤ºæ•¸æ“šä¸‹è¼‰é é¢"""
    st.title("ğŸ“¥ è‚¡ç¥¨æ•¸æ“šä¸‹è¼‰")
    
    # å‰µå»ºå…©æ¬„å¸ƒå±€
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“… ä¸‹è¼‰è¨­å®š")
        
        # æ—¥æœŸç¯„åœé¸æ“‡å™¨
        start_date = st.date_input(
            "é–‹å§‹æ—¥æœŸ",
            value=date(2021, 1, 1),
            min_value=date(2020, 1, 1),
            max_value=date.today(),
            help="é¸æ“‡æ•¸æ“šä¸‹è¼‰çš„é–‹å§‹æ—¥æœŸ"
        )
        
        end_date = st.date_input(
            "çµæŸæ—¥æœŸ", 
            value=date.today(),
            min_value=start_date,
            max_value=date.today(),
            help="é¸æ“‡æ•¸æ“šä¸‹è¼‰çš„çµæŸæ—¥æœŸ"
        )
        
        # æ¸¬è©¦æ¨¡å¼é¸é …
        test_mode = st.checkbox(
            "ğŸ§ª æ¸¬è©¦æ¨¡å¼",
            value=False,
            help="å•Ÿç”¨æ¸¬è©¦æ¨¡å¼å°‡åƒ…ä¸‹è¼‰å‰10æª”è‚¡ç¥¨ï¼Œç”¨æ–¼å¿«é€Ÿæ¸¬è©¦"
        )
        
        # æ•¸æ“šæºé¸æ“‡
        data_source = st.selectbox(
            "æ•¸æ“šæº",
            options=["AUTO", "TWSE", "TPEX", "YAHOO"],
            index=0,
            help="é¸æ“‡æ•¸æ“šä¾†æºï¼ŒAUTO æœƒè‡ªå‹•é¸æ“‡æœ€ä½³ä¾†æº"
        )
        
    with col2:
        st.subheader("ğŸ“‹ ä¸‹è¼‰èªªæ˜")
        st.info("""
        **ä¸‹è¼‰åŠŸèƒ½èªªæ˜ï¼š**
        - æ¸¬è©¦æ¨¡å¼ï¼šä¸‹è¼‰å‰10æª”è‚¡ç¥¨æ•¸æ“š
        - å®Œæ•´æ¨¡å¼ï¼šä¸‹è¼‰æ‰€æœ‰è‚¡ç¥¨æ•¸æ“š
        - æ”¯æ´å¤šæ•¸æ“šæºï¼šTWSEã€TPEXã€Yahoo Finance
        - è‡ªå‹•è™•ç†æ•¸æ“šé©—è­‰å’Œå­˜å„²
        """)

        st.warning("""
        **æ³¨æ„äº‹é …ï¼š**
        - ä¸‹è¼‰éç¨‹å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“
        - è«‹ç¢ºä¿ç¶²è·¯é€£æ¥ç©©å®š
        - å»ºè­°å…ˆä½¿ç”¨æ¸¬è©¦æ¨¡å¼é©—è­‰åŠŸèƒ½
        """)
    
    st.markdown("---")
    
    # ä¸‹è¼‰æ§åˆ¶å€åŸŸ
    col1, col2, col3 = st.columns([1, 1, 1])
    
    with col1:
        if st.button("ğŸš€ é–‹å§‹ä¸‹è¼‰", type="primary", use_container_width=True):
            if start_date >= end_date:
                st.error("âŒ é–‹å§‹æ—¥æœŸå¿…é ˆæ—©æ–¼çµæŸæ—¥æœŸ")
            else:
                start_download(start_date, end_date, test_mode, data_source)
    
    with col2:
        if st.button("â¹ï¸ åœæ­¢ä¸‹è¼‰", use_container_width=True):
            stop_download()
    
    with col3:
        if st.button("ğŸ“‹ æŸ¥çœ‹èªªæ˜", use_container_width=True):
            st.info("è«‹åƒè€ƒå³å´çš„ä¸‹è¼‰èªªæ˜é€²è¡Œæ“ä½œ")


def start_download(start_date: date, end_date: date, test_mode: bool, data_source: str):
    """é–‹å§‹æ•¸æ“šä¸‹è¼‰"""
    try:
        # å°å…¥å¿…è¦çš„æ¨¡çµ„
        from src.data_sources.enhanced_real_data_crawler import EnhancedRealDataCrawler
        from src.core.real_data_integration import RealDataIntegrationService

        # åˆå§‹åŒ–ä¸‹è¼‰å™¨
        from src.data_sources.real_data_crawler import RealDataCrawler
        crawler = RealDataCrawler(db_path='sqlite:///data/enhanced_stock_database.db')
        integration_service = RealDataIntegrationService(db_path='sqlite:///data/enhanced_stock_database.db')

        # ç²å–è‚¡ç¥¨åˆ—è¡¨
        if test_mode:
            # æ¸¬è©¦æ¨¡å¼ï¼šåƒ…ä¸‹è¼‰å‰10æª”è‚¡ç¥¨
            stock_list = integration_service.stock_universe[:10]
            st.info(f"ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šå°‡ä¸‹è¼‰ {len(stock_list)} æª”è‚¡ç¥¨")
        else:
            # å®Œæ•´æ¨¡å¼ï¼šä¸‹è¼‰æ‰€æœ‰è‚¡ç¥¨
            stock_list = integration_service.stock_universe
            st.info(f"ğŸ“ˆ å®Œæ•´æ¨¡å¼ï¼šå°‡ä¸‹è¼‰ {len(stock_list)} æª”è‚¡ç¥¨")

        # åˆå§‹åŒ–ä¸‹è¼‰é€²åº¦è¿½è¹¤
        total_symbols = len(stock_list)
        completed_symbols = 0
        failed_symbols = 0
        total_records = 0
        start_time = datetime.now()

        # å‰µå»ºé€²åº¦å®¹å™¨
        progress_container = st.container()

        with progress_container:
            st.subheader("ğŸ“Š ä¸‹è¼‰é€²åº¦")

            # å‰µå»ºé€²åº¦é¡¯ç¤ºå…ƒä»¶
            progress_bar = st.progress(0)
            progress_text = st.empty()
            status_text = st.empty()
            time_info = st.empty()

            # æ™ºèƒ½æƒæéšæ®µ
            st.markdown("### ğŸ” æ•¸æ“šåº«æƒæéšæ®µ")
            scan_status = st.empty()
            scan_status.text("æ­£åœ¨æƒææ•¸æ“šåº«ä¸­çš„ç¾æœ‰æ•¸æ“š...")

            # åŸ·è¡Œæ™ºèƒ½æƒæ
            scan_result = crawler.scan_existing_data(stock_list, start_date, end_date)

            # é¡¯ç¤ºæƒæçµæœ
            scan_status.success("âœ… æ•¸æ“šåº«æƒæå®Œæˆ")

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("å®Œæ•´æ•¸æ“š", len(scan_result['complete_symbols']), "å¯è·³é")
            with col2:
                st.metric("éƒ¨åˆ†æ•¸æ“š", len(scan_result['partial_symbols']), "éœ€è£œå……")
            with col3:
                st.metric("ç¼ºå¤±æ•¸æ“š", len(scan_result['missing_symbols']), "éœ€çˆ¬å–")
            with col4:
                st.metric("é è¨ˆè·³éè¨˜éŒ„", scan_result['total_existing_records'], "å·²å­˜åœ¨")

            # ç¢ºå®šéœ€è¦çˆ¬å–çš„è‚¡ç¥¨
            symbols_to_crawl = scan_result['missing_symbols'] + scan_result['partial_symbols']

            if not symbols_to_crawl:
                st.success("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ•¸æ“šå·²å­˜åœ¨ï¼Œç„¡éœ€ä¸‹è¼‰ï¼")
                return

            st.info(f"ğŸ“‹ ç™¼ç¾ {len(scan_result['complete_symbols'])} æª”è‚¡ç¥¨å·²æœ‰æ•¸æ“šï¼Œå°‡çˆ¬å– {len(symbols_to_crawl)} æª”æ–°è‚¡ç¥¨")

            # æ›´æ–°ç¸½æ•¸ç‚ºå¯¦éš›éœ€è¦çˆ¬å–çš„æ•¸é‡
            total_symbols = len(symbols_to_crawl)
            stock_list = symbols_to_crawl

            # é–‹å§‹ä¸‹è¼‰
            for i, symbol in enumerate(stock_list):
                try:
                    current_progress = (i + 1) / total_symbols

                    # æ›´æ–°è©³ç´°é€²åº¦ä¿¡æ¯
                    progress_text.text(f"ç›®å‰é€²åº¦ï¼š{i + 1}/{total_symbols} æª”è‚¡ç¥¨ ({current_progress:.1%})")
                    status_text.text(f"æ­£åœ¨ä¸‹è¼‰ {symbol}...")

                    # è¨ˆç®—é ä¼°å‰©é¤˜æ™‚é–“
                    if i > 0:
                        elapsed_time = datetime.now() - start_time
                        avg_time_per_stock = elapsed_time.total_seconds() / i
                        remaining_stocks = total_symbols - i
                        estimated_remaining = timedelta(seconds=avg_time_per_stock * remaining_stocks)

                        time_info.text(f"â±ï¸ å·²ç”¨æ™‚: {_format_duration(elapsed_time)} | "
                                     f"é ä¼°å‰©é¤˜: {_format_duration(estimated_remaining)}")

                    # åŸ·è¡Œä¸‹è¼‰
                    download_start = datetime.now()
                    result = crawler.crawl_stock_data_range(
                        symbol=symbol,
                        start_date=start_date,
                        end_date=end_date
                    )
                    download_duration = datetime.now() - download_start

                    if result is not None and not result.empty:
                        # ä¿å­˜åˆ°è³‡æ–™åº«
                        crawler.save_to_database(result)
                        completed_symbols += 1
                        total_records += len(result)

                        # é¡¯ç¤ºæˆåŠŸä¿¡æ¯ï¼ˆé™åˆ¶é¡¯ç¤ºæ•¸é‡é¿å…ç•Œé¢éé•·ï¼‰
                        if i < 5 or i % 10 == 0:  # åªé¡¯ç¤ºå‰5å€‹å’Œæ¯10å€‹
                            st.success(f"âœ… {symbol}: ä¸‹è¼‰ {len(result)} ç­†è¨˜éŒ„ "
                                     f"(è€—æ™‚: {download_duration.total_seconds():.1f}ç§’)")
                    else:
                        failed_symbols += 1
                        if i < 5 or i % 10 == 0:  # åªé¡¯ç¤ºå‰5å€‹å’Œæ¯10å€‹
                            st.warning(f"âš ï¸ {symbol}: ç„¡æ•¸æ“šæˆ–ä¸‹è¼‰å¤±æ•—")

                    # æ›´æ–°é€²åº¦æ¢
                    progress_bar.progress(current_progress)

                    # çµ±è¨ˆä¿¡æ¯å·²ç§»é™¤ï¼Œä¿æŒç°¡æ½”çš„é€²åº¦é¡¯ç¤º

                    # çŸ­æš«å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
                    time.sleep(0.5)

                except Exception as e:
                    failed_symbols += 1
                    if i < 5 or i % 10 == 0:  # åªé¡¯ç¤ºå‰5å€‹å’Œæ¯10å€‹
                        st.error(f"âŒ {symbol}: ä¸‹è¼‰å¤±æ•— - {str(e)}")
                    logger.error(f"ä¸‹è¼‰ {symbol} å¤±æ•—: {e}")

            # å®Œæˆä¸‹è¼‰
            total_duration = datetime.now() - start_time
            progress_text.text(f"âœ… ä¸‹è¼‰å®Œæˆï¼è™•ç†äº† {total_symbols} æª”è‚¡ç¥¨")
            status_text.text("æ‰€æœ‰ä¸‹è¼‰ä»»å‹™å·²å®Œæˆ")
            time_info.text(f"ğŸ‰ ç¸½è€—æ™‚: {_format_duration(total_duration)}")

            # æœ€çµ‚çµ±è¨ˆ
            st.success(f"""
            ğŸ‰ ä¸‹è¼‰ä»»å‹™å®Œæˆï¼
            - ç¸½è‚¡ç¥¨æ•¸: {total_symbols}
            - æˆåŠŸä¸‹è¼‰: {completed_symbols} ({completed_symbols/total_symbols:.1%})
            - å¤±æ•—æ•¸é‡: {failed_symbols}
            - ç¸½è¨˜éŒ„æ•¸: {total_records:,}
            - ç¸½è€—æ™‚: {_format_duration(total_duration)}
            """)
            
    except ImportError as e:
        st.error(f"âŒ æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
        st.info("è«‹ç¢ºä¿ç›¸é—œçš„æ•¸æ“šä¸‹è¼‰æ¨¡çµ„å·²æ­£ç¢ºå®‰è£")
    except Exception as e:
        st.error(f"âŒ ä¸‹è¼‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        logger.error(f"æ•¸æ“šä¸‹è¼‰å¤±æ•—: {e}")


def stop_download():
    """åœæ­¢æ•¸æ“šä¸‹è¼‰"""
    st.warning("â¹ï¸ ä¸‹è¼‰å·²åœæ­¢")
    # TODO: å¯¦ç¾ä¸‹è¼‰åœæ­¢é‚è¼¯





if __name__ == "__main__":
    show_data_download_page()
