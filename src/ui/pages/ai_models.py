"""
AI æ¨¡å‹ç®¡ç†é é¢

æ­¤æ¨¡çµ„å¯¦ç¾ AI æ¨¡å‹ç®¡ç†çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹æ¸…å–®ç®¡ç†èˆ‡ç€è¦½
- æ¨¡å‹è¨“ç·´èˆ‡åƒæ•¸è¨­å®š
- æ¨¡å‹æ¨è«–èˆ‡çµæœåˆ†æ
- æ¨¡å‹è§£é‡‹æ€§åˆ†æ (SHAP/LIME)
- æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶èˆ‡æ—¥èªŒç®¡ç†
"""

import json
import time
from datetime import datetime, timedelta
from typing import Dict

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go

# å°å…¥æœå‹™å±¤
from ...core.ai_model_management_service import AIModelManagementService

# å°å…¥çµ„ä»¶
from ..components.ai_model_components import (
    show_model_card,
    show_training_progress,
    show_feature_importance,
    show_model_explanation_analysis,
)


# åˆå§‹åŒ–æœå‹™
@st.cache_resource
def get_ai_model_service():
    """ç²å– AI æ¨¡å‹ç®¡ç†æœå‹™å¯¦ä¾‹ã€‚

    ä½¿ç”¨ Streamlit çš„ cache_resource è£é£¾å™¨ä¾†ç¢ºä¿æœå‹™å¯¦ä¾‹åœ¨æœƒè©±é–“å…±äº«ï¼Œ
    é¿å…é‡è¤‡åˆå§‹åŒ–é€ æˆçš„æ•ˆèƒ½å•é¡Œã€‚

    Returns:
        AIModelManagementService: AI æ¨¡å‹ç®¡ç†æœå‹™å¯¦ä¾‹ï¼Œæä¾›å®Œæ•´çš„æ¨¡å‹ç®¡ç†åŠŸèƒ½

    Example:
        ```python
        service = get_ai_model_service()
        models = service.get_models()
        ```

    Note:
        æ­¤å‡½æ•¸æœƒè¢« Streamlit å¿«å–ï¼Œç¢ºä¿åœ¨æ•´å€‹æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸä¸­
        åªæœƒå»ºç«‹ä¸€å€‹ AIModelManagementService å¯¦ä¾‹ã€‚
    """
    return AIModelManagementService()


def show():
    """é¡¯ç¤º AI æ¨¡å‹ç®¡ç†ä¸»é é¢ã€‚

    å»ºç«‹å®Œæ•´çš„ AI æ¨¡å‹ç®¡ç†ç³»çµ±ä½¿ç”¨è€…ä»‹é¢ï¼ŒåŒ…å«å…­å€‹ä¸»è¦åŠŸèƒ½æ¨™ç±¤é ï¼š
    æ¨¡å‹æ¸…å–®ã€æ¨¡å‹è¨“ç·´ã€æ¨¡å‹æ¨è«–ã€æ¨¡å‹è§£é‡‹ã€æ•ˆèƒ½ç›£æ§å’Œæ¨¡å‹ç®¡ç†ã€‚

    æ­¤å‡½æ•¸æœƒåˆå§‹åŒ–å¿…è¦çš„ session state è®Šæ•¸ï¼Œä¸¦å»ºç«‹æ¨™ç±¤é çµæ§‹
    ä¾†çµ„ç¹”ä¸åŒçš„ AI æ¨¡å‹ç®¡ç†åŠŸèƒ½æ¨¡çµ„ã€‚

    ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
    - æ¨¡å‹æ¸…å–®ç®¡ç†èˆ‡ç€è¦½
    - å¢å¼·ç‰ˆæ¨¡å‹è¨“ç·´åŠŸèƒ½
    - æ¨¡å‹æ¨è«–èˆ‡çµæœåˆ†æ
    - æ¨¡å‹è§£é‡‹æ€§åˆ†æ (SHAP/LIME)
    - æ¨¡å‹æ•ˆèƒ½ç›£æ§
    - æ¨¡å‹ç®¡ç†èˆ‡ç‰ˆæœ¬æ§åˆ¶

    Example:
        ```python
        from src.ui.pages.ai_models import show
        show()  # åœ¨ Streamlit æ‡‰ç”¨ä¸­é¡¯ç¤º AI æ¨¡å‹ç®¡ç†ç³»çµ±
        ```

    Note:
        - ä½¿ç”¨ session state ä¾†ç¶­è­·é¸ä¸­çš„æ¨¡å‹å’Œç•¶å‰æ¨™ç±¤é ç‹€æ…‹
        - æ‰€æœ‰å­åŠŸèƒ½éƒ½é€éç¨ç«‹çš„å‡½æ•¸ä¾†å¯¦ç¾ï¼Œä¿æŒç¨‹å¼ç¢¼æ¨¡çµ„åŒ–
        - æ¨™ç±¤é è¨­è¨ˆè®“ä½¿ç”¨è€…èƒ½å¤ è¼•é¬†åœ¨ä¸åŒåŠŸèƒ½é–“åˆ‡æ›
        - ä¾è³´æ–¼ AIModelManagementService ä¾†åŸ·è¡Œå¯¦éš›çš„æ¨¡å‹ç®¡ç†é‚è¼¯
    """
    st.header("ğŸ¤– AI æ¨¡å‹ç®¡ç†")

    # åˆå§‹åŒ– session state
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = None
    if "current_tab" not in st.session_state:
        st.session_state.current_tab = 0

    # å‰µå»ºæ¨™ç±¤é  - å¢å¼·ç‰ˆæœ¬
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
        [
            "ğŸ“‹ æ¨¡å‹æ¸…å–®",
            "ğŸ¯ æ¨¡å‹è¨“ç·´",
            "ğŸ”® æ¨¡å‹æ¨è«–",
            "ğŸ” æ¨¡å‹è§£é‡‹",
            "ğŸ“Š æ•ˆèƒ½ç›£æ§",
            "ğŸ”§ æ¨¡å‹ç®¡ç†",
        ]
    )

    with tab1:
        show_model_list()

    with tab2:
        show_model_training_enhanced()

    with tab3:
        show_model_inference()

    with tab4:
        show_model_interpretability()

    with tab5:
        show_model_performance_monitoring()

    with tab6:
        show_model_management_enhanced()


def show_model_list():
    """é¡¯ç¤ºæ¨¡å‹æ¸…å–®é é¢"""
    st.subheader("æ¨¡å‹æ¸…å–®ç®¡ç†")

    service = get_ai_model_service()

    # æ§åˆ¶é¢æ¿
    col1, col2, col3, col4 = st.columns([2, 2, 2, 2])

    with col1:
        # æ¨¡å‹é¡å‹éæ¿¾
        model_types = ["æ‰€æœ‰é¡å‹"] + list(service.get_model_types().keys())
        selected_type = st.selectbox("æ¨¡å‹é¡å‹", options=model_types, index=0)

    with col2:
        # æ¨¡å‹ç‹€æ…‹éæ¿¾
        status_options = [
            "æ‰€æœ‰ç‹€æ…‹",
            "created",
            "training",
            "trained",
            "deployed",
            "failed",
        ]
        selected_status = st.selectbox("æ¨¡å‹ç‹€æ…‹", options=status_options, index=0)

    with col3:
        # æœå°‹åŠŸèƒ½
        search_term = st.text_input("æœå°‹æ¨¡å‹", placeholder="è¼¸å…¥æ¨¡å‹åç¨±...")

    with col4:
        # æ’åºé¸é …
        sort_options = ["å‰µå»ºæ™‚é–“", "æ›´æ–°æ™‚é–“", "æ¨¡å‹åç¨±", "æ•ˆèƒ½æŒ‡æ¨™"]
        sort_by = st.selectbox("æ’åºæ–¹å¼", options=sort_options)

    # æ–°å¢æ¨¡å‹æŒ‰éˆ•
    col1, col2, col3 = st.columns([1, 1, 6])

    with col1:
        if st.button("â• æ–°å¢æ¨¡å‹", type="primary"):
            st.session_state.selected_model = None
            st.session_state.current_tab = 1
            st.rerun()

    with col2:
        # æ¨¡å‹ä¸Šå‚³
        uploaded_file = st.file_uploader(
            "ä¸Šå‚³æ¨¡å‹",
            type=["pkl", "joblib", "h5", "pt", "json"],
            help="æ”¯æ´æ ¼å¼: .pkl, .joblib, .h5, .pt, .json",
        )

        if uploaded_file is not None:
            show_model_upload_form(uploaded_file)

    # ç²å–ä¸¦éæ¿¾æ¨¡å‹åˆ—è¡¨
    try:
        models = service.get_models()

        # æ‡‰ç”¨éæ¿¾æ¢ä»¶
        if selected_type != "æ‰€æœ‰é¡å‹":
            models = [m for m in models if m.get("type") == selected_type]

        if selected_status != "æ‰€æœ‰ç‹€æ…‹":
            models = [m for m in models if m.get("status") == selected_status]

        if search_term:
            models = [
                m for m in models if search_term.lower() in m.get("name", "").lower()
            ]

        # æ’åº
        if sort_by == "å‰µå»ºæ™‚é–“":
            models.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        elif sort_by == "æ›´æ–°æ™‚é–“":
            models.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
        elif sort_by == "æ¨¡å‹åç¨±":
            models.sort(key=lambda x: x.get("name", ""))

        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ç¸½æ¨¡å‹æ•¸", len(models))

        with col2:
            trained_count = len(
                [m for m in models if m.get("status") in ["trained", "deployed"]]
            )
            st.metric("å·²è¨“ç·´", trained_count)

        with col3:
            deployed_count = len([m for m in models if m.get("status") == "deployed"])
            st.metric("å·²éƒ¨ç½²", deployed_count)

        with col4:
            training_count = len([m for m in models if m.get("status") == "training"])
            st.metric("è¨“ç·´ä¸­", training_count)

        # é¡¯ç¤ºæ¨¡å‹å¡ç‰‡
        if models:
            st.markdown("---")
            for model in models:
                with st.container():
                    show_model_card(model, show_actions=True)
                    st.markdown("---")
        else:
            st.info("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ¨¡å‹")

    except Exception as e:
        st.error(f"è¼‰å…¥æ¨¡å‹åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


def show_model_upload_form(uploaded_file):
    """é¡¯ç¤ºæ¨¡å‹ä¸Šå‚³è¡¨å–®"""
    with st.expander("æ¨¡å‹ä¸Šå‚³è¨­å®š", expanded=True):
        with st.form("upload_model_form"):
            col1, col2 = st.columns(2)

            with col1:
                model_name = st.text_input(
                    "æ¨¡å‹åç¨±", value=uploaded_file.name.split(".")[0]
                )
                model_type = st.selectbox(
                    "æ¨¡å‹é¡å‹",
                    options=list(get_ai_model_service().get_model_types().keys()),
                )

            with col2:
                sub_type = st.selectbox(
                    "å­é¡å‹",
                    options=get_ai_model_service()
                    .get_model_types()
                    .get(model_type, []),
                )
                author = st.text_input("ä½œè€…", value="ä½¿ç”¨è€…")

            description = st.text_area("æ¨¡å‹æè¿°")

            # ç‰¹å¾µå’Œç›®æ¨™è¨­å®š
            col1, col2 = st.columns(2)

            with col1:
                features_text = st.text_area(
                    "ç‰¹å¾µåˆ—è¡¨ (æ¯è¡Œä¸€å€‹)", placeholder="feature1\nfeature2\nfeature3"
                )

            with col2:
                target = st.text_input("ç›®æ¨™è®Šæ•¸", placeholder="target_column")

            # æ¨¡å‹åƒæ•¸
            parameters_text = st.text_area(
                "æ¨¡å‹åƒæ•¸ (JSONæ ¼å¼)",
                placeholder='{"param1": "value1", "param2": "value2"}',
            )

            submitted = st.form_submit_button("ä¸Šå‚³æ¨¡å‹", type="primary")

            if submitted:
                try:
                    # è§£æç‰¹å¾µåˆ—è¡¨
                    features = (
                        [f.strip() for f in features_text.split("\n") if f.strip()]
                        if features_text
                        else []
                    )

                    # è§£æåƒæ•¸
                    parameters = {}
                    if parameters_text:
                        try:
                            parameters = json.loads(parameters_text)
                        except json.JSONDecodeError:
                            st.error("åƒæ•¸æ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨æœ‰æ•ˆçš„JSONæ ¼å¼")
                            return

                    # ä¸Šå‚³æ¨¡å‹
                    service = get_ai_model_service()
                    model_id = service.upload_model(
                        model_file=uploaded_file.read(),
                        model_name=model_name,
                        model_type=model_type,
                        sub_type=sub_type,
                        description=description,
                        author=author,
                        parameters=parameters,
                        features=features,
                        target=target,
                    )

                    st.success(f"æ¨¡å‹ä¸Šå‚³æˆåŠŸï¼æ¨¡å‹ID: {model_id}")
                    st.rerun()

                except Exception as e:
                    st.error(f"æ¨¡å‹ä¸Šå‚³å¤±æ•—: {str(e)}")


def show_model_training_enhanced():
    """é¡¯ç¤ºå¢å¼·ç‰ˆæ¨¡å‹è¨“ç·´é é¢"""
    st.subheader("ğŸ¯ å¢å¼·ç‰ˆæ¨¡å‹è¨“ç·´")

    service = get_ai_model_service()

    # æª¢æŸ¥æ˜¯å¦æœ‰é¸æ“‡çš„æ¨¡å‹
    is_editing = st.session_state.selected_model is not None

    if is_editing:
        st.info(f"é‡æ–°è¨“ç·´æ¨¡å‹: {st.session_state.selected_model['name']}")
        model_data = st.session_state.selected_model
    else:
        st.info("å‰µå»ºæ–°æ¨¡å‹")
        model_data = {}

    # è¨“ç·´æ¨¡å¼é¸æ“‡
    training_mode = st.selectbox(
        "è¨“ç·´æ¨¡å¼",
        options=["å¿«é€Ÿè¨“ç·´", "æ¨™æº–è¨“ç·´", "æ·±åº¦è¨“ç·´", "è‡ªå‹•èª¿å„ª"],
        index=1,
        help="å¿«é€Ÿè¨“ç·´ï¼šåŸºæœ¬åƒæ•¸ï¼Œå¿«é€Ÿå®Œæˆï¼›æ¨™æº–è¨“ç·´ï¼šå¹³è¡¡æ•ˆèƒ½èˆ‡æ™‚é–“ï¼›æ·±åº¦è¨“ç·´ï¼šæœ€ä½³æ•ˆèƒ½ï¼›è‡ªå‹•èª¿å„ªï¼šè‡ªå‹•å°‹æ‰¾æœ€ä½³åƒæ•¸",
    )

    # æ ¹æ“šè¨“ç·´æ¨¡å¼é¡¯ç¤ºä¸åŒçš„é…ç½®é¸é …
    if training_mode == "å¿«é€Ÿè¨“ç·´":
        show_quick_training_config(model_data, is_editing)
    elif training_mode == "æ¨™æº–è¨“ç·´":
        show_standard_training_config(model_data, is_editing)
    elif training_mode == "æ·±åº¦è¨“ç·´":
        show_deep_training_config(model_data, is_editing)
    else:  # è‡ªå‹•èª¿å„ª
        show_auto_tuning_config(model_data, is_editing)


def show_quick_training_config(model_data, is_editing):
    """é¡¯ç¤ºå¿«é€Ÿè¨“ç·´é…ç½®"""
    st.markdown("### âš¡ å¿«é€Ÿè¨“ç·´é…ç½®")

    with st.form("quick_training_form"):
        col1, col2 = st.columns(2)

        with col1:
            model_name = st.text_input(
                "æ¨¡å‹åç¨±", value=model_data.get("name", ""), disabled=is_editing
            )

            model_type = st.selectbox(
                "æ¨¡å‹é¡å‹", options=["éš¨æ©Ÿæ£®æ—", "XGBoost", "ç·šæ€§å›æ­¸"], index=0
            )

        with col2:
            target_type = st.selectbox(
                "é æ¸¬ç›®æ¨™", options=["è‚¡åƒ¹é æ¸¬", "æ–¹å‘é æ¸¬"], index=0
            )

            data_period = st.selectbox(
                "è³‡æ–™æœŸé–“", options=["1å€‹æœˆ", "3å€‹æœˆ", "6å€‹æœˆ"], index=1
            )

        # å¿«é€Ÿç‰¹å¾µé¸æ“‡
        st.markdown("**ç‰¹å¾µé¸æ“‡**")
        feature_preset = st.selectbox(
            "ç‰¹å¾µé è¨­", options=["åŸºæœ¬æŠ€è¡“æŒ‡æ¨™", "åƒ¹é‡æŒ‡æ¨™", "å…¨éƒ¨æŒ‡æ¨™"], index=0
        )

        submitted = st.form_submit_button("ğŸš€ é–‹å§‹å¿«é€Ÿè¨“ç·´", type="primary")

        if submitted:
            if not model_name:
                st.error("è«‹è¼¸å…¥æ¨¡å‹åç¨±")
                return

            # åŸ·è¡Œå¿«é€Ÿè¨“ç·´
            with st.spinner("æ­£åœ¨åŸ·è¡Œå¿«é€Ÿè¨“ç·´..."):
                try:
                    # æ¨¡æ“¬è¨“ç·´éç¨‹
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    for i in range(100):
                        progress_bar.progress(i + 1)
                        if i < 30:
                            status_text.text("æº–å‚™è³‡æ–™ä¸­...")
                        elif i < 70:
                            status_text.text("è¨“ç·´æ¨¡å‹ä¸­...")
                        else:
                            status_text.text("è©•ä¼°æ¨¡å‹ä¸­...")
                        time.sleep(0.02)

                    st.success("å¿«é€Ÿè¨“ç·´å®Œæˆï¼")

                    # é¡¯ç¤ºè¨“ç·´çµæœ
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.metric("æº–ç¢ºç‡", "85.2%")

                    with col2:
                        st.metric("F1åˆ†æ•¸", "0.83")

                    with col3:
                        st.metric("è¨“ç·´æ™‚é–“", "2.3åˆ†é˜")

                except Exception as e:
                    st.error(f"è¨“ç·´å¤±æ•—: {str(e)}")


def show_standard_training_config(model_data, is_editing):
    """é¡¯ç¤ºæ¨™æº–è¨“ç·´é…ç½®"""
    st.markdown("### ğŸ“Š æ¨™æº–è¨“ç·´é…ç½®")

    with st.form("standard_training_form"):
        # åŸºæœ¬è¨­å®š
        st.markdown("**åŸºæœ¬è¨­å®š**")
        col1, col2 = st.columns(2)

        with col1:
            model_name = st.text_input(
                "æ¨¡å‹åç¨±", value=model_data.get("name", ""), disabled=is_editing
            )

            model_type = st.selectbox(
                "æ¨¡å‹é¡å‹",
                options=["éš¨æ©Ÿæ£®æ—", "XGBoost", "LightGBM", "ç¥ç¶“ç¶²è·¯", "SVM"],
                index=1,
            )

        with col2:
            target_type = st.selectbox(
                "é æ¸¬ç›®æ¨™", options=["è‚¡åƒ¹é æ¸¬", "æ–¹å‘é æ¸¬", "æ³¢å‹•ç‡é æ¸¬"], index=0
            )

            validation_method = st.selectbox(
                "é©—è­‰æ–¹æ³•", options=["æ™‚é–“åºåˆ—åˆ†å‰²", "KæŠ˜äº¤å‰é©—è­‰", "ç•™å‡ºé©—è­‰"], index=0
            )

        # è³‡æ–™è¨­å®š
        st.markdown("**è³‡æ–™è¨­å®š**")
        col1, col2, col3 = st.columns(3)

        with col1:
            train_ratio = st.slider("è¨“ç·´é›†æ¯”ä¾‹", 0.6, 0.9, 0.8)

        with col2:
            val_ratio = st.slider("é©—è­‰é›†æ¯”ä¾‹", 0.1, 0.3, 0.15)

        with col3:
            lookback_days = st.number_input("å›çœ‹å¤©æ•¸", 5, 60, 20)

        # ç‰¹å¾µå·¥ç¨‹
        st.markdown("**ç‰¹å¾µå·¥ç¨‹**")
        col1, col2 = st.columns(2)

        with col1:
            feature_scaling = st.selectbox(
                "ç‰¹å¾µç¸®æ”¾", options=["æ¨™æº–åŒ–", "æ­£è¦åŒ–", "ç„¡"], index=0
            )

            handle_missing = st.selectbox(
                "ç¼ºå¤±å€¼è™•ç†", options=["å‰å‘å¡«å……", "æ’å€¼", "åˆªé™¤"], index=0
            )

        with col2:
            feature_selection = st.checkbox("è‡ªå‹•ç‰¹å¾µé¸æ“‡", value=True)
            outlier_removal = st.checkbox("ç•°å¸¸å€¼ç§»é™¤", value=True)

        # æ¨¡å‹åƒæ•¸
        st.markdown("**æ¨¡å‹åƒæ•¸**")
        if model_type in ["éš¨æ©Ÿæ£®æ—", "XGBoost", "LightGBM"]:
            col1, col2, col3 = st.columns(3)

            with col1:
                n_estimators = st.number_input("æ¨¹çš„æ•¸é‡", 50, 500, 100)

            with col2:
                max_depth = st.number_input("æœ€å¤§æ·±åº¦", 3, 20, 6)

            with col3:
                learning_rate = st.number_input("å­¸ç¿’ç‡", 0.01, 0.3, 0.1)

        # è¨“ç·´è¨­å®š
        st.markdown("**è¨“ç·´è¨­å®š**")
        col1, col2 = st.columns(2)

        with col1:
            early_stopping = st.checkbox("æ—©åœæ©Ÿåˆ¶", value=True)
            if early_stopping:
                patience = st.number_input("è€å¿ƒå€¼", 5, 50, 10)

        with col2:
            save_checkpoints = st.checkbox("ä¿å­˜æª¢æŸ¥é»", value=True)
            log_metrics = st.checkbox("è¨˜éŒ„æŒ‡æ¨™", value=True)

        submitted = st.form_submit_button("ğŸ¯ é–‹å§‹æ¨™æº–è¨“ç·´", type="primary")

        if submitted:
            if not model_name:
                st.error("è«‹è¼¸å…¥æ¨¡å‹åç¨±")
                return

            # åŸ·è¡Œæ¨™æº–è¨“ç·´
            with st.spinner("æ­£åœ¨åŸ·è¡Œæ¨™æº–è¨“ç·´..."):
                try:
                    # æ¨¡æ“¬è¨“ç·´éç¨‹
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    training_stages = [
                        "è¼‰å…¥è³‡æ–™...",
                        "è³‡æ–™é è™•ç†...",
                        "ç‰¹å¾µå·¥ç¨‹...",
                        "æ¨¡å‹è¨“ç·´...",
                        "æ¨¡å‹é©—è­‰...",
                        "æ•ˆèƒ½è©•ä¼°...",
                    ]

                    for i, stage in enumerate(training_stages):
                        for j in range(17):  # æ¯éšæ®µç´„17æ­¥
                            progress = (i * 17 + j) / (len(training_stages) * 17)
                            progress_bar.progress(progress)
                            status_text.text(stage)
                            time.sleep(0.05)

                    st.success("æ¨™æº–è¨“ç·´å®Œæˆï¼")

                    # é¡¯ç¤ºè©³ç´°è¨“ç·´çµæœ
                    show_training_results()

                except Exception as e:
                    st.error(f"è¨“ç·´å¤±æ•—: {str(e)}")


def show_training_results():
    """é¡¯ç¤ºè¨“ç·´çµæœ"""
    st.markdown("### ğŸ“ˆ è¨“ç·´çµæœ")

    # æ•ˆèƒ½æŒ‡æ¨™
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("æº–ç¢ºç‡", "87.5%", delta="2.3%")

    with col2:
        st.metric("ç²¾ç¢ºç‡", "86.2%", delta="1.8%")

    with col3:
        st.metric("å¬å›ç‡", "88.1%", delta="2.1%")

    with col4:
        st.metric("F1åˆ†æ•¸", "0.871", delta="0.019")

    # è¨“ç·´æ›²ç·š
    st.markdown("**è¨“ç·´æ›²ç·š**")

    # ç”Ÿæˆæ¨¡æ“¬è¨“ç·´æ•¸æ“š
    epochs = list(range(1, 51))
    train_loss = [0.8 - 0.01 * i + np.random.normal(0, 0.02) for i in epochs]
    val_loss = [0.85 - 0.008 * i + np.random.normal(0, 0.03) for i in epochs]

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=epochs, y=train_loss, mode="lines", name="è¨“ç·´æå¤±"))
    fig.add_trace(go.Scatter(x=epochs, y=val_loss, mode="lines", name="é©—è­‰æå¤±"))

    fig.update_layout(
        title="è¨“ç·´èˆ‡é©—è­‰æå¤±", xaxis_title="Epoch", yaxis_title="æå¤±", height=400
    )

    st.plotly_chart(fig, use_container_width=True)

    # ç‰¹å¾µé‡è¦æ€§
    st.markdown("**ç‰¹å¾µé‡è¦æ€§**")

    features = ["close", "volume", "rsi", "macd", "sma_20", "bollinger_upper", "atr"]
    importance = [0.25, 0.18, 0.15, 0.12, 0.10, 0.08, 0.12]

    fig = go.Figure(data=[go.Bar(x=features, y=importance)])
    fig.update_layout(
        title="ç‰¹å¾µé‡è¦æ€§æ’åº", xaxis_title="ç‰¹å¾µ", yaxis_title="é‡è¦æ€§", height=400
    )

    st.plotly_chart(fig, use_container_width=True)


def show_deep_training_config(model_data, is_editing):
    """é¡¯ç¤ºæ·±åº¦è¨“ç·´é…ç½®"""
    st.markdown("### ğŸ§  æ·±åº¦è¨“ç·´é…ç½®")
    st.info("æ·±åº¦è¨“ç·´æ¨¡å¼å°‡ä½¿ç”¨æœ€å…ˆé€²çš„æŠ€è¡“å’Œæœ€ä½³åŒ–åƒæ•¸ï¼Œè¨“ç·´æ™‚é–“è¼ƒé•·ä½†æ•ˆèƒ½æœ€ä½³")

    # æ·±åº¦è¨“ç·´çš„è©³ç´°é…ç½®...
    st.markdown("**é«˜ç´šé…ç½®é¸é …**")

    with st.expander("æ¨¡å‹æ¶æ§‹è¨­å®š", expanded=True):
        col1, col2 = st.columns(2)

        with col1:
            architecture = st.selectbox(
                "æ¨¡å‹æ¶æ§‹",
                options=["Transformer", "LSTM", "GRU", "CNN-LSTM", "Attention"],
                index=0,
            )

            layers = st.number_input("å±¤æ•¸", 2, 10, 4)

        with col2:
            hidden_size = st.number_input("éš±è—å±¤å¤§å°", 64, 512, 128)
            attention_heads = st.number_input("æ³¨æ„åŠ›é ­æ•¸", 4, 16, 8)

    with st.expander("å„ªåŒ–å™¨è¨­å®š"):
        col1, col2 = st.columns(2)

        with col1:
            optimizer = st.selectbox(
                "å„ªåŒ–å™¨", options=["AdamW", "Adam", "SGD", "RMSprop"], index=0
            )

            learning_rate = st.number_input("å­¸ç¿’ç‡", 1e-5, 1e-2, 1e-3, format="%.5f")

        with col2:
            weight_decay = st.number_input("æ¬Šé‡è¡°æ¸›", 0.0, 0.1, 0.01)
            scheduler = st.selectbox(
                "å­¸ç¿’ç‡èª¿åº¦å™¨",
                options=["CosineAnnealing", "StepLR", "ReduceLROnPlateau"],
                index=0,
            )

    if st.button("ğŸš€ é–‹å§‹æ·±åº¦è¨“ç·´", type="primary"):
        st.warning("æ·±åº¦è¨“ç·´éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œå»ºè­°åœ¨å¾Œå°åŸ·è¡Œ")
        # æ·±åº¦è¨“ç·´é‚è¼¯...


def show_auto_tuning_config(model_data, is_editing):
    """é¡¯ç¤ºè‡ªå‹•èª¿å„ªé…ç½®"""
    st.markdown("### ğŸ”§ è‡ªå‹•èª¿å„ªé…ç½®")
    st.info("è‡ªå‹•èª¿å„ªå°‡ä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–ç­‰å…ˆé€²æŠ€è¡“è‡ªå‹•å°‹æ‰¾æœ€ä½³åƒæ•¸çµ„åˆ")

    col1, col2 = st.columns(2)

    with col1:
        tuning_method = st.selectbox(
            "èª¿å„ªæ–¹æ³•",
            options=["è²è‘‰æ–¯å„ªåŒ–", "éºå‚³ç®—æ³•", "éš¨æ©Ÿæœç´¢", "ç¶²æ ¼æœç´¢"],
            index=0,
        )

        max_trials = st.number_input("æœ€å¤§è©¦é©—æ¬¡æ•¸", 10, 200, 50)

    with col2:
        optimization_metric = st.selectbox(
            "å„ªåŒ–æŒ‡æ¨™", options=["æº–ç¢ºç‡", "F1åˆ†æ•¸", "AUC", "å¤æ™®æ¯”ç‡"], index=1
        )

        timeout_hours = st.number_input("è¶…æ™‚æ™‚é–“(å°æ™‚)", 1, 24, 6)

    # åƒæ•¸æœç´¢ç©ºé–“
    with st.expander("åƒæ•¸æœç´¢ç©ºé–“", expanded=True):
        st.markdown("**æ¨¡å‹åƒæ•¸ç¯„åœ**")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**æ¨¹æ¨¡å‹åƒæ•¸**")
            n_estimators_range = st.slider("æ¨¹æ•¸é‡ç¯„åœ", 50, 1000, (100, 500))
            max_depth_range = st.slider("æœ€å¤§æ·±åº¦ç¯„åœ", 3, 20, (5, 15))

        with col2:
            st.markdown("**å­¸ç¿’åƒæ•¸**")
            lr_range = st.slider("å­¸ç¿’ç‡ç¯„åœ", 0.01, 0.3, (0.05, 0.2))
            subsample_range = st.slider("å­æ¨£æœ¬æ¯”ä¾‹", 0.5, 1.0, (0.7, 0.9))

    if st.button("ğŸ¯ é–‹å§‹è‡ªå‹•èª¿å„ª", type="primary"):
        with st.spinner("æ­£åœ¨åŸ·è¡Œè‡ªå‹•èª¿å„ª..."):
            # æ¨¡æ“¬è‡ªå‹•èª¿å„ªéç¨‹
            progress_bar = st.progress(0)
            status_text = st.empty()

            for trial in range(max_trials):
                progress = (trial + 1) / max_trials
                progress_bar.progress(progress)
                status_text.text(
                    f"è©¦é©— {trial + 1}/{max_trials} - ç•¶å‰æœ€ä½³åˆ†æ•¸: {0.85 + trial * 0.001:.3f}"
                )
                time.sleep(0.1)

            st.success("è‡ªå‹•èª¿å„ªå®Œæˆï¼")

            # é¡¯ç¤ºæœ€ä½³åƒæ•¸
            st.markdown("**æœ€ä½³åƒæ•¸çµ„åˆ**")
            best_params = {
                "n_estimators": 287,
                "max_depth": 8,
                "learning_rate": 0.087,
                "subsample": 0.83,
            }

            for param, value in best_params.items():
                st.write(f"- {param}: {value}")


def show_model_performance_monitoring():
    """é¡¯ç¤ºæ¨¡å‹æ•ˆèƒ½ç›£æ§é é¢"""
    st.subheader("ğŸ“Š æ¨¡å‹æ•ˆèƒ½ç›£æ§")

    service = get_ai_model_service()

    # ç›£æ§é¸é …
    monitoring_type = st.selectbox(
        "ç›£æ§é¡å‹", options=["å¯¦æ™‚ç›£æ§", "æ­·å²åˆ†æ", "æ•ˆèƒ½æ¯”è¼ƒ", "ç•°å¸¸æª¢æ¸¬"], index=0
    )

    if monitoring_type == "å¯¦æ™‚ç›£æ§":
        show_realtime_monitoring()
    elif monitoring_type == "æ­·å²åˆ†æ":
        show_historical_analysis()
    elif monitoring_type == "æ•ˆèƒ½æ¯”è¼ƒ":
        show_performance_comparison()
    else:  # ç•°å¸¸æª¢æ¸¬
        show_anomaly_detection()


def show_realtime_monitoring():
    """é¡¯ç¤ºå¯¦æ™‚ç›£æ§"""
    st.markdown("### âš¡ å¯¦æ™‚æ•ˆèƒ½ç›£æ§")

    # æ¨¡å‹é¸æ“‡
    service = get_ai_model_service()
    try:
        models = service.get_models()
        deployed_models = [m for m in models if m.get("status") == "deployed"]

        if not deployed_models:
            st.warning("æ²’æœ‰å·²éƒ¨ç½²çš„æ¨¡å‹å¯ä¾›ç›£æ§")
            return

        selected_model = st.selectbox(
            "é¸æ“‡ç›£æ§æ¨¡å‹", options=deployed_models, format_func=lambda x: x["name"]
        )

        # ç›£æ§æŒ‡æ¨™
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            # æ¨¡æ“¬å¯¦æ™‚æº–ç¢ºç‡
            accuracy = np.random.uniform(0.82, 0.88)
            st.metric(
                "å¯¦æ™‚æº–ç¢ºç‡",
                f"{accuracy:.2%}",
                delta=f"{np.random.uniform(-0.02, 0.02):.2%}",
            )

        with col2:
            # æ¨¡æ“¬æ¨è«–å»¶é²
            latency = np.random.uniform(50, 150)
            st.metric(
                "æ¨è«–å»¶é²",
                f"{latency:.0f}ms",
                delta=f"{np.random.uniform(-10, 10):.0f}ms",
            )

        with col3:
            # æ¨¡æ“¬ååé‡
            throughput = np.random.uniform(800, 1200)
            st.metric(
                "ååé‡",
                f"{throughput:.0f}/s",
                delta=f"{np.random.uniform(-50, 50):.0f}/s",
            )

        with col4:
            # æ¨¡æ“¬éŒ¯èª¤ç‡
            error_rate = np.random.uniform(0.01, 0.05)
            st.metric(
                "éŒ¯èª¤ç‡",
                f"{error_rate:.2%}",
                delta=f"{np.random.uniform(-0.01, 0.01):.2%}",
            )

        # å¯¦æ™‚åœ–è¡¨
        st.markdown("**å¯¦æ™‚æ•ˆèƒ½è¶¨å‹¢**")

        # ç”Ÿæˆæ¨¡æ“¬å¯¦æ™‚æ•¸æ“š
        timestamps = pd.date_range(
            start=datetime.now() - timedelta(hours=1), end=datetime.now(), freq="1min"
        )
        accuracy_data = [0.85 + np.random.normal(0, 0.02) for _ in timestamps]
        latency_data = [100 + np.random.normal(0, 20) for _ in timestamps]

        fig = go.Figure()

        # æº–ç¢ºç‡è¶¨å‹¢
        fig.add_trace(
            go.Scatter(
                x=timestamps,
                y=accuracy_data,
                mode="lines",
                name="æº–ç¢ºç‡",
                yaxis="y",
                line=dict(color="blue"),
            )
        )

        # å»¶é²è¶¨å‹¢
        fig.add_trace(
            go.Scatter(
                x=timestamps,
                y=latency_data,
                mode="lines",
                name="å»¶é²(ms)",
                yaxis="y2",
                line=dict(color="red"),
            )
        )

        fig.update_layout(
            title="å¯¦æ™‚æ•ˆèƒ½ç›£æ§",
            xaxis_title="æ™‚é–“",
            yaxis=dict(title="æº–ç¢ºç‡", side="left"),
            yaxis2=dict(title="å»¶é²(ms)", side="right", overlaying="y"),
            height=400,
        )

        st.plotly_chart(fig, use_container_width=True)

        # è­¦å ±è¨­å®š
        st.markdown("**è­¦å ±è¨­å®š**")

        col1, col2 = st.columns(2)

        with col1:
            accuracy_threshold = st.slider("æº–ç¢ºç‡è­¦å ±é–¾å€¼", 0.5, 0.95, 0.8)
            latency_threshold = st.number_input("å»¶é²è­¦å ±é–¾å€¼(ms)", 50, 1000, 200)

        with col2:
            error_threshold = st.slider("éŒ¯èª¤ç‡è­¦å ±é–¾å€¼", 0.01, 0.1, 0.05)
            enable_alerts = st.checkbox("å•Ÿç”¨è­¦å ±", value=True)

        if enable_alerts:
            # æª¢æŸ¥è­¦å ±æ¢ä»¶
            if accuracy < accuracy_threshold:
                st.error(
                    f"âš ï¸ æº–ç¢ºç‡è­¦å ±ï¼šç•¶å‰æº–ç¢ºç‡ {accuracy:.2%} ä½æ–¼é–¾å€¼ {accuracy_threshold:.2%}"
                )

            if latency > latency_threshold:
                st.error(
                    f"âš ï¸ å»¶é²è­¦å ±ï¼šç•¶å‰å»¶é² {latency:.0f}ms é«˜æ–¼é–¾å€¼ {latency_threshold}ms"
                )

            if error_rate > error_threshold:
                st.error(
                    f"âš ï¸ éŒ¯èª¤ç‡è­¦å ±ï¼šç•¶å‰éŒ¯èª¤ç‡ {error_rate:.2%} é«˜æ–¼é–¾å€¼ {error_threshold:.2%}"
                )

    except Exception as e:
        st.error(f"è¼‰å…¥ç›£æ§æ•¸æ“šå¤±æ•—: {str(e)}")


def show_historical_analysis():
    """é¡¯ç¤ºæ­·å²åˆ†æ"""
    st.markdown("### ğŸ“ˆ æ­·å²æ•ˆèƒ½åˆ†æ")

    # æ™‚é–“ç¯„åœé¸æ“‡
    col1, col2 = st.columns(2)

    with col1:
        start_date = st.date_input(
            "é–‹å§‹æ—¥æœŸ", value=datetime.now() - timedelta(days=30)
        )

    with col2:
        end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.now())

    # åˆ†æç¶­åº¦
    analysis_dimension = st.selectbox(
        "åˆ†æç¶­åº¦", options=["æ—¥è¶¨å‹¢", "é€±è¶¨å‹¢", "æœˆè¶¨å‹¢", "å°æ™‚åˆ†å¸ƒ"], index=0
    )

    # ç”Ÿæˆæ­·å²æ•¸æ“š
    if analysis_dimension == "æ—¥è¶¨å‹¢":
        dates = pd.date_range(start=start_date, end=end_date, freq="D")
        accuracy_trend = [0.85 + np.random.normal(0, 0.03) for _ in dates]

        fig = go.Figure()
        fig.add_trace(
            go.Scatter(x=dates, y=accuracy_trend, mode="lines+markers", name="æº–ç¢ºç‡")
        )
        fig.update_layout(
            title="æ—¥æº–ç¢ºç‡è¶¨å‹¢", xaxis_title="æ—¥æœŸ", yaxis_title="æº–ç¢ºç‡"
        )

    elif analysis_dimension == "å°æ™‚åˆ†å¸ƒ":
        hours = list(range(24))
        accuracy_by_hour = [0.85 + np.random.normal(0, 0.02) for _ in hours]

        fig = go.Figure()
        fig.add_trace(go.Bar(x=hours, y=accuracy_by_hour, name="æº–ç¢ºç‡"))
        fig.update_layout(
            title="24å°æ™‚æº–ç¢ºç‡åˆ†å¸ƒ", xaxis_title="å°æ™‚", yaxis_title="æº–ç¢ºç‡"
        )

    st.plotly_chart(fig, use_container_width=True)

    # çµ±è¨ˆæ‘˜è¦
    st.markdown("**çµ±è¨ˆæ‘˜è¦**")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("å¹³å‡æº–ç¢ºç‡", "85.3%")

    with col2:
        st.metric("æœ€é«˜æº–ç¢ºç‡", "92.1%")

    with col3:
        st.metric("æœ€ä½æº–ç¢ºç‡", "78.4%")

    with col4:
        st.metric("æ¨™æº–å·®", "3.2%")


def show_performance_comparison():
    """é¡¯ç¤ºæ•ˆèƒ½æ¯”è¼ƒ"""
    st.markdown("### ğŸ”„ æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ")

    service = get_ai_model_service()

    try:
        models = service.get_models()

        if len(models) < 2:
            st.warning("éœ€è¦è‡³å°‘2å€‹æ¨¡å‹æ‰èƒ½é€²è¡Œæ¯”è¼ƒ")
            return

        # æ¨¡å‹é¸æ“‡
        selected_models = st.multiselect(
            "é¸æ“‡è¦æ¯”è¼ƒçš„æ¨¡å‹",
            options=models,
            format_func=lambda x: x["name"],
            default=models[:3] if len(models) >= 3 else models,
        )

        if len(selected_models) < 2:
            st.warning("è«‹é¸æ“‡è‡³å°‘2å€‹æ¨¡å‹é€²è¡Œæ¯”è¼ƒ")
            return

        # æ¯”è¼ƒæŒ‡æ¨™
        comparison_metrics = st.multiselect(
            "é¸æ“‡æ¯”è¼ƒæŒ‡æ¨™",
            options=["æº–ç¢ºç‡", "ç²¾ç¢ºç‡", "å¬å›ç‡", "F1åˆ†æ•¸", "AUC", "æ¨è«–é€Ÿåº¦"],
            default=["æº–ç¢ºç‡", "F1åˆ†æ•¸", "æ¨è«–é€Ÿåº¦"],
        )

        # ç”Ÿæˆæ¯”è¼ƒæ•¸æ“š
        comparison_data = []
        for model in selected_models:
            model_metrics = {
                "æ¨¡å‹åç¨±": model["name"],
                "æº–ç¢ºç‡": np.random.uniform(0.8, 0.9),
                "ç²¾ç¢ºç‡": np.random.uniform(0.75, 0.88),
                "å¬å›ç‡": np.random.uniform(0.78, 0.92),
                "F1åˆ†æ•¸": np.random.uniform(0.76, 0.89),
                "AUC": np.random.uniform(0.82, 0.94),
                "æ¨è«–é€Ÿåº¦": np.random.uniform(50, 200),
            }
            comparison_data.append(model_metrics)

        # é¡¯ç¤ºæ¯”è¼ƒè¡¨æ ¼
        df = pd.DataFrame(comparison_data)
        st.dataframe(df, use_container_width=True)

        # æ¯”è¼ƒåœ–è¡¨
        if comparison_metrics:
            fig = go.Figure()

            for metric in comparison_metrics:
                if metric in df.columns:
                    fig.add_trace(
                        go.Bar(
                            name=metric,
                            x=df["æ¨¡å‹åç¨±"],
                            y=df[metric],
                            text=df[metric].round(3),
                            textposition="auto",
                        )
                    )

            fig.update_layout(
                title="æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ",
                xaxis_title="æ¨¡å‹",
                yaxis_title="æŒ‡æ¨™å€¼",
                barmode="group",
                height=500,
            )

            st.plotly_chart(fig, use_container_width=True)

        # æ’ååˆ†æ
        st.markdown("**æ’ååˆ†æ**")

        for metric in comparison_metrics:
            if metric in df.columns:
                sorted_df = df.sort_values(metric, ascending=False)
                st.write(f"**{metric}æ’åï¼š**")
                for i, row in sorted_df.iterrows():
                    st.write(f"{i+1}. {row['æ¨¡å‹åç¨±']}: {row[metric]:.3f}")
                st.markdown("---")

    except Exception as e:
        st.error(f"è¼‰å…¥æ¯”è¼ƒæ•¸æ“šå¤±æ•—: {str(e)}")


def show_anomaly_detection():
    """é¡¯ç¤ºç•°å¸¸æª¢æ¸¬"""
    st.markdown("### ğŸš¨ ç•°å¸¸æª¢æ¸¬")

    # ç•°å¸¸æª¢æ¸¬è¨­å®š
    col1, col2 = st.columns(2)

    with col1:
        detection_method = st.selectbox(
            "æª¢æ¸¬æ–¹æ³•",
            options=["çµ±è¨ˆæ–¹æ³•", "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’", "è¦å‰‡å¼•æ“"],
            index=0,
        )

        sensitivity = st.slider("æ•æ„Ÿåº¦", 0.1, 1.0, 0.7)

    with col2:
        time_window = st.selectbox(
            "æ™‚é–“çª—å£", options=["1å°æ™‚", "6å°æ™‚", "24å°æ™‚", "7å¤©"], index=2
        )

        auto_alert = st.checkbox("è‡ªå‹•è­¦å ±", value=True)

    # åŸ·è¡Œç•°å¸¸æª¢æ¸¬
    if st.button("ğŸ” åŸ·è¡Œç•°å¸¸æª¢æ¸¬"):
        with st.spinner("æ­£åœ¨åŸ·è¡Œç•°å¸¸æª¢æ¸¬..."):
            time.sleep(2)

            # æ¨¡æ“¬ç•°å¸¸æª¢æ¸¬çµæœ
            anomalies = [
                {
                    "æ™‚é–“": datetime.now() - timedelta(hours=2),
                    "é¡å‹": "æº–ç¢ºç‡ä¸‹é™",
                    "åš´é‡ç¨‹åº¦": "ä¸­ç­‰",
                    "æè¿°": "æ¨¡å‹æº–ç¢ºç‡åœ¨éå»1å°æ™‚å…§ä¸‹é™äº†5%",
                },
                {
                    "æ™‚é–“": datetime.now() - timedelta(hours=6),
                    "é¡å‹": "æ¨è«–å»¶é²å¢åŠ ",
                    "åš´é‡ç¨‹åº¦": "ä½",
                    "æè¿°": "å¹³å‡æ¨è«–å»¶é²å¢åŠ äº†20ms",
                },
                {
                    "æ™‚é–“": datetime.now() - timedelta(days=1),
                    "é¡å‹": "æ•¸æ“šæ¼‚ç§»",
                    "åš´é‡ç¨‹åº¦": "é«˜",
                    "æè¿°": "è¼¸å…¥æ•¸æ“šåˆ†å¸ƒç™¼ç”Ÿé¡¯è‘—è®ŠåŒ–",
                },
            ]

            st.success("ç•°å¸¸æª¢æ¸¬å®Œæˆï¼")

            # é¡¯ç¤ºç•°å¸¸åˆ—è¡¨
            st.markdown("**æª¢æ¸¬åˆ°çš„ç•°å¸¸ï¼š**")

            for i, anomaly in enumerate(anomalies):
                severity_color = {"é«˜": "ğŸ”´", "ä¸­ç­‰": "ğŸŸ¡", "ä½": "ğŸŸ¢"}

                with st.expander(
                    f"{severity_color[anomaly['åš´é‡ç¨‹åº¦']]} {anomaly['é¡å‹']} - {anomaly['æ™‚é–“'].strftime('%Y-%m-%d %H:%M')}"
                ):
                    st.write(f"**åš´é‡ç¨‹åº¦**: {anomaly['åš´é‡ç¨‹åº¦']}")
                    st.write(f"**æè¿°**: {anomaly['æè¿°']}")
                    st.write(f"**æ™‚é–“**: {anomaly['æ™‚é–“']}")

                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button(f"æ¨™è¨˜å·²è™•ç†", key=f"resolve_{i}"):
                            st.success("ç•°å¸¸å·²æ¨™è¨˜ç‚ºå·²è™•ç†")

                    with col2:
                        if st.button(f"æŸ¥çœ‹è©³æƒ…", key=f"detail_{i}"):
                            st.info("è©³ç´°åˆ†æåŠŸèƒ½é–‹ç™¼ä¸­...")


def show_model_management_enhanced():
    """é¡¯ç¤ºå¢å¼·ç‰ˆæ¨¡å‹ç®¡ç†é é¢"""
    st.subheader("ğŸ”§ å¢å¼·ç‰ˆæ¨¡å‹ç®¡ç†")

    # ç®¡ç†åŠŸèƒ½é¸æ“‡
    management_function = st.selectbox(
        "ç®¡ç†åŠŸèƒ½",
        options=["æ¨¡å‹ç”Ÿå‘½é€±æœŸ", "ç‰ˆæœ¬ç®¡ç†", "éƒ¨ç½²ç®¡ç†", "è³‡æºç®¡ç†", "å®‰å…¨ç®¡ç†"],
        index=0,
    )

    if management_function == "æ¨¡å‹ç”Ÿå‘½é€±æœŸ":
        show_model_lifecycle_management()
    elif management_function == "ç‰ˆæœ¬ç®¡ç†":
        show_model_version_management()
    elif management_function == "éƒ¨ç½²ç®¡ç†":
        show_model_deployment_management()
    elif management_function == "è³‡æºç®¡ç†":
        show_model_resource_management()
    else:  # å®‰å…¨ç®¡ç†
        show_model_security_management()


def show_model_lifecycle_management():
    """é¡¯ç¤ºæ¨¡å‹ç”Ÿå‘½é€±æœŸç®¡ç†"""
    st.markdown("### ğŸ”„ æ¨¡å‹ç”Ÿå‘½é€±æœŸç®¡ç†")

    service = get_ai_model_service()

    try:
        models = service.get_models()

        # ç”Ÿå‘½é€±æœŸçµ±è¨ˆ
        lifecycle_stats = {
            "é–‹ç™¼ä¸­": len([m for m in models if m.get("status") == "created"]),
            "è¨“ç·´ä¸­": len([m for m in models if m.get("status") == "training"]),
            "å·²è¨“ç·´": len([m for m in models if m.get("status") == "trained"]),
            "å·²éƒ¨ç½²": len([m for m in models if m.get("status") == "deployed"]),
            "å·²é€€å½¹": len([m for m in models if m.get("status") == "retired"]),
        }

        col1, col2, col3, col4, col5 = st.columns(5)

        with col1:
            st.metric("é–‹ç™¼ä¸­", lifecycle_stats["é–‹ç™¼ä¸­"])

        with col2:
            st.metric("è¨“ç·´ä¸­", lifecycle_stats["è¨“ç·´ä¸­"])

        with col3:
            st.metric("å·²è¨“ç·´", lifecycle_stats["å·²è¨“ç·´"])

        with col4:
            st.metric("å·²éƒ¨ç½²", lifecycle_stats["å·²éƒ¨ç½²"])

        with col5:
            st.metric("å·²é€€å½¹", lifecycle_stats["å·²é€€å½¹"])

        # ç”Ÿå‘½é€±æœŸæµç¨‹åœ–
        st.markdown("**ç”Ÿå‘½é€±æœŸæµç¨‹**")

        # ä½¿ç”¨ç°¡å–®çš„æ–‡å­—è¡¨ç¤ºæµç¨‹
        st.markdown(
            """
        ```
        é–‹ç™¼ä¸­ â†’ è¨“ç·´ä¸­ â†’ å·²è¨“ç·´ â†’ æ¸¬è©¦ä¸­ â†’ å·²éƒ¨ç½² â†’ ç›£æ§ä¸­ â†’ å·²é€€å½¹
           â†“        â†“        â†“        â†“        â†“        â†“
        å¤±æ•—    è¨“ç·´å¤±æ•—   æ¸¬è©¦å¤±æ•—   éƒ¨ç½²å¤±æ•—   æ•ˆèƒ½ä¸‹é™   ç”Ÿå‘½é€±æœŸçµæŸ
        ```
        """
        )

        # æ¨¡å‹ç‹€æ…‹ç®¡ç†
        st.markdown("**æ¨¡å‹ç‹€æ…‹ç®¡ç†**")

        if models:
            selected_model = st.selectbox(
                "é¸æ“‡æ¨¡å‹",
                options=models,
                format_func=lambda x: f"{x['name']} ({x.get('status', 'unknown')})",
            )

            current_status = selected_model.get("status", "created")

            # å¯ç”¨çš„ç‹€æ…‹è½‰æ›
            status_transitions = {
                "created": ["training", "failed"],
                "training": ["trained", "failed"],
                "trained": ["testing", "deployed"],
                "testing": ["deployed", "failed"],
                "deployed": ["monitoring", "retired"],
                "monitoring": ["deployed", "retired"],
                "failed": ["created"],
                "retired": [],
            }

            available_statuses = status_transitions.get(current_status, [])

            if available_statuses:
                new_status = st.selectbox(
                    f"å°‡ç‹€æ…‹å¾ '{current_status}' è®Šæ›´ç‚º", options=available_statuses
                )

                if st.button("æ›´æ–°ç‹€æ…‹"):
                    # é€™è£¡æ‡‰è©²èª¿ç”¨æœå‹™æ›´æ–°ç‹€æ…‹
                    st.success(f"æ¨¡å‹ç‹€æ…‹å·²æ›´æ–°ç‚º: {new_status}")
            else:
                st.info(f"ç•¶å‰ç‹€æ…‹ '{current_status}' ç„¡å¯ç”¨çš„ç‹€æ…‹è½‰æ›")

    except Exception as e:
        st.error(f"è¼‰å…¥ç”Ÿå‘½é€±æœŸæ•¸æ“šå¤±æ•—: {str(e)}")


def show_model_version_management():
    """é¡¯ç¤ºæ¨¡å‹ç‰ˆæœ¬ç®¡ç†"""
    st.markdown("### ğŸ“‹ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†")
    st.info("ç‰ˆæœ¬ç®¡ç†åŠŸèƒ½é–‹ç™¼ä¸­...")


def show_model_deployment_management():
    """é¡¯ç¤ºæ¨¡å‹éƒ¨ç½²ç®¡ç†"""
    st.markdown("### ğŸš€ æ¨¡å‹éƒ¨ç½²ç®¡ç†")
    st.info("éƒ¨ç½²ç®¡ç†åŠŸèƒ½é–‹ç™¼ä¸­...")


def show_model_resource_management():
    """é¡¯ç¤ºæ¨¡å‹è³‡æºç®¡ç†"""
    st.markdown("### ğŸ’¾ æ¨¡å‹è³‡æºç®¡ç†")
    st.info("è³‡æºç®¡ç†åŠŸèƒ½é–‹ç™¼ä¸­...")


def show_model_security_management():
    """é¡¯ç¤ºæ¨¡å‹å®‰å…¨ç®¡ç†"""
    st.markdown("### ğŸ”’ æ¨¡å‹å®‰å…¨ç®¡ç†")
    st.info("å®‰å…¨ç®¡ç†åŠŸèƒ½é–‹ç™¼ä¸­...")


def show_model_training():
    """é¡¯ç¤ºæ¨¡å‹è¨“ç·´é é¢"""
    st.subheader("æ¨¡å‹è¨“ç·´")

    service = get_ai_model_service()

    # æª¢æŸ¥æ˜¯å¦æœ‰é¸æ“‡çš„æ¨¡å‹
    is_editing = st.session_state.selected_model is not None

    if is_editing:
        st.info(f"é‡æ–°è¨“ç·´æ¨¡å‹: {st.session_state.selected_model['name']}")
        model_data = st.session_state.selected_model
    else:
        st.info("å‰µå»ºæ–°æ¨¡å‹")
        model_data = {}

    # è¨“ç·´è¡¨å–®
    with st.form("model_training_form"):
        st.markdown("### ğŸ“ åŸºæœ¬è³‡è¨Š")

        col1, col2 = st.columns(2)

        with col1:
            model_name = st.text_input(
                "æ¨¡å‹åç¨±", value=model_data.get("name", ""), disabled=is_editing
            )

            model_type = st.selectbox(
                "æ¨¡å‹é¡å‹",
                options=list(service.get_model_types().keys()),
                index=(
                    list(service.get_model_types().keys()).index(
                        model_data.get("type", "æ©Ÿå™¨å­¸ç¿’æ¨¡å‹")
                    )
                    if model_data.get("type") in service.get_model_types().keys()
                    else 0
                ),
            )

        with col2:
            sub_type = st.selectbox(
                "å­é¡å‹",
                options=service.get_model_types().get(model_type, []),
                index=(
                    service.get_model_types()
                    .get(model_type, [])
                    .index(model_data.get("sub_type", ""))
                    if model_data.get("sub_type")
                    in service.get_model_types().get(model_type, [])
                    else 0
                ),
            )

            author = st.text_input("ä½œè€…", value=model_data.get("author", "ç³»çµ±"))

        description = st.text_area(
            "æ¨¡å‹æè¿°", value=model_data.get("description", ""), height=100
        )

        st.markdown("### ğŸ“Š è³‡æ–™è¨­å®š")

        col1, col2 = st.columns(2)

        with col1:
            # è‚¡ç¥¨é¸æ“‡
            stock_symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "AMZN", "NVDA", "META"]
            selected_stocks = st.multiselect(
                "é¸æ“‡è‚¡ç¥¨", options=stock_symbols, default=["AAPL"]
            )

            # æ—¥æœŸç¯„åœ
            date_range = st.date_input(
                "è¨“ç·´è³‡æ–™æ—¥æœŸç¯„åœ",
                value=[
                    datetime.now() - timedelta(days=365),
                    datetime.now() - timedelta(days=30),
                ],
                max_value=datetime.now().date(),
            )

        with col2:
            # è³‡æ–™åˆ†å‰²æ¯”ä¾‹
            train_ratio = st.slider("è¨“ç·´é›†æ¯”ä¾‹", 0.5, 0.9, 0.8, 0.05)
            val_ratio = st.slider("é©—è­‰é›†æ¯”ä¾‹", 0.05, 0.3, 0.15, 0.05)

            # è³‡æ–™é è™•ç†é¸é …
            normalize_data = st.checkbox("æ¨™æº–åŒ–è³‡æ–™", value=True)
            handle_missing = st.selectbox(
                "ç¼ºå¤±å€¼è™•ç†", options=["å‰å‘å¡«å……", "å¾Œå‘å¡«å……", "ç·šæ€§æ’å€¼", "åˆªé™¤"]
            )

        st.markdown("### ğŸ¯ ç‰¹å¾µèˆ‡ç›®æ¨™")

        col1, col2 = st.columns(2)

        with col1:
            # ç‰¹å¾µé¸æ“‡
            available_features = [
                "open",
                "high",
                "low",
                "close",
                "volume",
                "sma_5",
                "sma_10",
                "sma_20",
                "ema_12",
                "ema_26",
                "rsi",
                "macd",
                "bollinger_upper",
                "bollinger_lower",
                "atr",
                "stoch_k",
                "stoch_d",
            ]

            selected_features = st.multiselect(
                "é¸æ“‡ç‰¹å¾µ",
                options=available_features,
                default=["close", "volume", "sma_5", "rsi", "macd"],
            )

        with col2:
            # ç›®æ¨™è®Šæ•¸è¨­å®š
            target_type = st.selectbox(
                "é æ¸¬ç›®æ¨™é¡å‹", options=["åƒ¹æ ¼é æ¸¬", "æ–¹å‘é æ¸¬", "æ³¢å‹•ç‡é æ¸¬", "è‡ªå®šç¾©"]
            )

            if target_type == "åƒ¹æ ¼é æ¸¬":
                target_column = "close_next"
                prediction_horizon = st.selectbox(
                    "é æ¸¬æ™‚é–“ç¯„åœ", [1, 5, 10, 20], index=0
                )
            elif target_type == "æ–¹å‘é æ¸¬":
                target_column = "direction"
                prediction_horizon = st.selectbox(
                    "é æ¸¬æ™‚é–“ç¯„åœ", [1, 5, 10, 20], index=0
                )
            elif target_type == "æ³¢å‹•ç‡é æ¸¬":
                target_column = "volatility"
                prediction_horizon = st.selectbox(
                    "é æ¸¬æ™‚é–“ç¯„åœ", [1, 5, 10, 20], index=0
                )
            else:
                target_column = st.text_input("è‡ªå®šç¾©ç›®æ¨™æ¬„ä½")
                prediction_horizon = 1

        st.markdown("### âš™ï¸ æ¨¡å‹åƒæ•¸")

        # æ ¹æ“šæ¨¡å‹é¡å‹é¡¯ç¤ºä¸åŒåƒæ•¸
        if model_type == "æ©Ÿå™¨å­¸ç¿’æ¨¡å‹":
            if sub_type in ["éš¨æ©Ÿæ£®æ— (Random Forest)", "XGBoost", "LightGBM"]:
                col1, col2, col3 = st.columns(3)

                with col1:
                    n_estimators = st.number_input("æ¨¹çš„æ•¸é‡", 10, 1000, 100)
                    max_depth = st.number_input("æœ€å¤§æ·±åº¦", 1, 50, 10)

                with col2:
                    learning_rate = st.number_input("å­¸ç¿’ç‡", 0.001, 1.0, 0.1, 0.001)
                    min_samples_split = st.number_input("æœ€å°åˆ†å‰²æ¨£æœ¬æ•¸", 2, 20, 2)

                with col3:
                    random_state = st.number_input("éš¨æ©Ÿç¨®å­", 0, 9999, 42)
                    early_stopping = st.checkbox("æ—©åœæ©Ÿåˆ¶", value=True)

                model_params = {
                    "n_estimators": n_estimators,
                    "max_depth": max_depth,
                    "learning_rate": learning_rate,
                    "min_samples_split": min_samples_split,
                    "random_state": random_state,
                    "early_stopping": early_stopping,
                }

            else:
                # å…¶ä»–æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„é€šç”¨åƒæ•¸
                col1, col2 = st.columns(2)

                with col1:
                    regularization = st.number_input("æ­£å‰‡åŒ–å¼·åº¦", 0.0, 10.0, 1.0, 0.1)
                    random_state = st.number_input("éš¨æ©Ÿç¨®å­", 0, 9999, 42)

                with col2:
                    max_iter = st.number_input("æœ€å¤§è¿­ä»£æ¬¡æ•¸", 100, 10000, 1000)
                    tolerance = st.number_input(
                        "æ”¶æ–‚å®¹å¿åº¦", 1e-6, 1e-2, 1e-4, format="%.6f"
                    )

                model_params = {
                    "regularization": regularization,
                    "max_iter": max_iter,
                    "tolerance": tolerance,
                    "random_state": random_state,
                }

        elif model_type == "æ·±åº¦å­¸ç¿’æ¨¡å‹":
            col1, col2, col3 = st.columns(3)

            with col1:
                hidden_layers = st.number_input("éš±è—å±¤æ•¸", 1, 10, 3)
                hidden_units = st.number_input("éš±è—å–®å…ƒæ•¸", 16, 512, 128)

            with col2:
                learning_rate = st.number_input("å­¸ç¿’ç‡", 0.0001, 0.1, 0.001, 0.0001)
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 16, 256, 32)

            with col3:
                epochs = st.number_input("è¨“ç·´è¼ªæ•¸", 10, 1000, 100)
                dropout_rate = st.number_input("Dropoutç‡", 0.0, 0.8, 0.2, 0.1)

            model_params = {
                "hidden_layers": hidden_layers,
                "hidden_units": hidden_units,
                "learning_rate": learning_rate,
                "batch_size": batch_size,
                "epochs": epochs,
                "dropout_rate": dropout_rate,
            }

        else:
            # è¦å‰‡å‹æ¨¡å‹æˆ–é›†æˆæ¨¡å‹çš„åƒæ•¸
            st.info("æ­¤æ¨¡å‹é¡å‹ä½¿ç”¨é è¨­åƒæ•¸é…ç½®")
            model_params = {}

        st.markdown("### ğŸš€ è¨“ç·´è¨­å®š")

        col1, col2 = st.columns(2)

        with col1:
            cross_validation = st.checkbox("äº¤å‰é©—è­‰", value=True)
            if cross_validation:
                cv_folds = st.number_input("äº¤å‰é©—è­‰æŠ˜æ•¸", 3, 10, 5)
            else:
                cv_folds = None

        with col2:
            hyperparameter_tuning = st.checkbox("è¶…åƒæ•¸èª¿å„ª", value=False)
            if hyperparameter_tuning:
                tuning_method = st.selectbox(
                    "èª¿å„ªæ–¹æ³•", options=["ç¶²æ ¼æœç´¢", "éš¨æ©Ÿæœç´¢", "è²è‘‰æ–¯å„ªåŒ–"]
                )
            else:
                tuning_method = None

        # æäº¤æŒ‰éˆ•
        submitted = st.form_submit_button("ğŸ¯ é–‹å§‹è¨“ç·´", type="primary")

        if submitted:
            # é©—è­‰è¼¸å…¥
            if not model_name:
                st.error("è«‹è¼¸å…¥æ¨¡å‹åç¨±")
                return

            if not selected_stocks:
                st.error("è«‹é¸æ“‡è‡³å°‘ä¸€å€‹è‚¡ç¥¨")
                return

            if not selected_features:
                st.error("è«‹é¸æ“‡è‡³å°‘ä¸€å€‹ç‰¹å¾µ")
                return

            # æº–å‚™è¨“ç·´è³‡æ–™
            training_config = {
                "model_info": {
                    "name": model_name,
                    "type": model_type,
                    "sub_type": sub_type,
                    "description": description,
                    "author": author,
                },
                "data_config": {
                    "stocks": selected_stocks,
                    "date_range": [d.isoformat() for d in date_range],
                    "features": selected_features,
                    "target": target_column,
                    "prediction_horizon": prediction_horizon,
                    "train_ratio": train_ratio,
                    "val_ratio": val_ratio,
                    "normalize": normalize_data,
                    "missing_handling": handle_missing,
                },
                "model_params": model_params,
                "training_config": {
                    "cross_validation": cross_validation,
                    "cv_folds": cv_folds,
                    "hyperparameter_tuning": hyperparameter_tuning,
                    "tuning_method": tuning_method,
                },
            }

            # é–‹å§‹è¨“ç·´
            try:
                if is_editing:
                    # é‡æ–°è¨“ç·´ç¾æœ‰æ¨¡å‹
                    training_id = service.start_training(
                        model_id=st.session_state.selected_model["id"],
                        training_data=training_config["data_config"],
                        training_params=training_config,
                    )
                else:
                    # å‰µå»ºæ–°æ¨¡å‹ä¸¦é–‹å§‹è¨“ç·´
                    model_id = service.create_model(
                        name=model_name,
                        model_type=model_type,
                        sub_type=sub_type,
                        description=description,
                        author=author,
                        parameters=model_params,
                        features=selected_features,
                        target=target_column,
                    )

                    training_id = service.start_training(
                        model_id=model_id,
                        training_data=training_config["data_config"],
                        training_params=training_config,
                    )

                st.success(f"è¨“ç·´å·²é–‹å§‹ï¼è¨“ç·´ID: {training_id}")
                st.info("è«‹åˆ°æ¨¡å‹æ¸…å–®é é¢æŸ¥çœ‹è¨“ç·´é€²åº¦")

                # æ¸…é™¤é¸æ“‡çš„æ¨¡å‹
                st.session_state.selected_model = None

            except Exception as e:
                st.error(f"å•Ÿå‹•è¨“ç·´æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    # é¡¯ç¤ºè¨“ç·´æ­·å²
    if is_editing and st.session_state.selected_model:
        st.markdown("---")
        st.subheader("ğŸ“ˆ è¨“ç·´æ­·å²")

        try:
            training_logs = service.get_training_logs(
                st.session_state.selected_model["id"]
            )
            if training_logs:
                show_training_progress(training_logs)
            else:
                st.info("æ­¤æ¨¡å‹å°šç„¡è¨“ç·´è¨˜éŒ„")
        except Exception as e:
            st.error(f"è¼‰å…¥è¨“ç·´æ­·å²æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


def show_model_inference():
    """é¡¯ç¤ºæ¨¡å‹æ¨è«–é é¢"""
    st.subheader("æ¨¡å‹æ¨è«–")

    service = get_ai_model_service()

    # ç²å–å¯ç”¨æ¨¡å‹
    try:
        models = service.get_models()
        active_models = [
            m for m in models if m.get("status") in ["trained", "deployed"]
        ]

        if not active_models:
            st.warning("æ²’æœ‰å¯ç”¨çš„å·²è¨“ç·´æ¨¡å‹")
            return

        # æ¨¡å‹é¸æ“‡
        col1, col2 = st.columns([2, 1])

        with col1:
            model_options = [
                f"{m['name']} - {m['type']} ({m['status']})" for m in active_models
            ]
            selected_model_idx = st.selectbox(
                "é¸æ“‡æ¨¡å‹",
                options=range(len(model_options)),
                format_func=lambda x: model_options[x],
            )
            selected_model = active_models[selected_model_idx]

        with col2:
            # é¡¯ç¤ºæ¨¡å‹åŸºæœ¬è³‡è¨Š
            st.metric("æ¨¡å‹ç‹€æ…‹", selected_model["status"])
            if selected_model.get("performance_metrics"):
                metrics = selected_model["performance_metrics"]
                if isinstance(metrics, str):
                    try:
                        metrics = json.loads(metrics)
                    except:
                        metrics = {}

                if "accuracy" in metrics:
                    st.metric("æº–ç¢ºç‡", f"{metrics['accuracy']:.2%}")
                elif "mse" in metrics:
                    st.metric("MSE", f"{metrics['mse']:.4f}")

        # æ¨è«–æ¨¡å¼é¸æ“‡
        st.markdown("### ğŸ”® æ¨è«–è¨­å®š")

        inference_mode = st.selectbox(
            "æ¨è«–æ¨¡å¼",
            options=["å–®ä¸€æ¨£æœ¬æ¨è«–", "æ‰¹é‡æ¨è«–", "å¯¦æ™‚æ¨è«–"],
            help="é¸æ“‡æ¨è«–æ¨¡å¼ï¼šå–®ä¸€æ¨£æœ¬é©åˆæ¸¬è©¦ï¼Œæ‰¹é‡æ¨è«–é©åˆå¤§é‡æ•¸æ“šï¼Œå¯¦æ™‚æ¨è«–é©åˆå³æ™‚é æ¸¬",
        )

        if inference_mode == "å–®ä¸€æ¨£æœ¬æ¨è«–":
            show_single_inference(selected_model, service)
        elif inference_mode == "æ‰¹é‡æ¨è«–":
            show_batch_inference(selected_model, service)
        else:  # å¯¦æ™‚æ¨è«–
            show_realtime_inference(selected_model, service)

    except Exception as e:
        st.error(f"è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


def show_single_inference(model: Dict, service: AIModelManagementService):
    """é¡¯ç¤ºå–®ä¸€æ¨£æœ¬æ¨è«–ä»‹é¢"""

    with st.form("single_inference_form"):
        st.markdown("#### ğŸ“Š è¼¸å…¥æ•¸æ“š")

        # æ ¹æ“šæ¨¡å‹ç‰¹å¾µå‹•æ…‹ç”Ÿæˆè¼¸å…¥æ¬„ä½
        features = model.get("features", [])
        if isinstance(features, str):
            try:
                features = json.loads(features)
            except:
                features = []

        if not features:
            st.warning("æ­¤æ¨¡å‹æ²’æœ‰å®šç¾©ç‰¹å¾µï¼Œä½¿ç”¨é è¨­ç‰¹å¾µ")
            features = ["close", "volume", "sma_5", "rsi", "macd"]

        # å‰µå»ºè¼¸å…¥æ¬„ä½
        input_data = {}

        # åˆ†æˆå¤šåˆ—é¡¯ç¤º
        cols = st.columns(min(3, len(features)))

        for i, feature in enumerate(features):
            col_idx = i % len(cols)
            with cols[col_idx]:
                if feature in ["close", "open", "high", "low"]:
                    input_data[feature] = st.number_input(
                        f"{feature.upper()}",
                        value=100.0,
                        min_value=0.0,
                        step=0.01,
                        key=f"single_{feature}",
                    )
                elif feature == "volume":
                    input_data[feature] = st.number_input(
                        "Volume",
                        value=1000000,
                        min_value=0,
                        step=1000,
                        key=f"single_{feature}",
                    )
                elif "sma" in feature or "ema" in feature:
                    input_data[feature] = st.number_input(
                        feature.upper(),
                        value=100.0,
                        min_value=0.0,
                        step=0.01,
                        key=f"single_{feature}",
                    )
                elif feature == "rsi":
                    input_data[feature] = st.number_input(
                        "RSI",
                        value=50.0,
                        min_value=0.0,
                        max_value=100.0,
                        step=0.1,
                        key=f"single_{feature}",
                    )
                elif feature == "macd":
                    input_data[feature] = st.number_input(
                        "MACD",
                        value=0.0,
                        step=0.001,
                        format="%.3f",
                        key=f"single_{feature}",
                    )
                else:
                    input_data[feature] = st.number_input(
                        feature, value=0.0, step=0.01, key=f"single_{feature}"
                    )

        # é«˜ç´šé¸é …
        with st.expander("é«˜ç´šé¸é …"):
            return_probabilities = st.checkbox("è¿”å›é æ¸¬æ©Ÿç‡", value=True)
            return_confidence = st.checkbox("è¿”å›ä¿¡å¿ƒåº¦", value=True)
            explain_prediction = st.checkbox("è§£é‡‹é æ¸¬çµæœ", value=False)

        submitted = st.form_submit_button("ğŸ”® åŸ·è¡Œæ¨è«–", type="primary")

        if submitted:
            try:
                # åŸ·è¡Œæ¨è«–
                with st.spinner("æ­£åœ¨åŸ·è¡Œæ¨è«–..."):
                    result = service.run_inference(
                        model_id=model["id"],
                        input_data=input_data,
                        return_probabilities=return_probabilities,
                        return_confidence=return_confidence,
                    )

                # é¡¯ç¤ºçµæœ
                st.markdown("#### ğŸ¯ æ¨è«–çµæœ")

                col1, col2, col3 = st.columns(3)

                with col1:
                    if "prediction" in result:
                        prediction = result["prediction"]
                        if isinstance(prediction, (int, float)):
                            st.metric("é æ¸¬å€¼", f"{prediction:.4f}")
                        else:
                            st.metric("é æ¸¬é¡åˆ¥", str(prediction))

                with col2:
                    if "confidence" in result:
                        confidence = result["confidence"]
                        st.metric("ä¿¡å¿ƒåº¦", f"{confidence:.2%}")

                with col3:
                    if "signal" in result:
                        signal = result["signal"]
                        signal_color = {"buy": "ğŸŸ¢", "sell": "ğŸ”´", "hold": "ğŸŸ¡"}.get(
                            signal, "âšª"
                        )
                        st.metric("äº¤æ˜“ä¿¡è™Ÿ", f"{signal_color} {signal.upper()}")

                # é¡¯ç¤ºæ©Ÿç‡åˆ†ä½ˆï¼ˆå¦‚æœæœ‰ï¼‰
                if "probabilities" in result and return_probabilities:
                    st.markdown("#### ğŸ“Š é æ¸¬æ©Ÿç‡åˆ†ä½ˆ")
                    probs = result["probabilities"]
                    if isinstance(probs, list) and len(probs) == 2:
                        # äºŒåˆ†é¡
                        prob_df = pd.DataFrame(
                            {"é¡åˆ¥": ["ä¸‹è·Œ", "ä¸Šæ¼²"], "æ©Ÿç‡": probs}
                        )

                        fig = px.bar(
                            prob_df,
                            x="é¡åˆ¥",
                            y="æ©Ÿç‡",
                            title="é æ¸¬æ©Ÿç‡åˆ†ä½ˆ",
                            color="æ©Ÿç‡",
                            color_continuous_scale="RdYlGn",
                        )
                        st.plotly_chart(fig, use_container_width=True)

                # é¡¯ç¤ºè§£é‡‹ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if explain_prediction and "explanation" in result:
                    st.markdown("#### ğŸ” é æ¸¬è§£é‡‹")
                    show_model_explanation_analysis(result["explanation"])

                # é¡¯ç¤ºåŸå§‹çµæœ
                with st.expander("åŸå§‹çµæœæ•¸æ“š"):
                    st.json(result)

            except Exception as e:
                st.error(f"æ¨è«–åŸ·è¡Œå¤±æ•—: {str(e)}")


def show_batch_inference(model: Dict, service: AIModelManagementService):
    """é¡¯ç¤ºæ‰¹é‡æ¨è«–ä»‹é¢"""

    st.markdown("#### ğŸ“ æ‰¹é‡æ•¸æ“šè¼¸å…¥")

    # æ•¸æ“šè¼¸å…¥æ–¹å¼é¸æ“‡
    input_method = st.selectbox(
        "æ•¸æ“šè¼¸å…¥æ–¹å¼", options=["ä¸Šå‚³CSVæª”æ¡ˆ", "æ‰‹å‹•è¼¸å…¥", "å¾è³‡æ–™åº«æŸ¥è©¢"]
    )

    input_data = None

    if input_method == "ä¸Šå‚³CSVæª”æ¡ˆ":
        uploaded_file = st.file_uploader(
            "ä¸Šå‚³CSVæª”æ¡ˆ", type=["csv"], help="è«‹ç¢ºä¿CSVæª”æ¡ˆåŒ…å«æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ç‰¹å¾µæ¬„ä½"
        )

        if uploaded_file is not None:
            try:
                input_data = pd.read_csv(uploaded_file)
                st.success(f"æˆåŠŸè¼‰å…¥ {len(input_data)} ç­†è¨˜éŒ„")

                # é¡¯ç¤ºæ•¸æ“šé è¦½
                st.markdown("##### æ•¸æ“šé è¦½")
                st.dataframe(input_data.head(), use_container_width=True)

                # æª¢æŸ¥ç‰¹å¾µæ¬„ä½
                model_features = model.get("features", [])
                if isinstance(model_features, str):
                    try:
                        model_features = json.loads(model_features)
                    except:
                        model_features = []

                missing_features = set(model_features) - set(input_data.columns)
                if missing_features:
                    st.warning(f"ç¼ºå°‘ç‰¹å¾µæ¬„ä½: {', '.join(missing_features)}")

            except Exception as e:
                st.error(f"è®€å–CSVæª”æ¡ˆå¤±æ•—: {str(e)}")

    elif input_method == "æ‰‹å‹•è¼¸å…¥":
        st.markdown("##### æ‰‹å‹•è¼¸å…¥æ•¸æ“š")

        # ç²å–æ¨¡å‹ç‰¹å¾µ
        features = model.get("features", [])
        if isinstance(features, str):
            try:
                features = json.loads(features)
            except:
                features = ["close", "volume", "sma_5", "rsi", "macd"]

        # å‰µå»ºç©ºçš„DataFrameä¾›ç·¨è¼¯
        if "batch_data" not in st.session_state:
            st.session_state.batch_data = pd.DataFrame(columns=features)

        # æ•¸æ“šç·¨è¼¯å™¨
        edited_data = st.data_editor(
            st.session_state.batch_data,
            num_rows="dynamic",
            use_container_width=True,
            key="batch_data_editor",
        )

        if len(edited_data) > 0:
            input_data = edited_data
            st.session_state.batch_data = edited_data

    else:  # å¾è³‡æ–™åº«æŸ¥è©¢
        st.markdown("##### å¾è³‡æ–™åº«æŸ¥è©¢æ•¸æ“š")

        col1, col2 = st.columns(2)

        with col1:
            stock_symbol = st.selectbox(
                "è‚¡ç¥¨ä»£ç¢¼", options=["AAPL", "GOOGL", "MSFT", "TSLA", "AMZN"]
            )

            date_range = st.date_input(
                "æ—¥æœŸç¯„åœ",
                value=[datetime.now() - timedelta(days=30), datetime.now()],
                max_value=datetime.now().date(),
            )

        with col2:
            limit = st.number_input("è¨˜éŒ„æ•¸é™åˆ¶", 1, 1000, 100)

            if st.button("æŸ¥è©¢æ•¸æ“š"):
                # é€™è£¡æ‡‰è©²å¾å¯¦éš›è³‡æ–™åº«æŸ¥è©¢
                # ç›®å‰ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
                dates = pd.date_range(start=date_range[0], end=date_range[1], freq="D")
                input_data = pd.DataFrame(
                    {
                        "date": dates[:limit],
                        "close": np.random.normal(100, 10, len(dates[:limit])),
                        "volume": np.random.randint(
                            1000000, 10000000, len(dates[:limit])
                        ),
                        "sma_5": np.random.normal(100, 8, len(dates[:limit])),
                        "rsi": np.random.uniform(20, 80, len(dates[:limit])),
                        "macd": np.random.normal(0, 2, len(dates[:limit])),
                    }
                )

                st.success(f"æŸ¥è©¢åˆ° {len(input_data)} ç­†è¨˜éŒ„")
                st.dataframe(input_data.head(), use_container_width=True)

    # åŸ·è¡Œæ‰¹é‡æ¨è«–
    if input_data is not None and len(input_data) > 0:
        st.markdown("#### âš™ï¸ æ¨è«–è¨­å®š")

        col1, col2 = st.columns(2)

        with col1:
            batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", 1, 1000, 100)
            return_probabilities = st.checkbox("è¿”å›é æ¸¬æ©Ÿç‡", value=False)

        with col2:
            save_results = st.checkbox("ä¿å­˜çµæœ", value=True)
            result_format = st.selectbox("çµæœæ ¼å¼", ["CSV", "JSON", "Excel"])

        if st.button("ğŸš€ åŸ·è¡Œæ‰¹é‡æ¨è«–", type="primary"):
            try:
                with st.spinner("æ­£åœ¨åŸ·è¡Œæ‰¹é‡æ¨è«–..."):
                    # åˆ†æ‰¹è™•ç†
                    results = []
                    progress_bar = st.progress(0)

                    total_batches = len(input_data) // batch_size + (
                        1 if len(input_data) % batch_size > 0 else 0
                    )

                    for i in range(0, len(input_data), batch_size):
                        batch_data = input_data.iloc[i : i + batch_size]

                        # åŸ·è¡Œæ¨è«–
                        for _, row in batch_data.iterrows():
                            result = service.run_inference(
                                model_id=model["id"],
                                input_data=row.to_dict(),
                                return_probabilities=return_probabilities,
                            )
                            results.append(result)

                        # æ›´æ–°é€²åº¦
                        progress = min((i + batch_size) / len(input_data), 1.0)
                        progress_bar.progress(progress)

                # æ•´ç†çµæœ
                results_df = pd.DataFrame(results)

                st.success(f"æ‰¹é‡æ¨è«–å®Œæˆï¼è™•ç†äº† {len(results)} ç­†è¨˜éŒ„")

                # é¡¯ç¤ºçµæœçµ±è¨ˆ
                st.markdown("#### ğŸ“Š æ¨è«–çµæœçµ±è¨ˆ")

                col1, col2, col3 = st.columns(3)

                with col1:
                    if "prediction" in results_df.columns:
                        avg_prediction = results_df["prediction"].mean()
                        st.metric("å¹³å‡é æ¸¬å€¼", f"{avg_prediction:.4f}")

                with col2:
                    if "confidence" in results_df.columns:
                        avg_confidence = results_df["confidence"].mean()
                        st.metric("å¹³å‡ä¿¡å¿ƒåº¦", f"{avg_confidence:.2%}")

                with col3:
                    if "signal" in results_df.columns:
                        signal_counts = results_df["signal"].value_counts()
                        most_common = (
                            signal_counts.index[0] if len(signal_counts) > 0 else "N/A"
                        )
                        st.metric("ä¸»è¦ä¿¡è™Ÿ", most_common)

                # é¡¯ç¤ºçµæœè¡¨æ ¼
                st.markdown("#### ğŸ“‹ è©³ç´°çµæœ")
                st.dataframe(results_df, use_container_width=True)

                # çµæœä¸‹è¼‰
                if save_results:
                    if result_format == "CSV":
                        csv = results_df.to_csv(index=False)
                        st.download_button(
                            "ä¸‹è¼‰CSVçµæœ",
                            csv,
                            f"batch_inference_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            "text/csv",
                        )
                    elif result_format == "JSON":
                        json_str = results_df.to_json(orient="records", indent=2)
                        st.download_button(
                            "ä¸‹è¼‰JSONçµæœ",
                            json_str,
                            f"batch_inference_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            "application/json",
                        )

            except Exception as e:
                st.error(f"æ‰¹é‡æ¨è«–åŸ·è¡Œå¤±æ•—: {str(e)}")


def show_realtime_inference(model: Dict, service: AIModelManagementService):
    """é¡¯ç¤ºå¯¦æ™‚æ¨è«–ä»‹é¢"""

    st.markdown("#### âš¡ å¯¦æ™‚æ¨è«–è¨­å®š")

    col1, col2 = st.columns(2)

    with col1:
        # æ•¸æ“šä¾†æºè¨­å®š
        data_source = st.selectbox(
            "æ•¸æ“šä¾†æº", options=["æ¨¡æ“¬æ•¸æ“š", "APIæ¥å£", "è³‡æ–™åº«"]
        )

        if data_source == "æ¨¡æ“¬æ•¸æ“š":
            stock_symbol = st.selectbox(
                "è‚¡ç¥¨ä»£ç¢¼", options=["AAPL", "GOOGL", "MSFT", "TSLA", "AMZN"]
            )
        elif data_source == "APIæ¥å£":
            api_endpoint = st.text_input(
                "APIç«¯é»", placeholder="https://api.example.com/data"
            )
            api_key = st.text_input("APIå¯†é‘°", type="password")
        else:  # è³‡æ–™åº«
            db_query = st.text_area(
                "SQLæŸ¥è©¢", placeholder="SELECT * FROM stock_data WHERE..."
            )

    with col2:
        # æ¨è«–è¨­å®š
        update_interval = st.selectbox(
            "æ›´æ–°é–“éš”", options=[1, 5, 10, 30, 60], format_func=lambda x: f"{x} ç§’"
        )

        auto_start = st.checkbox("è‡ªå‹•é–‹å§‹", value=False)
        show_charts = st.checkbox("é¡¯ç¤ºåœ–è¡¨", value=True)

    # æ§åˆ¶æŒ‰éˆ•
    col1, col2, col3 = st.columns(3)

    with col1:
        start_button = st.button("ğŸš€ é–‹å§‹å¯¦æ™‚æ¨è«–", type="primary")

    with col2:
        stop_button = st.button("â¹ï¸ åœæ­¢æ¨è«–")

    with col3:
        clear_button = st.button("ğŸ—‘ï¸ æ¸…é™¤æ•¸æ“š")

    # åˆå§‹åŒ– session state
    if "realtime_running" not in st.session_state:
        st.session_state.realtime_running = False
    if "realtime_data" not in st.session_state:
        st.session_state.realtime_data = []

    # è™•ç†æŒ‰éˆ•äº‹ä»¶
    if start_button or auto_start:
        st.session_state.realtime_running = True

    if stop_button:
        st.session_state.realtime_running = False

    if clear_button:
        st.session_state.realtime_data = []

    # å¯¦æ™‚æ¨è«–é‚è¼¯
    if st.session_state.realtime_running:
        st.info("ğŸ”„ å¯¦æ™‚æ¨è«–é€²è¡Œä¸­...")

        # å‰µå»ºä½”ä½ç¬¦
        status_placeholder = st.empty()
        chart_placeholder = st.empty()
        data_placeholder = st.empty()

        try:
            # æ¨¡æ“¬ç²å–å¯¦æ™‚æ•¸æ“š
            current_time = datetime.now()

            # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
            if data_source == "æ¨¡æ“¬æ•¸æ“š":
                new_data = {
                    "timestamp": current_time,
                    "close": np.random.normal(100, 5),
                    "volume": np.random.randint(1000000, 5000000),
                    "sma_5": np.random.normal(100, 3),
                    "rsi": np.random.uniform(30, 70),
                    "macd": np.random.normal(0, 1),
                }
            else:
                # å…¶ä»–æ•¸æ“šä¾†æºçš„è™•ç†é‚è¼¯
                new_data = {
                    "timestamp": current_time,
                    "close": 100.0,
                    "volume": 1000000,
                    "sma_5": 100.0,
                    "rsi": 50.0,
                    "macd": 0.0,
                }

            # åŸ·è¡Œæ¨è«–
            inference_input = {k: v for k, v in new_data.items() if k != "timestamp"}
            result = service.run_inference(
                model_id=model["id"],
                input_data=inference_input,
                return_probabilities=True,
                return_confidence=True,
            )

            # æ·»åŠ æ™‚é–“æˆ³å’Œçµæœ
            new_data.update(result)
            st.session_state.realtime_data.append(new_data)

            # ä¿æŒæœ€è¿‘100ç­†è¨˜éŒ„
            if len(st.session_state.realtime_data) > 100:
                st.session_state.realtime_data = st.session_state.realtime_data[-100:]

            # æ›´æ–°ç‹€æ…‹é¡¯ç¤º
            with status_placeholder.container():
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    st.metric("ç•¶å‰åƒ¹æ ¼", f"{new_data['close']:.2f}")

                with col2:
                    prediction = result.get("prediction", 0)
                    st.metric("é æ¸¬å€¼", f"{prediction:.4f}")

                with col3:
                    confidence = result.get("confidence", 0)
                    st.metric("ä¿¡å¿ƒåº¦", f"{confidence:.2%}")

                with col4:
                    signal = result.get("signal", "hold")
                    signal_color = {"buy": "ğŸŸ¢", "sell": "ğŸ”´", "hold": "ğŸŸ¡"}.get(
                        signal, "âšª"
                    )
                    st.metric("ä¿¡è™Ÿ", f"{signal_color} {signal.upper()}")

            # æ›´æ–°åœ–è¡¨
            if show_charts and len(st.session_state.realtime_data) > 1:
                with chart_placeholder.container():
                    df = pd.DataFrame(st.session_state.realtime_data)

                    # å‰µå»ºå­åœ–
                    from plotly.subplots import make_subplots

                    fig = make_subplots(
                        rows=2,
                        cols=1,
                        subplot_titles=("åƒ¹æ ¼èˆ‡é æ¸¬", "ä¿¡å¿ƒåº¦"),
                        vertical_spacing=0.1,
                    )

                    # åƒ¹æ ¼ç·š
                    fig.add_trace(
                        go.Scatter(
                            x=df["timestamp"],
                            y=df["close"],
                            mode="lines",
                            name="å¯¦éš›åƒ¹æ ¼",
                            line=dict(color="blue"),
                        ),
                        row=1,
                        col=1,
                    )

                    # é æ¸¬ç·š
                    if "prediction" in df.columns:
                        fig.add_trace(
                            go.Scatter(
                                x=df["timestamp"],
                                y=df["prediction"],
                                mode="lines",
                                name="é æ¸¬å€¼",
                                line=dict(color="red", dash="dash"),
                            ),
                            row=1,
                            col=1,
                        )

                    # ä¿¡å¿ƒåº¦
                    if "confidence" in df.columns:
                        fig.add_trace(
                            go.Scatter(
                                x=df["timestamp"],
                                y=df["confidence"],
                                mode="lines",
                                name="ä¿¡å¿ƒåº¦",
                                line=dict(color="green"),
                            ),
                            row=2,
                            col=1,
                        )

                    fig.update_layout(
                        height=500, title_text="å¯¦æ™‚æ¨è«–çµæœ", showlegend=True
                    )

                    fig.update_xaxes(title_text="æ™‚é–“")
                    fig.update_yaxes(title_text="åƒ¹æ ¼", row=1, col=1)
                    fig.update_yaxes(title_text="ä¿¡å¿ƒåº¦", row=2, col=1)

                    st.plotly_chart(fig, use_container_width=True)

            # æ›´æ–°æ•¸æ“šè¡¨æ ¼
            with data_placeholder.container():
                if st.session_state.realtime_data:
                    df = pd.DataFrame(st.session_state.realtime_data)
                    st.markdown("#### ğŸ“Š å¯¦æ™‚æ•¸æ“š")
                    st.dataframe(df.tail(10), use_container_width=True)

            # è‡ªå‹•åˆ·æ–°
            import time

            time.sleep(update_interval)
            st.rerun()

        except Exception as e:
            st.error(f"å¯¦æ™‚æ¨è«–åŸ·è¡Œå¤±æ•—: {str(e)}")
            st.session_state.realtime_running = False

    else:
        # é¡¯ç¤ºæ­·å²æ•¸æ“šï¼ˆå¦‚æœæœ‰ï¼‰
        if st.session_state.realtime_data:
            st.markdown("#### ğŸ“Š æ­·å²æ•¸æ“š")
            df = pd.DataFrame(st.session_state.realtime_data)
            st.dataframe(df, use_container_width=True)

            # æä¾›ä¸‹è¼‰åŠŸèƒ½
            csv = df.to_csv(index=False)
            st.download_button(
                "ä¸‹è¼‰æ­·å²æ•¸æ“š",
                csv,
                f"realtime_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv",
            )


def show_model_interpretability():
    """é¡¯ç¤ºæ¨¡å‹è§£é‡‹æ€§åˆ†æé é¢"""
    st.subheader("æ¨¡å‹è§£é‡‹æ€§åˆ†æ")

    service = get_ai_model_service()

    # ç²å–å¯ç”¨æ¨¡å‹
    try:
        models = service.get_models()
        active_models = [
            m for m in models if m.get("status") in ["trained", "deployed"]
        ]

        if not active_models:
            st.warning("æ²’æœ‰å¯ç”¨çš„å·²è¨“ç·´æ¨¡å‹")
            return

        # æ¨¡å‹é¸æ“‡
        col1, col2 = st.columns([2, 1])

        with col1:
            model_options = [f"{m['name']} - {m['type']}" for m in active_models]
            selected_model_idx = st.selectbox(
                "é¸æ“‡è¦è§£é‡‹çš„æ¨¡å‹",
                options=range(len(model_options)),
                format_func=lambda x: model_options[x],
            )
            selected_model = active_models[selected_model_idx]

        with col2:
            st.metric("æ¨¡å‹é¡å‹", selected_model["type"])
            st.metric("æ¨¡å‹ç‹€æ…‹", selected_model["status"])

        # è§£é‡‹æ–¹æ³•é¸æ“‡
        st.markdown("### ğŸ” è§£é‡‹æ–¹æ³•è¨­å®š")

        explanation_method = st.selectbox(
            "è§£é‡‹æ–¹æ³•",
            options=["SHAP", "LIME", "ç‰¹å¾µé‡è¦æ€§", "éƒ¨åˆ†ä¾è³´åœ–", "å…¨å±€è§£é‡‹"],
            help="é¸æ“‡æ¨¡å‹è§£é‡‹æ–¹æ³•ï¼šSHAPé©åˆæ¨¹æ¨¡å‹ï¼ŒLIMEé©åˆé»‘ç›’æ¨¡å‹ï¼Œç‰¹å¾µé‡è¦æ€§é¡¯ç¤ºæ•´é«”é‡è¦æ€§",
        )

        # æ•¸æ“šé¸æ“‡
        st.markdown("### ğŸ“Š æ•¸æ“šè¨­å®š")

        col1, col2 = st.columns(2)

        with col1:
            data_source = st.selectbox(
                "æ•¸æ“šä¾†æº", options=["æ¸¬è©¦æ•¸æ“š", "ä¸Šå‚³æ•¸æ“š", "æ‰‹å‹•è¼¸å…¥"]
            )

            sample_size = st.number_input(
                "æ¨£æœ¬æ•¸é‡",
                min_value=1,
                max_value=1000,
                value=100,
                help="ç”¨æ–¼è§£é‡‹çš„æ¨£æœ¬æ•¸é‡ï¼Œæ•¸é‡è¶Šå¤šçµæœè¶Šæº–ç¢ºä½†è¨ˆç®—æ™‚é–“è¶Šé•·",
            )

        with col2:
            if explanation_method in ["SHAP", "LIME"]:
                background_samples = st.number_input(
                    "èƒŒæ™¯æ¨£æœ¬æ•¸",
                    min_value=10,
                    max_value=500,
                    value=100,
                    help="ç”¨æ–¼è¨ˆç®—åŸºæº–å€¼çš„èƒŒæ™¯æ¨£æœ¬æ•¸é‡",
                )

            if explanation_method == "éƒ¨åˆ†ä¾è³´åœ–":
                target_features = st.multiselect(
                    "ç›®æ¨™ç‰¹å¾µ",
                    options=["close", "volume", "sma_5", "rsi", "macd"],
                    default=["close", "rsi"],
                    help="é¸æ“‡è¦åˆ†æéƒ¨åˆ†ä¾è³´é—œä¿‚çš„ç‰¹å¾µ",
                )

        # è§£é‡‹åƒæ•¸è¨­å®š
        with st.expander("é«˜ç´šåƒæ•¸è¨­å®š"):
            if explanation_method == "SHAP":
                shap_explainer_type = st.selectbox(
                    "SHAPè§£é‡‹å™¨é¡å‹",
                    options=["TreeExplainer", "LinearExplainer", "KernelExplainer"],
                    help="é¸æ“‡é©åˆæ¨¡å‹é¡å‹çš„SHAPè§£é‡‹å™¨",
                )

                plot_type = st.selectbox(
                    "åœ–è¡¨é¡å‹",
                    options=[
                        "summary_plot",
                        "waterfall",
                        "force_plot",
                        "dependence_plot",
                    ],
                )

            elif explanation_method == "LIME":
                lime_mode = st.selectbox(
                    "LIMEæ¨¡å¼", options=["tabular", "text", "image"]
                )

                num_features = st.number_input(
                    "è§£é‡‹ç‰¹å¾µæ•¸", min_value=1, max_value=20, value=5
                )

        # åŸ·è¡Œè§£é‡‹åˆ†æ
        if st.button("ğŸ” åŸ·è¡Œè§£é‡‹åˆ†æ", type="primary"):
            try:
                with st.spinner("æ­£åœ¨åŸ·è¡Œæ¨¡å‹è§£é‡‹åˆ†æ..."):

                    # æº–å‚™è§£é‡‹åƒæ•¸
                    explanation_params = {
                        "method": explanation_method,
                        "sample_size": sample_size,
                    }

                    if explanation_method in ["SHAP", "LIME"]:
                        explanation_params["background_samples"] = background_samples

                    if explanation_method == "SHAP":
                        explanation_params["explainer_type"] = shap_explainer_type
                        explanation_params["plot_type"] = plot_type

                    elif explanation_method == "LIME":
                        explanation_params["mode"] = lime_mode
                        explanation_params["num_features"] = num_features

                    elif explanation_method == "éƒ¨åˆ†ä¾è³´åœ–":
                        explanation_params["target_features"] = target_features

                    # åŸ·è¡Œè§£é‡‹åˆ†æ
                    if explanation_method == "å…¨å±€è§£é‡‹":
                        explanation_result = service.generate_global_explanation(
                            model_id=selected_model["id"], sample_size=sample_size
                        )
                    else:
                        # ç”Ÿæˆæ¨£æœ¬æ•¸æ“šç”¨æ–¼è§£é‡‹
                        sample_data = generate_sample_data(selected_model, sample_size)

                        explanation_result = service.explain_prediction(
                            model_id=selected_model["id"],
                            input_data=sample_data,
                            method=explanation_method,
                            **explanation_params,
                        )

                    # é¡¯ç¤ºè§£é‡‹çµæœ
                    if explanation_result and "error" not in explanation_result:
                        st.success("è§£é‡‹åˆ†æå®Œæˆï¼")

                        # é¡¯ç¤ºè§£é‡‹çµæœ
                        show_explanation_results(explanation_result, explanation_method)

                        # æä¾›ä¸‹è¼‰åŠŸèƒ½
                        result_json = json.dumps(
                            explanation_result, indent=2, default=str
                        )
                        st.download_button(
                            "ä¸‹è¼‰è§£é‡‹çµæœ",
                            result_json,
                            f"explanation_{explanation_method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            "application/json",
                        )

                    else:
                        error_msg = (
                            explanation_result.get("error", "æœªçŸ¥éŒ¯èª¤")
                            if explanation_result
                            else "è§£é‡‹åˆ†æå¤±æ•—"
                        )
                        st.error(f"è§£é‡‹åˆ†æå¤±æ•—: {error_msg}")

            except Exception as e:
                st.error(f"åŸ·è¡Œè§£é‡‹åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

        # é¡¯ç¤ºè§£é‡‹æ–¹æ³•èªªæ˜
        show_explanation_method_info(explanation_method)

    except Exception as e:
        st.error(f"è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


def generate_sample_data(model: Dict, sample_size: int) -> Dict:
    """ç”Ÿæˆç”¨æ–¼è§£é‡‹çš„æ¨£æœ¬æ•¸æ“š"""

    features = model.get("features", [])
    if isinstance(features, str):
        try:
            features = json.loads(features)
        except:
            features = ["close", "volume", "sma_5", "rsi", "macd"]

    # ç”Ÿæˆéš¨æ©Ÿæ¨£æœ¬æ•¸æ“š
    sample_data = {}

    for feature in features:
        if feature in ["close", "open", "high", "low"]:
            sample_data[feature] = np.random.normal(100, 10, sample_size).tolist()
        elif feature == "volume":
            sample_data[feature] = np.random.randint(
                1000000, 10000000, sample_size
            ).tolist()
        elif "sma" in feature or "ema" in feature:
            sample_data[feature] = np.random.normal(100, 8, sample_size).tolist()
        elif feature == "rsi":
            sample_data[feature] = np.random.uniform(20, 80, sample_size).tolist()
        elif feature == "macd":
            sample_data[feature] = np.random.normal(0, 2, sample_size).tolist()
        else:
            sample_data[feature] = np.random.normal(0, 1, sample_size).tolist()

    return sample_data


def show_explanation_results(explanation_result: Dict, method: str):
    """é¡¯ç¤ºè§£é‡‹åˆ†æçµæœ"""

    if method == "SHAP":
        st.markdown("#### ğŸ¯ SHAP è§£é‡‹çµæœ")

        # ç‰¹å¾µé‡è¦æ€§
        if explanation_result.get("feature_importance"):
            st.markdown("##### ç‰¹å¾µé‡è¦æ€§")
            show_feature_importance(explanation_result["feature_importance"])

        # SHAPå€¼åˆ†ä½ˆ
        if explanation_result.get("shap_values"):
            st.markdown("##### SHAPå€¼åˆ†ä½ˆ")
            shap_data = explanation_result["shap_values"]

            # å‰µå»ºSHAPå€¼çš„ç®±ç·šåœ–
            if isinstance(shap_data, dict):
                features = list(shap_data.keys())
                values = [shap_data[f] for f in features]

                fig = go.Figure()
                for i, (feature, vals) in enumerate(zip(features, values)):
                    fig.add_trace(go.Box(y=vals, name=feature, boxpoints="outliers"))

                fig.update_layout(
                    title="SHAPå€¼åˆ†ä½ˆ",
                    xaxis_title="ç‰¹å¾µ",
                    yaxis_title="SHAPå€¼",
                    height=400,
                )

                st.plotly_chart(fig, use_container_width=True)

    elif method == "LIME":
        st.markdown("#### ğŸ‹ LIME è§£é‡‹çµæœ")

        # å±€éƒ¨è§£é‡‹
        if explanation_result.get("local_explanation"):
            st.markdown("##### å±€éƒ¨è§£é‡‹")
            show_model_explanation_analysis(explanation_result)

        # ç‰¹å¾µè²¢ç»
        if explanation_result.get("feature_contributions"):
            st.markdown("##### ç‰¹å¾µè²¢ç»åˆ†æ")
            contributions = explanation_result["feature_contributions"]

            # å‰µå»ºç‰¹å¾µè²¢ç»åœ–
            features = list(contributions.keys())
            values = list(contributions.values())
            colors = ["red" if v < 0 else "green" for v in values]

            fig = go.Figure(
                go.Bar(
                    x=values,
                    y=features,
                    orientation="h",
                    marker_color=colors,
                    text=[f"{v:.3f}" for v in values],
                    textposition="auto",
                )
            )

            fig.update_layout(
                title="LIME ç‰¹å¾µè²¢ç»",
                xaxis_title="è²¢ç»å€¼",
                yaxis_title="ç‰¹å¾µ",
                height=max(400, len(features) * 25),
            )

            st.plotly_chart(fig, use_container_width=True)

    elif method == "ç‰¹å¾µé‡è¦æ€§":
        st.markdown("#### ğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ")

        if explanation_result.get("feature_importance"):
            show_feature_importance(explanation_result["feature_importance"])

        # ç‰¹å¾µçµ±è¨ˆ
        if explanation_result.get("feature_statistics"):
            st.markdown("##### ç‰¹å¾µçµ±è¨ˆè³‡è¨Š")
            stats_df = pd.DataFrame(explanation_result["feature_statistics"]).T
            st.dataframe(stats_df, use_container_width=True)

    elif method == "éƒ¨åˆ†ä¾è³´åœ–":
        st.markdown("#### ğŸ“ˆ éƒ¨åˆ†ä¾è³´åœ–åˆ†æ")

        if explanation_result.get("partial_dependence"):
            pd_data = explanation_result["partial_dependence"]

            for feature, data in pd_data.items():
                st.markdown(f"##### {feature} çš„éƒ¨åˆ†ä¾è³´é—œä¿‚")

                if "values" in data and "predictions" in data:
                    fig = go.Figure()

                    fig.add_trace(
                        go.Scatter(
                            x=data["values"],
                            y=data["predictions"],
                            mode="lines+markers",
                            name=f"{feature} éƒ¨åˆ†ä¾è³´",
                            line=dict(color="blue", width=2),
                        )
                    )

                    fig.update_layout(
                        title=f"{feature} éƒ¨åˆ†ä¾è³´åœ–",
                        xaxis_title=feature,
                        yaxis_title="é æ¸¬å€¼",
                        height=400,
                    )

                    st.plotly_chart(fig, use_container_width=True)

    elif method == "å…¨å±€è§£é‡‹":
        st.markdown("#### ğŸŒ å…¨å±€æ¨¡å‹è§£é‡‹")

        # æ¨¡å‹è¡Œç‚ºåˆ†æ
        if explanation_result.get("model_behavior"):
            behavior = explanation_result["model_behavior"]

            col1, col2, col3 = st.columns(3)

            with col1:
                if behavior.get("prediction_range"):
                    pred_range = behavior["prediction_range"]
                    st.metric("é æ¸¬ç¯„åœ", f"[{pred_range[0]:.3f}, {pred_range[1]:.3f}]")

            with col2:
                if behavior.get("confidence_distribution"):
                    conf_dist = behavior["confidence_distribution"]
                    st.metric("å¹³å‡ä¿¡å¿ƒåº¦", f"{conf_dist.get('mean', 0):.2%}")

            with col3:
                if behavior.get("feature_sensitivity"):
                    sensitivity = behavior["feature_sensitivity"]
                    most_sensitive = max(sensitivity.items(), key=lambda x: x[1])[0]
                    st.metric("æœ€æ•æ„Ÿç‰¹å¾µ", most_sensitive)

        # ç‰¹å¾µæ•æ„Ÿæ€§åˆ†æ
        if explanation_result.get("model_behavior", {}).get("feature_sensitivity"):
            st.markdown("##### ç‰¹å¾µæ•æ„Ÿæ€§åˆ†æ")
            sensitivity = explanation_result["model_behavior"]["feature_sensitivity"]
            show_feature_importance(sensitivity, top_n=15)

    # é¡¯ç¤ºåŸå§‹æ•¸æ“š
    with st.expander("åŸå§‹è§£é‡‹æ•¸æ“š"):
        st.json(explanation_result)


def show_explanation_method_info(method: str):
    """é¡¯ç¤ºè§£é‡‹æ–¹æ³•çš„èªªæ˜è³‡è¨Š"""

    st.markdown("---")
    st.markdown("### ğŸ“š è§£é‡‹æ–¹æ³•èªªæ˜")

    if method == "SHAP":
        st.markdown(
            """
        **SHAP (SHapley Additive exPlanations)**

        SHAPæ˜¯ä¸€ç¨®åŸºæ–¼åšå¼ˆè«–çš„æ¨¡å‹è§£é‡‹æ–¹æ³•ï¼Œèƒ½å¤ ç‚ºæ¯å€‹ç‰¹å¾µåˆ†é…ä¸€å€‹é‡è¦æ€§å€¼ã€‚

        **å„ªé»ï¼š**
        - æä¾›ä¸€è‡´ä¸”å…¬å¹³çš„ç‰¹å¾µé‡è¦æ€§åˆ†é…
        - æ”¯æ´å¤šç¨®æ¨¡å‹é¡å‹
        - å…·æœ‰è‰¯å¥½çš„æ•¸å­¸ç†è«–åŸºç¤

        **é©ç”¨å ´æ™¯ï¼š**
        - æ¨¹æ¨¡å‹ï¼ˆRandom Forest, XGBoost, LightGBMï¼‰
        - ç·šæ€§æ¨¡å‹
        - æ·±åº¦å­¸ç¿’æ¨¡å‹

        **è§£é‡‹çµæœï¼š**
        - ç‰¹å¾µé‡è¦æ€§æ’åº
        - æ¯å€‹æ¨£æœ¬çš„ç‰¹å¾µè²¢ç»
        - å…¨å±€å’Œå±€éƒ¨è§£é‡‹
        """
        )

    elif method == "LIME":
        st.markdown(
            """
        **LIME (Local Interpretable Model-agnostic Explanations)**

        LIMEé€šéåœ¨å±€éƒ¨å€åŸŸæ“¬åˆç°¡å–®æ¨¡å‹ä¾†è§£é‡‹è¤‡é›œæ¨¡å‹çš„é æ¸¬çµæœã€‚

        **å„ªé»ï¼š**
        - æ¨¡å‹ç„¡é—œï¼Œé©ç”¨æ–¼ä»»ä½•æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
        - æä¾›å±€éƒ¨è§£é‡‹ï¼Œæ˜“æ–¼ç†è§£
        - æ”¯æ´å¤šç¨®æ•¸æ“šé¡å‹ï¼ˆè¡¨æ ¼ã€æ–‡æœ¬ã€åœ–åƒï¼‰

        **é©ç”¨å ´æ™¯ï¼š**
        - é»‘ç›’æ¨¡å‹è§£é‡‹
        - æ·±åº¦å­¸ç¿’æ¨¡å‹
        - è¤‡é›œé›†æˆæ¨¡å‹

        **è§£é‡‹çµæœï¼š**
        - å±€éƒ¨ç‰¹å¾µé‡è¦æ€§
        - ç‰¹å¾µå°é æ¸¬çš„æ­£è² å½±éŸ¿
        - æ±ºç­–é‚Šç•Œå¯è¦–åŒ–
        """
        )

    elif method == "ç‰¹å¾µé‡è¦æ€§":
        st.markdown(
            """
        **ç‰¹å¾µé‡è¦æ€§åˆ†æ**

        ç›´æ¥å¾æ¨¡å‹ä¸­æå–ç‰¹å¾µé‡è¦æ€§åˆ†æ•¸ï¼Œé¡¯ç¤ºå„ç‰¹å¾µå°æ¨¡å‹é æ¸¬çš„æ•´é«”è²¢ç»ã€‚

        **å„ªé»ï¼š**
        - è¨ˆç®—å¿«é€Ÿï¼Œè³‡æºæ¶ˆè€—å°‘
        - æä¾›å…¨å±€è¦–è§’çš„ç‰¹å¾µé‡è¦æ€§
        - æ˜“æ–¼ç†è§£å’Œè§£é‡‹

        **é©ç”¨å ´æ™¯ï¼š**
        - æ¨¹æ¨¡å‹ï¼ˆå…§å»ºç‰¹å¾µé‡è¦æ€§ï¼‰
        - ç·šæ€§æ¨¡å‹ï¼ˆä¿‚æ•¸é‡è¦æ€§ï¼‰
        - ç‰¹å¾µé¸æ“‡å’Œé™ç¶­

        **è§£é‡‹çµæœï¼š**
        - ç‰¹å¾µé‡è¦æ€§æ’åº
        - é‡è¦æ€§åˆ†æ•¸åˆ†ä½ˆ
        - ç‰¹å¾µçµ±è¨ˆè³‡è¨Š
        """
        )

    elif method == "éƒ¨åˆ†ä¾è³´åœ–":
        st.markdown(
            """
        **éƒ¨åˆ†ä¾è³´åœ– (Partial Dependence Plot)**

        é¡¯ç¤ºç‰¹å®šç‰¹å¾µå°æ¨¡å‹é æ¸¬çµæœçš„é‚Šéš›æ•ˆæ‡‰ï¼Œä¿æŒå…¶ä»–ç‰¹å¾µä¸è®Šã€‚

        **å„ªé»ï¼š**
        - ç›´è§€é¡¯ç¤ºç‰¹å¾µèˆ‡é æ¸¬çš„é—œä¿‚
        - èƒ½å¤ ç™¼ç¾éç·šæ€§é—œä¿‚
        - å¹«åŠ©ç†è§£ç‰¹å¾µçš„å½±éŸ¿æ¨¡å¼

        **é©ç”¨å ´æ™¯ï¼š**
        - ç†è§£ç‰¹å¾µèˆ‡ç›®æ¨™çš„é—œä¿‚
        - ç™¼ç¾ç‰¹å¾µçš„æœ€ä½³å–å€¼ç¯„åœ
        - æ¨¡å‹è¡Œç‚ºåˆ†æ

        **è§£é‡‹çµæœï¼š**
        - ç‰¹å¾µå€¼èˆ‡é æ¸¬å€¼çš„é—œä¿‚æ›²ç·š
        - ç‰¹å¾µçš„å½±éŸ¿è¶¨å‹¢
        - æœ€ä½³ç‰¹å¾µå–å€¼å€é–“
        """
        )

    elif method == "å…¨å±€è§£é‡‹":
        st.markdown(
            """
        **å…¨å±€æ¨¡å‹è§£é‡‹**

        å¾æ•´é«”è§’åº¦åˆ†ææ¨¡å‹çš„è¡Œç‚ºæ¨¡å¼å’Œæ±ºç­–é‚è¼¯ã€‚

        **å„ªé»ï¼š**
        - æä¾›æ¨¡å‹æ•´é«”è¡Œç‚ºçš„æ´å¯Ÿ
        - å¹«åŠ©ç†è§£æ¨¡å‹çš„æ±ºç­–é‚è¼¯
        - æ”¯æ´æ¨¡å‹é©—è­‰å’Œæ”¹é€²

        **é©ç”¨å ´æ™¯ï¼š**
        - æ¨¡å‹è¡Œç‚ºåˆ†æ
        - æ¨¡å‹é©—è­‰å’Œèª¿è©¦
        - æ¥­å‹™ç†è§£å’Œæ±ºç­–æ”¯æ´

        **è§£é‡‹çµæœï¼š**
        - æ¨¡å‹é æ¸¬ç¯„åœå’Œåˆ†ä½ˆ
        - ç‰¹å¾µæ•æ„Ÿæ€§åˆ†æ
        - æ¨¡å‹ä¿¡å¿ƒåº¦åˆ†ä½ˆ
        - æ±ºç­–æ¨¡å¼ç¸½çµ
        """
        )

    # ä½¿ç”¨å»ºè­°
    st.markdown("### ğŸ’¡ ä½¿ç”¨å»ºè­°")

    if method in ["SHAP", "LIME"]:
        st.info(
            """
        **è¨ˆç®—è³‡æºè€ƒé‡ï¼š**
        - æ¨£æœ¬æ•¸é‡æœƒå½±éŸ¿è¨ˆç®—æ™‚é–“ï¼Œå»ºè­°å¾å°æ¨£æœ¬é–‹å§‹æ¸¬è©¦
        - è¤‡é›œæ¨¡å‹çš„è§£é‡‹è¨ˆç®—è¼ƒè€—æ™‚ï¼Œè«‹è€å¿ƒç­‰å¾…
        - å¯ä»¥å…ˆä½¿ç”¨ç‰¹å¾µé‡è¦æ€§é€²è¡Œå¿«é€Ÿåˆ†æ
        """
        )

    st.warning(
        """
    **è§£é‡‹çµæœæ³¨æ„äº‹é …ï¼š**
    - è§£é‡‹çµæœåŸºæ–¼æ¨¡å‹å­¸ç¿’åˆ°çš„æ¨¡å¼ï¼Œä¸ä¸€å®šåæ˜ çœŸå¯¦çš„å› æœé—œä¿‚
    - ä¸åŒè§£é‡‹æ–¹æ³•å¯èƒ½çµ¦å‡ºä¸åŒçš„çµæœï¼Œå»ºè­°çµåˆå¤šç¨®æ–¹æ³•åˆ†æ
    - è§£é‡‹çµæœæ‡‰è©²çµåˆæ¥­å‹™çŸ¥è­˜é€²è¡Œé©—è­‰å’Œç†è§£
    """
    )
