"""
AI æ¨¡å‹æ¨è«–æ¨¡çµ„

æ­¤æ¨¡çµ„æä¾›æ¨¡å‹æ¨è«–çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å–®ä¸€æ¨£æœ¬æ¨è«–
- æ‰¹é‡æ¨è«–
- å¯¦æ™‚æ¨è«–
- æ¨è«–çµæœåˆ†æ
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Any

from .base import get_ai_model_service, safe_strftime, create_info_card


def show_model_inference():
    """é¡¯ç¤ºæ¨¡å‹æ¨è«–é é¢"""
    st.header("ğŸ”® æ¨¡å‹æ¨è«–")
    
    # ç²å–å¯ç”¨æ¨¡å‹
    service = get_ai_model_service()
    models = service.get_models()
    active_models = [m for m in models if m.get('status') == 'active']
    
    if not active_models:
        st.warning("âš ï¸ æ²’æœ‰å¯ç”¨çš„æ´»èºæ¨¡å‹")
        return
    
    # æ¨¡å‹é¸æ“‡
    model_options = {f"{m['name']} ({m['id']})": m for m in active_models}
    selected_model_key = st.selectbox("é¸æ“‡æ¨¡å‹", options=list(model_options.keys()))
    selected_model = model_options[selected_model_key]
    
    # æ¨è«–æ¨¡å¼é¸æ“‡
    inference_modes = {
        "å–®ä¸€æ¨£æœ¬æ¨è«–": "single",
        "æ‰¹é‡æ¨è«–": "batch",
        "å¯¦æ™‚æ¨è«–": "realtime"
    }
    
    selected_mode = st.selectbox("æ¨è«–æ¨¡å¼", options=list(inference_modes.keys()))
    mode_key = inference_modes[selected_mode]
    
    # æ ¹æ“šæ¨¡å¼é¡¯ç¤ºå°æ‡‰ä»‹é¢
    if mode_key == "single":
        show_single_inference(selected_model, service)
    elif mode_key == "batch":
        show_batch_inference(selected_model, service)
    elif mode_key == "realtime":
        show_realtime_inference(selected_model, service)


def show_single_inference(model: Dict, service):
    """é¡¯ç¤ºå–®ä¸€æ¨£æœ¬æ¨è«–ä»‹é¢"""
    st.subheader("ğŸ¯ å–®ä¸€æ¨£æœ¬æ¨è«–")
    
    create_info_card(
        "å–®ä¸€æ¨£æœ¬æ¨è«–",
        "è¼¸å…¥å–®ä¸€æ¨£æœ¬çš„ç‰¹å¾µå€¼ï¼Œç²å¾—æ¨¡å‹é æ¸¬çµæœå’Œç½®ä¿¡åº¦ã€‚",
        "ğŸ¯"
    )
    
    # ç‰¹å¾µè¼¸å…¥è¡¨å–®
    with st.form("single_inference_form"):
        st.write("**è¼¸å…¥ç‰¹å¾µå€¼ï¼š**")
        
        # æ¨¡æ“¬ç‰¹å¾µæ¬„ä½
        features = ['price', 'volume', 'ma_5', 'ma_20', 'rsi', 'macd']
        feature_values = {}
        
        col1, col2 = st.columns(2)
        
        for i, feature in enumerate(features):
            with col1 if i % 2 == 0 else col2:
                feature_values[feature] = st.number_input(
                    f"{feature}",
                    value=0.0,
                    format="%.4f"
                )
        
        # æ¨è«–æŒ‰éˆ•
        submitted = st.form_submit_button("ğŸ”® é–‹å§‹æ¨è«–", use_container_width=True)
        
        if submitted:
            _perform_single_inference(model, feature_values, service)


def show_batch_inference(model: Dict, service):
    """é¡¯ç¤ºæ‰¹é‡æ¨è«–ä»‹é¢"""
    st.subheader("ğŸ“Š æ‰¹é‡æ¨è«–")
    
    create_info_card(
        "æ‰¹é‡æ¨è«–",
        "ä¸Šå‚³åŒ…å«å¤šå€‹æ¨£æœ¬çš„æª”æ¡ˆï¼Œé€²è¡Œæ‰¹é‡é æ¸¬ä¸¦ä¸‹è¼‰çµæœã€‚",
        "ğŸ“Š"
    )
    
    # æª”æ¡ˆä¸Šå‚³
    uploaded_file = st.file_uploader(
        "ä¸Šå‚³è³‡æ–™æª”æ¡ˆ",
        type=['csv', 'xlsx'],
        help="æª”æ¡ˆæ‡‰åŒ…å«æ¨¡å‹æ‰€éœ€çš„ç‰¹å¾µæ¬„ä½"
    )
    
    if uploaded_file is not None:
        try:
            # è®€å–æª”æ¡ˆ
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            st.write("**è³‡æ–™é è¦½ï¼š**")
            st.dataframe(df.head(), use_container_width=True)
            
            # æ‰¹é‡æ¨è«–è¨­å®š
            col1, col2 = st.columns(2)
            
            with col1:
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å°", min_value=1, max_value=1000, value=100)
                include_confidence = st.checkbox("åŒ…å«ç½®ä¿¡åº¦", value=True)
            
            with col2:
                output_format = st.selectbox("è¼¸å‡ºæ ¼å¼", ["CSV", "Excel", "JSON"])
                add_timestamp = st.checkbox("æ·»åŠ æ™‚é–“æˆ³", value=True)
            
            # é–‹å§‹æ‰¹é‡æ¨è«–
            if st.button("ğŸš€ é–‹å§‹æ‰¹é‡æ¨è«–", use_container_width=True):
                _perform_batch_inference(model, df, batch_size, include_confidence, 
                                       output_format, add_timestamp, service)
                
        except Exception as e:
            st.error(f"âŒ æª”æ¡ˆè®€å–å¤±æ•—: {e}")


def show_realtime_inference(model: Dict, service):
    """é¡¯ç¤ºå¯¦æ™‚æ¨è«–ä»‹é¢"""
    st.subheader("âš¡ å¯¦æ™‚æ¨è«–")
    
    create_info_card(
        "å¯¦æ™‚æ¨è«–",
        "é€£æ¥å³æ™‚è³‡æ–™æµï¼Œé€²è¡ŒæŒçºŒçš„æ¨¡å‹æ¨è«–å’Œç›£æ§ã€‚",
        "âš¡"
    )
    
    # å¯¦æ™‚æ¨è«–è¨­å®š
    col1, col2 = st.columns(2)
    
    with col1:
        data_source = st.selectbox(
            "è³‡æ–™ä¾†æº",
            ["æ¨¡æ“¬è³‡æ–™æµ", "WebSocket", "Kafka", "Redis Stream"]
        )
        
        update_interval = st.slider(
            "æ›´æ–°é–“éš” (ç§’)",
            min_value=1,
            max_value=60,
            value=5
        )
    
    with col2:
        max_records = st.number_input(
            "æœ€å¤§è¨˜éŒ„æ•¸",
            min_value=10,
            max_value=1000,
            value=100
        )
        
        auto_scroll = st.checkbox("è‡ªå‹•æ»¾å‹•", value=True)
    
    # æ§åˆ¶æŒ‰éˆ•
    col1, col2, col3 = st.columns(3)
    
    with col1:
        start_button = st.button("â–¶ï¸ é–‹å§‹", use_container_width=True)
    
    with col2:
        stop_button = st.button("â¸ï¸ åœæ­¢", use_container_width=True)
    
    with col3:
        clear_button = st.button("ğŸ—‘ï¸ æ¸…é™¤", use_container_width=True)
    
    # å¯¦æ™‚çµæœé¡¯ç¤ºå€åŸŸ
    if start_button:
        _start_realtime_inference(model, data_source, update_interval, max_records, service)


def _perform_single_inference(model: Dict, features: Dict, service) -> None:
    """åŸ·è¡Œå–®ä¸€æ¨£æœ¬æ¨è«–"""
    try:
        with st.spinner("æ­£åœ¨é€²è¡Œæ¨è«–..."):
            # æ¨¡æ“¬æ¨è«–çµæœ
            result = service.predict(model['id'], features)
            
            st.success("âœ… æ¨è«–å®Œæˆï¼")
            
            # é¡¯ç¤ºçµæœ
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("é æ¸¬çµæœ", f"{result['predictions'][0]:.4f}")
                st.metric("ç½®ä¿¡åº¦", f"{result['confidence']:.3f}")
            
            with col2:
                # é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§ï¼ˆæ¨¡æ“¬ï¼‰
                feature_importance = {
                    'price': 0.35,
                    'volume': 0.25,
                    'ma_5': 0.20,
                    'ma_20': 0.15,
                    'rsi': 0.03,
                    'macd': 0.02
                }
                
                st.write("**ç‰¹å¾µè²¢ç»åº¦ï¼š**")
                for feature, importance in feature_importance.items():
                    st.write(f"- {feature}: {importance:.3f}")
            
            # æ¨è«–æ­·å²
            if 'inference_history' not in st.session_state:
                st.session_state.inference_history = []
            
            st.session_state.inference_history.append({
                'timestamp': datetime.now(),
                'model': model['name'],
                'prediction': result['predictions'][0],
                'confidence': result['confidence']
            })
            
            # é¡¯ç¤ºæ¨è«–æ­·å²
            if st.session_state.inference_history:
                st.subheader("ğŸ“ˆ æ¨è«–æ­·å²")
                history_df = pd.DataFrame(st.session_state.inference_history)
                st.dataframe(history_df, use_container_width=True)
                
    except Exception as e:
        st.error(f"âŒ æ¨è«–å¤±æ•—: {e}")


def _perform_batch_inference(model: Dict, df: pd.DataFrame, batch_size: int, 
                           include_confidence: bool, output_format: str, 
                           add_timestamp: bool, service) -> None:
    """åŸ·è¡Œæ‰¹é‡æ¨è«–"""
    try:
        with st.spinner(f"æ­£åœ¨è™•ç† {len(df)} å€‹æ¨£æœ¬..."):
            # æ¨¡æ“¬æ‰¹é‡æ¨è«–
            predictions = np.random.random(len(df))
            confidences = np.random.uniform(0.7, 0.95, len(df))
            
            # æº–å‚™çµæœè³‡æ–™æ¡†
            result_df = df.copy()
            result_df['prediction'] = predictions
            
            if include_confidence:
                result_df['confidence'] = confidences
            
            if add_timestamp:
                result_df['inference_timestamp'] = datetime.now()
            
            st.success(f"âœ… æ‰¹é‡æ¨è«–å®Œæˆï¼è™•ç†äº† {len(df)} å€‹æ¨£æœ¬")
            
            # é¡¯ç¤ºçµæœé è¦½
            st.subheader("ğŸ“Š çµæœé è¦½")
            st.dataframe(result_df.head(10), use_container_width=True)
            
            # æä¾›ä¸‹è¼‰
            _provide_download(result_df, output_format, model['name'])
            
    except Exception as e:
        st.error(f"âŒ æ‰¹é‡æ¨è«–å¤±æ•—: {e}")


def _start_realtime_inference(model: Dict, data_source: str, interval: int, 
                            max_records: int, service) -> None:
    """é–‹å§‹å¯¦æ™‚æ¨è«–"""
    st.info(f"ğŸ”„ å¯¦æ™‚æ¨è«–å·²å•Ÿå‹• - è³‡æ–™ä¾†æº: {data_source}")
    
    # å‰µå»ºå¯¦æ™‚è³‡æ–™é¡¯ç¤ºå€åŸŸ
    placeholder = st.empty()
    
    # æ¨¡æ“¬å¯¦æ™‚è³‡æ–™
    if 'realtime_data' not in st.session_state:
        st.session_state.realtime_data = []
    
    # ç”Ÿæˆæ–°çš„è³‡æ–™é»
    new_data = {
        'timestamp': datetime.now(),
        'prediction': np.random.random(),
        'confidence': np.random.uniform(0.7, 0.95),
        'features': {
            'price': np.random.uniform(100, 200),
            'volume': np.random.uniform(1000, 10000)
        }
    }
    
    st.session_state.realtime_data.append(new_data)
    
    # é™åˆ¶è¨˜éŒ„æ•¸é‡
    if len(st.session_state.realtime_data) > max_records:
        st.session_state.realtime_data = st.session_state.realtime_data[-max_records:]
    
    # é¡¯ç¤ºå¯¦æ™‚è³‡æ–™
    with placeholder.container():
        st.subheader("ğŸ“Š å¯¦æ™‚æ¨è«–çµæœ")
        
        if st.session_state.realtime_data:
            df = pd.DataFrame([
                {
                    'æ™‚é–“': safe_strftime(d['timestamp']),
                    'é æ¸¬å€¼': f"{d['prediction']:.4f}",
                    'ç½®ä¿¡åº¦': f"{d['confidence']:.3f}",
                    'åƒ¹æ ¼': f"{d['features']['price']:.2f}",
                    'æˆäº¤é‡': f"{d['features']['volume']:.0f}"
                }
                for d in st.session_state.realtime_data[-10:]  # åªé¡¯ç¤ºæœ€è¿‘10ç­†
            ])
            
            st.dataframe(df, use_container_width=True)


def _provide_download(df: pd.DataFrame, format_type: str, model_name: str) -> None:
    """æä¾›çµæœä¸‹è¼‰"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"inference_results_{model_name}_{timestamp}"
    
    if format_type == "CSV":
        csv = df.to_csv(index=False)
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ CSV",
            data=csv,
            file_name=f"{filename}.csv",
            mime="text/csv"
        )
    
    elif format_type == "Excel":
        # æ³¨æ„ï¼šå¯¦éš›å¯¦ä½œéœ€è¦å®‰è£ openpyxl
        st.info("Excel ä¸‹è¼‰åŠŸèƒ½éœ€è¦å®‰è£ openpyxl å¥—ä»¶")
    
    elif format_type == "JSON":
        json_str = df.to_json(orient='records', indent=2)
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ JSON",
            data=json_str,
            file_name=f"{filename}.json",
            mime="application/json"
        )
