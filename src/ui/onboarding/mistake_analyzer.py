# -*- coding: utf-8 -*-
"""
éŒ¯èª¤åˆ†æå·¥å…·

æ­¤æ¨¡çµ„æä¾›äº¤æ˜“éŒ¯èª¤çš„æ·±åº¦åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- éŒ¯èª¤æ¨¡å¼è­˜åˆ¥
- éŒ¯èª¤æˆæœ¬è¨ˆç®—
- éŒ¯èª¤é »ç‡åˆ†æ
- æ”¹æ­£å»ºè­°ç”Ÿæˆ
- å­¸ç¿’é€²åº¦è¿½è¹¤

Author: AI Trading System
Version: 1.0.0
"""

from typing import Dict, List, Optional, Any, Tuple
import logging
from datetime import datetime, timedelta
import json

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

logger = logging.getLogger(__name__)


class MistakeAnalyzer:
    """
    éŒ¯èª¤åˆ†æå·¥å…·

    æä¾›äº¤æ˜“éŒ¯èª¤çš„æ·±åº¦åˆ†æåŠŸèƒ½ï¼Œå¹«åŠ©ç”¨æˆ¶è­˜åˆ¥ã€
    ç†è§£å’Œæ”¹æ­£å¸¸è¦‹çš„äº¤æ˜“éŒ¯èª¤ã€‚

    Attributes:
        mistake_records (List): éŒ¯èª¤è¨˜éŒ„
        mistake_categories (Dict): éŒ¯èª¤åˆ†é¡
        learning_progress (Dict): å­¸ç¿’é€²åº¦

    Example:
        >>> analyzer = MistakeAnalyzer()
        >>> analyzer.record_mistake('overtrading', 'ä»Šå¤©äº¤æ˜“éæ–¼é »ç¹', -0.02)
        >>> analysis = analyzer.analyze_mistake_patterns()
    """

    def __init__(self):
        """åˆå§‹åŒ–éŒ¯èª¤åˆ†æå·¥å…·"""
        self.mistake_records = []
        self.mistake_categories = self._initialize_mistake_categories()
        self.learning_progress = self._initialize_learning_progress()

    def _initialize_mistake_categories(self) -> Dict[str, Dict[str, Any]]:
        """
        åˆå§‹åŒ–éŒ¯èª¤åˆ†é¡

        Returns:
            Dict[str, Dict[str, Any]]: éŒ¯èª¤åˆ†é¡å­—å…¸
        """
        return {
            'overtrading': {
                'name': 'éåº¦äº¤æ˜“',
                'description': 'äº¤æ˜“é »ç‡éé«˜ï¼Œå°è‡´äº¤æ˜“æˆæœ¬å¢åŠ ',
                'severity': 'high',
                'common_causes': [
                    'ç¼ºä¹è€å¿ƒç­‰å¾…å¥½æ©Ÿæœƒ',
                    'æƒ…ç·’åŒ–æ±ºç­–',
                    'å°å¸‚å ´éåº¦åæ‡‰',
                    'æ²’æœ‰æ˜ç¢ºçš„äº¤æ˜“è¨ˆåŠƒ'
                ],
                'prevention_tips': [
                    'åˆ¶å®šæ˜ç¢ºçš„äº¤æ˜“è¦å‰‡',
                    'è¨­å®šæ¯æ—¥æœ€å¤§äº¤æ˜“æ¬¡æ•¸',
                    'è¨ˆç®—äº¤æ˜“æˆæœ¬å½±éŸ¿',
                    'åŸ¹é¤Šè€å¿ƒç­‰å¾…çš„ç¿’æ…£'
                ],
                'cost_impact': 'high'
            },
            'poor_timing': {
                'name': 'é€²å‡ºå ´æ™‚æ©Ÿä¸ç•¶',
                'description': 'åœ¨ä¸é©ç•¶çš„æ™‚æ©Ÿé€²å…¥æˆ–é€€å‡ºå¸‚å ´',
                'severity': 'high',
                'common_causes': [
                    'ç¼ºä¹æŠ€è¡“åˆ†ææŠ€èƒ½',
                    'å¿½è¦–å¸‚å ´ç’°å¢ƒ',
                    'æƒ…ç·’åŒ–æ±ºç­–',
                    'æ²’æœ‰ç­‰å¾…ç¢ºèªä¿¡è™Ÿ'
                ],
                'prevention_tips': [
                    'å­¸ç¿’æŠ€è¡“åˆ†ææ–¹æ³•',
                    'ç­‰å¾…æ˜ç¢ºçš„é€²å ´ä¿¡è™Ÿ',
                    'è€ƒæ…®å¸‚å ´æ•´é«”ç’°å¢ƒ',
                    'ä½¿ç”¨æ­¢æä¿è­·éƒ¨ä½'
                ],
                'cost_impact': 'high'
            },
            'inadequate_risk_management': {
                'name': 'é¢¨éšªç®¡ç†ä¸ç•¶',
                'description': 'æ²’æœ‰é©ç•¶æ§åˆ¶é¢¨éšªï¼Œå°è‡´éå¤§æå¤±',
                'severity': 'critical',
                'common_causes': [
                    'æ²’æœ‰è¨­å®šåœæé»',
                    'éƒ¨ä½éå¤§',
                    'æ²’æœ‰åˆ†æ•£æŠ•è³‡',
                    'å¿½è¦–é¢¨éšªè©•ä¼°'
                ],
                'prevention_tips': [
                    'å¼·åˆ¶è¨­å®šåœæé»',
                    'æ§åˆ¶å–®ä¸€éƒ¨ä½å¤§å°',
                    'åˆ†æ•£æŠ•è³‡çµ„åˆ',
                    'å®šæœŸè©•ä¼°é¢¨éšª'
                ],
                'cost_impact': 'critical'
            },
            'emotional_decisions': {
                'name': 'æƒ…ç·’åŒ–æ±ºç­–',
                'description': 'å—ææ‡¼ã€è²ªå©ªç­‰æƒ…ç·’å½±éŸ¿åšå‡ºéŒ¯èª¤æ±ºç­–',
                'severity': 'medium',
                'common_causes': [
                    'å¸‚å ´ææ…Œæ™‚è³£å‡º',
                    'å¸‚å ´ç‹‚ç†±æ™‚è²·å…¥',
                    'å ±å¾©æ€§äº¤æ˜“',
                    'éåº¦è‡ªä¿¡'
                ],
                'prevention_tips': [
                    'åˆ¶å®šäº¤æ˜“è¦å‰‡ä¸¦åš´æ ¼åŸ·è¡Œ',
                    'ä½¿ç”¨è‡ªå‹•åŒ–äº¤æ˜“',
                    'å­¸ç¿’æƒ…ç·’ç®¡ç†æŠ€å·§',
                    'å®šæœŸæª¢è¨äº¤æ˜“æ±ºç­–'
                ],
                'cost_impact': 'medium'
            },
            'insufficient_research': {
                'name': 'ç ”ç©¶ä¸è¶³',
                'description': 'æ²’æœ‰å……åˆ†ç ”ç©¶å°±é€²è¡Œäº¤æ˜“',
                'severity': 'medium',
                'common_causes': [
                    'æ€¥æ–¼é€²å ´',
                    'ä¾è³´å°é“æ¶ˆæ¯',
                    'å¿½è¦–åŸºæœ¬é¢åˆ†æ',
                    'æ²’æœ‰é©—è­‰è³‡è¨Šä¾†æº'
                ],
                'prevention_tips': [
                    'å»ºç«‹ç ”ç©¶æª¢æŸ¥æ¸…å–®',
                    'å¤šæ–¹é©—è­‰è³‡è¨Š',
                    'å­¸ç¿’åŸºæœ¬é¢åˆ†æ',
                    'ä¿æŒæ‡·ç–‘æ…‹åº¦'
                ],
                'cost_impact': 'medium'
            },
            'ignoring_stop_loss': {
                'name': 'å¿½è¦–åœæ',
                'description': 'æ²’æœ‰åŸ·è¡Œé è¨­çš„åœæç­–ç•¥',
                'severity': 'high',
                'common_causes': [
                    'å¸Œæœ›åƒ¹æ ¼åå½ˆ',
                    'ä¸é¡˜æ‰¿èªéŒ¯èª¤',
                    'ç§»å‹•åœæé»',
                    'æƒ…ç·’åŒ–å …æŒ'
                ],
                'prevention_tips': [
                    'åš´æ ¼åŸ·è¡Œåœæè¦å‰‡',
                    'ä½¿ç”¨è‡ªå‹•åœæå–®',
                    'æ¥å—å°é¡æå¤±',
                    'å­¸ç¿’èªéŒ¯çš„é‡è¦æ€§'
                ],
                'cost_impact': 'high'
            }
        }

    def _initialize_learning_progress(self) -> Dict[str, Any]:
        """
        åˆå§‹åŒ–å­¸ç¿’é€²åº¦

        Returns:
            Dict[str, Any]: å­¸ç¿’é€²åº¦å­—å…¸
        """
        return {
            'total_mistakes': 0,
            'resolved_mistakes': 0,
            'improvement_rate': 0.0,
            'last_mistake_date': None,
            'mistake_free_days': 0,
            'learning_milestones': []
        }

    def record_mistake(self, mistake_type: str, description: str,
                      cost_impact: float = 0.0, trade_id: str = '',
                      lessons_learned: str = '') -> str:
        """
        è¨˜éŒ„äº¤æ˜“éŒ¯èª¤

        Args:
            mistake_type: éŒ¯èª¤é¡å‹
            description: éŒ¯èª¤æè¿°
            cost_impact: æˆæœ¬å½±éŸ¿ï¼ˆè² æ•¸è¡¨ç¤ºæå¤±ï¼‰
            trade_id: ç›¸é—œäº¤æ˜“ID
            lessons_learned: å­¸ç¿’å¿ƒå¾—

        Returns:
            str: éŒ¯èª¤è¨˜éŒ„ID
        """
        mistake_id = f"mistake_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        mistake_record = {
            'id': mistake_id,
            'timestamp': datetime.now().isoformat(),
            'mistake_type': mistake_type,
            'description': description,
            'cost_impact': cost_impact,
            'trade_id': trade_id,
            'lessons_learned': lessons_learned,
            'severity': self.mistake_categories.get(mistake_type, {}).get('severity', 'medium'),
            'resolved': False,
            'resolution_date': None,
            'prevention_actions': [],
            'recurrence_count': 1
        }

        # æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡éŒ¯èª¤
        similar_mistakes = [
            m for m in self.mistake_records
            if m['mistake_type'] == mistake_type and
            (datetime.now() - datetime.fromisoformat(m['timestamp'])).days <= 30
        ]

        if similar_mistakes:
            mistake_record['recurrence_count'] = len(similar_mistakes) + 1

        self.mistake_records.append(mistake_record)
        self._update_learning_progress()

        logger.info("éŒ¯èª¤å·²è¨˜éŒ„: %s - %s", mistake_id, mistake_type)

        return mistake_id

    def _update_learning_progress(self) -> None:
        """æ›´æ–°å­¸ç¿’é€²åº¦"""
        total_mistakes = len(self.mistake_records)
        resolved_mistakes = len([m for m in self.mistake_records if m['resolved']])

        self.learning_progress.update({
            'total_mistakes': total_mistakes,
            'resolved_mistakes': resolved_mistakes,
            'improvement_rate': resolved_mistakes / total_mistakes if total_mistakes > 0 else 0,
            'last_mistake_date': max([m['timestamp'] for m in self.mistake_records]) if self.mistake_records else None
        })

        # è¨ˆç®—ç„¡éŒ¯èª¤å¤©æ•¸
        if self.learning_progress['last_mistake_date']:
            last_date = datetime.fromisoformat(self.learning_progress['last_mistake_date'])
            self.learning_progress['mistake_free_days'] = (datetime.now() - last_date).days

    def mark_mistake_resolved(self, mistake_id: str,
                            prevention_actions: List[str] = None) -> bool:
        """
        æ¨™è¨˜éŒ¯èª¤å·²è§£æ±º

        Args:
            mistake_id: éŒ¯èª¤è¨˜éŒ„ID
            prevention_actions: é é˜²æªæ–½

        Returns:
            bool: æ¨™è¨˜æ˜¯å¦æˆåŠŸ
        """
        for mistake in self.mistake_records:
            if mistake['id'] == mistake_id:
                mistake['resolved'] = True
                mistake['resolution_date'] = datetime.now().isoformat()
                mistake['prevention_actions'] = prevention_actions or []

                self._update_learning_progress()
                logger.info("éŒ¯èª¤å·²æ¨™è¨˜ç‚ºè§£æ±º: %s", mistake_id)
                return True

        logger.warning("æ‰¾ä¸åˆ°éŒ¯èª¤è¨˜éŒ„: %s", mistake_id)
        return False

    def analyze_mistake_patterns(self) -> Dict[str, Any]:
        """
        åˆ†æéŒ¯èª¤æ¨¡å¼

        Returns:
            Dict[str, Any]: éŒ¯èª¤æ¨¡å¼åˆ†æçµæœ
        """
        if not self.mistake_records:
            return {'message': 'å°šç„¡éŒ¯èª¤è¨˜éŒ„å¯ä¾›åˆ†æ'}

        df = pd.DataFrame(self.mistake_records)

        # éŒ¯èª¤é¡å‹åˆ†å¸ƒ
        type_distribution = df['mistake_type'].value_counts().to_dict()

        # åš´é‡ç¨‹åº¦åˆ†æ
        severity_distribution = df['severity'].value_counts().to_dict()

        # æˆæœ¬å½±éŸ¿åˆ†æ
        total_cost = df['cost_impact'].sum()
        avg_cost = df['cost_impact'].mean()

        # é‡è¤‡éŒ¯èª¤åˆ†æ
        recurring_mistakes = df[df['recurrence_count'] > 1]

        # æ™‚é–“è¶¨å‹¢åˆ†æ
        df['date'] = pd.to_datetime(df['timestamp']).dt.date
        daily_mistakes = df.groupby('date').size()

        # è§£æ±ºç‡åˆ†æ
        resolution_rate = df['resolved'].mean()

        return {
            'total_mistakes': len(df),
            'type_distribution': type_distribution,
            'severity_distribution': severity_distribution,
            'total_cost_impact': total_cost,
            'average_cost_impact': avg_cost,
            'recurring_mistakes': len(recurring_mistakes),
            'resolution_rate': resolution_rate,
            'daily_average': daily_mistakes.mean(),
            'most_common_mistake': df['mistake_type'].mode().iloc[0] if not df.empty else None,
            'improvement_suggestions': self._generate_pattern_suggestions(df)
        }

    def _generate_pattern_suggestions(self, df: pd.DataFrame) -> List[str]:
        """åŸºæ–¼æ¨¡å¼ç”Ÿæˆå»ºè­°"""
        suggestions = []

        # åˆ†ææœ€å¸¸è¦‹éŒ¯èª¤
        most_common = df['mistake_type'].value_counts()
        if len(most_common) > 0:
            top_mistake = most_common.index[0]
            count = most_common.iloc[0]
            if count >= 3:
                suggestions.append(f"æœ€å¸¸è¦‹éŒ¯èª¤æ˜¯ã€Œ{self.mistake_categories[top_mistake]['name']}ã€ï¼Œå»ºè­°é‡é»æ”¹é€²")

        # åˆ†æé‡è¤‡éŒ¯èª¤
        recurring = df[df['recurrence_count'] > 1]
        if len(recurring) > 0:
            suggestions.append("å­˜åœ¨é‡è¤‡éŒ¯èª¤ï¼Œå»ºè­°åŠ å¼·é é˜²æªæ–½çš„åŸ·è¡Œ")

        # åˆ†ææˆæœ¬å½±éŸ¿
        high_cost_mistakes = df[df['cost_impact'] < -0.05]  # æå¤±è¶…é5%
        if len(high_cost_mistakes) > 0:
            suggestions.append("å­˜åœ¨é«˜æˆæœ¬éŒ¯èª¤ï¼Œå»ºè­°åŠ å¼·é¢¨éšªæ§åˆ¶")

        # åˆ†æè§£æ±ºç‡
        resolution_rate = df['resolved'].mean()
        if resolution_rate < 0.5:
            suggestions.append("éŒ¯èª¤è§£æ±ºç‡åä½ï¼Œå»ºè­°æ›´ç©æ¥µåœ°æ¡å–æ”¹é€²æªæ–½")

        # åˆ†æéŒ¯èª¤é »ç‡
        days = (pd.to_datetime(df['timestamp']).max() - pd.to_datetime(df['timestamp']).min()).days
        if days > 0:
            frequency = len(df) / days
            if frequency > 0.5:  # æ¯å…©å¤©ä¸€å€‹éŒ¯èª¤
                suggestions.append("éŒ¯èª¤é »ç‡è¼ƒé«˜ï¼Œå»ºè­°æ”¾æ…¢äº¤æ˜“ç¯€å¥ï¼ŒåŠ å¼·å­¸ç¿’")

        return suggestions

    def generate_improvement_plan(self, mistake_type: str = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ”¹é€²è¨ˆåŠƒ

        Args:
            mistake_type: ç‰¹å®šéŒ¯èª¤é¡å‹ï¼ˆå¯é¸ï¼‰

        Returns:
            Dict[str, Any]: æ”¹é€²è¨ˆåŠƒ
        """
        if mistake_type:
            # é‡å°ç‰¹å®šéŒ¯èª¤é¡å‹çš„æ”¹é€²è¨ˆåŠƒ
            if mistake_type not in self.mistake_categories:
                return {'error': f'æœªçŸ¥çš„éŒ¯èª¤é¡å‹: {mistake_type}'}

            category = self.mistake_categories[mistake_type]

            # åˆ†æè©²é¡å‹éŒ¯èª¤çš„æ­·å²è¨˜éŒ„
            related_mistakes = [
                m for m in self.mistake_records
                if m['mistake_type'] == mistake_type
            ]

            plan = {
                'mistake_type': mistake_type,
                'mistake_name': category['name'],
                'severity': category['severity'],
                'occurrence_count': len(related_mistakes),
                'total_cost': sum(m['cost_impact'] for m in related_mistakes),
                'prevention_tips': category['prevention_tips'],
                'action_items': self._generate_action_items(mistake_type, related_mistakes),
                'timeline': self._generate_timeline(category['severity']),
                'success_metrics': self._generate_success_metrics(mistake_type)
            }

        else:
            # ç¶œåˆæ”¹é€²è¨ˆåŠƒ
            analysis = self.analyze_mistake_patterns()

            if 'message' in analysis:
                return analysis

            # è­˜åˆ¥å„ªå…ˆæ”¹é€²çš„éŒ¯èª¤é¡å‹
            priority_mistakes = self._identify_priority_mistakes()

            plan = {
                'type': 'comprehensive',
                'priority_mistakes': priority_mistakes,
                'overall_goals': [
                    'æ¸›å°‘éŒ¯èª¤é »ç‡',
                    'æé«˜éŒ¯èª¤è§£æ±ºç‡',
                    'é™ä½éŒ¯èª¤æˆæœ¬å½±éŸ¿',
                    'å»ºç«‹é é˜²æ©Ÿåˆ¶'
                ],
                'action_plan': self._generate_comprehensive_action_plan(),
                'timeline': '3å€‹æœˆ',
                'review_schedule': 'æ¯é€±æª¢è¨é€²åº¦'
            }

        return plan

    def _generate_action_items(self, mistake_type: str,
                             related_mistakes: List[Dict]) -> List[str]:
        """ç”Ÿæˆå…·é«”è¡Œå‹•é …ç›®"""
        category = self.mistake_categories[mistake_type]
        actions = []

        # åŸºæ–¼éŒ¯èª¤é¡å‹çš„é€šç”¨è¡Œå‹•
        if mistake_type == 'overtrading':
            actions.extend([
                'è¨­å®šæ¯æ—¥æœ€å¤§äº¤æ˜“æ¬¡æ•¸é™åˆ¶',
                'è¨ˆç®—ä¸¦ç›£æ§äº¤æ˜“æˆæœ¬',
                'å»ºç«‹äº¤æ˜“æ—¥èªŒè¨˜éŒ„æ±ºç­–éç¨‹',
                'å­¸ç¿’è€å¿ƒç­‰å¾…çš„æŠ€å·§'
            ])
        elif mistake_type == 'inadequate_risk_management':
            actions.extend([
                'ç‚ºæ¯ç­†äº¤æ˜“è¨­å®šåœæé»',
                'é™åˆ¶å–®ä¸€éƒ¨ä½ä¸è¶…éç¸½è³‡é‡‘çš„10%',
                'å»ºç«‹é¢¨éšªè©•ä¼°æª¢æŸ¥æ¸…å–®',
                'å®šæœŸæª¢è¦–æŠ•è³‡çµ„åˆé¢¨éšª'
            ])
        elif mistake_type == 'emotional_decisions':
            actions.extend([
                'åˆ¶å®šäº¤æ˜“è¦å‰‡ä¸¦å¯«ä¸‹ä¾†',
                'ä½¿ç”¨å†·éœæœŸåˆ¶åº¦ï¼ˆæ±ºç­–å‰ç­‰å¾…24å°æ™‚ï¼‰',
                'å­¸ç¿’å†¥æƒ³æˆ–å…¶ä»–æƒ…ç·’ç®¡ç†æŠ€å·§',
                'å°‹æ‰¾äº¤æ˜“å¤¥ä¼´äº’ç›¸ç›£ç£'
            ])

        # åŸºæ–¼æ­·å²è¨˜éŒ„çš„å€‹äººåŒ–è¡Œå‹•
        if len(related_mistakes) > 2:
            actions.append('è©²éŒ¯èª¤é‡è¤‡å‡ºç¾ï¼Œéœ€è¦æ›´åš´æ ¼çš„é é˜²æªæ–½')

        if any(m['cost_impact'] < -0.1 for m in related_mistakes):
            actions.append('è©²éŒ¯èª¤é€ æˆé‡å¤§æå¤±ï¼Œåˆ—ç‚ºæœ€é«˜å„ªå…ˆç´šæ”¹é€²é …ç›®')

        return actions

    def _generate_timeline(self, severity: str) -> str:
        """æ ¹æ“šåš´é‡ç¨‹åº¦ç”Ÿæˆæ™‚é–“è¡¨"""
        timelines = {
            'critical': 'ç«‹å³é–‹å§‹ï¼Œ1é€±å…§è¦‹æ•ˆ',
            'high': '1é€±å…§é–‹å§‹ï¼Œ2é€±å…§è¦‹æ•ˆ',
            'medium': '2é€±å…§é–‹å§‹ï¼Œ1å€‹æœˆå…§è¦‹æ•ˆ',
            'low': '1å€‹æœˆå…§é–‹å§‹ï¼Œ2å€‹æœˆå…§è¦‹æ•ˆ'
        }
        return timelines.get(severity, '1å€‹æœˆå…§é–‹å§‹')

    def _generate_success_metrics(self, mistake_type: str) -> List[str]:
        """ç”ŸæˆæˆåŠŸæŒ‡æ¨™"""
        base_metrics = [
            'è©²é¡å‹éŒ¯èª¤ç™¼ç”Ÿé »ç‡é™ä½50%',
            'éŒ¯èª¤é€ æˆçš„å¹³å‡æå¤±æ¸›å°‘30%',
            'é€£çºŒ30å¤©ç„¡æ­¤é¡éŒ¯èª¤'
        ]

        specific_metrics = {
            'overtrading': ['æ¯æ—¥äº¤æ˜“æ¬¡æ•¸ä¸è¶…é3æ¬¡', 'æœˆäº¤æ˜“æˆæœ¬ä½æ–¼ç¸½è³‡é‡‘1%'],
            'inadequate_risk_management': ['100%äº¤æ˜“è¨­å®šåœæ', 'å–®ç­†æœ€å¤§æå¤±ä¸è¶…é2%'],
            'emotional_decisions': ['æ±ºç­–å‰å†·éœæœŸåŸ·è¡Œç‡100%', 'æƒ…ç·’åŒ–äº¤æ˜“æ¬¡æ•¸ç‚ºé›¶']
        }

        return base_metrics + specific_metrics.get(mistake_type, [])

    def _identify_priority_mistakes(self) -> List[Dict[str, Any]]:
        """è­˜åˆ¥å„ªå…ˆæ”¹é€²çš„éŒ¯èª¤"""
        if not self.mistake_records:
            return []

        df = pd.DataFrame(self.mistake_records)

        # è¨ˆç®—æ¯ç¨®éŒ¯èª¤çš„å„ªå…ˆç´šåˆ†æ•¸
        mistake_priority = []

        for mistake_type in df['mistake_type'].unique():
            type_mistakes = df[df['mistake_type'] == mistake_type]

            # è¨ˆç®—å„ªå…ˆç´šåˆ†æ•¸ï¼ˆé »ç‡ + æˆæœ¬å½±éŸ¿ + åš´é‡ç¨‹åº¦ï¼‰
            frequency_score = len(type_mistakes) * 10
            cost_score = abs(type_mistakes['cost_impact'].sum()) * 100
            severity_score = {'critical': 50, 'high': 30, 'medium': 20, 'low': 10}.get(
                self.mistake_categories.get(mistake_type, {}).get('severity', 'medium'), 20
            )

            total_score = frequency_score + cost_score + severity_score

            mistake_priority.append({
                'mistake_type': mistake_type,
                'mistake_name': self.mistake_categories.get(mistake_type, {}).get('name', mistake_type),
                'frequency': len(type_mistakes),
                'total_cost': type_mistakes['cost_impact'].sum(),
                'priority_score': total_score
            })

        # æŒ‰å„ªå…ˆç´šåˆ†æ•¸æ’åº
        mistake_priority.sort(key=lambda x: x['priority_score'], reverse=True)

        return mistake_priority[:3]  # è¿”å›å‰3å€‹å„ªå…ˆç´šæœ€é«˜çš„éŒ¯èª¤

    def _generate_comprehensive_action_plan(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç¶œåˆè¡Œå‹•è¨ˆåŠƒ"""
        return [
            {
                'phase': 'ç¬¬ä¸€éšæ®µï¼ˆç¬¬1-2é€±ï¼‰',
                'focus': 'éŒ¯èª¤è­˜åˆ¥å’Œè¨˜éŒ„',
                'actions': [
                    'å»ºç«‹éŒ¯èª¤è¨˜éŒ„ç¿’æ…£',
                    'å­¸ç¿’éŒ¯èª¤åˆ†é¡æ–¹æ³•',
                    'è¨­å®šéŒ¯èª¤æé†’æ©Ÿåˆ¶',
                    'åˆ†ææ­·å²äº¤æ˜“è¨˜éŒ„'
                ]
            },
            {
                'phase': 'ç¬¬äºŒéšæ®µï¼ˆç¬¬3-6é€±ï¼‰',
                'focus': 'é é˜²æªæ–½å¯¦æ–½',
                'actions': [
                    'é‡å°é«˜å„ªå…ˆç´šéŒ¯èª¤åˆ¶å®šé é˜²æªæ–½',
                    'å»ºç«‹äº¤æ˜“æª¢æŸ¥æ¸…å–®',
                    'å¯¦æ–½é¢¨éšªæ§åˆ¶è¦å‰‡',
                    'é–‹å§‹æƒ…ç·’ç®¡ç†è¨“ç·´'
                ]
            },
            {
                'phase': 'ç¬¬ä¸‰éšæ®µï¼ˆç¬¬7-12é€±ï¼‰',
                'focus': 'ç¿’æ…£é¤Šæˆå’Œå„ªåŒ–',
                'actions': [
                    'éå›ºè‰¯å¥½çš„äº¤æ˜“ç¿’æ…£',
                    'å„ªåŒ–é é˜²æªæ–½',
                    'å®šæœŸæª¢è¨å’Œèª¿æ•´',
                    'åˆ†äº«ç¶“é©—å’Œå­¸ç¿’'
                ]
            }
        ]


def show_mistake_analyzer() -> None:
    """
    é¡¯ç¤ºéŒ¯èª¤åˆ†æå·¥å…·é é¢

    æä¾›äº¤æ˜“éŒ¯èª¤çš„æ·±åº¦åˆ†æåŠŸèƒ½ï¼Œå¹«åŠ©ç”¨æˆ¶è­˜åˆ¥ã€
    ç†è§£å’Œæ”¹æ­£å¸¸è¦‹çš„äº¤æ˜“éŒ¯èª¤ã€‚

    Side Effects:
        - åœ¨ Streamlit ç•Œé¢é¡¯ç¤ºéŒ¯èª¤åˆ†æå·¥å…·
        - æä¾›éŒ¯èª¤è¨˜éŒ„å’Œåˆ†æåŠŸèƒ½
    """
    st.title("ğŸ” éŒ¯èª¤åˆ†æå·¥å…·")
    st.markdown("æ·±å…¥åˆ†æäº¤æ˜“éŒ¯èª¤ï¼Œå¾éŒ¯èª¤ä¸­å­¸ç¿’ï¼ŒæŒçºŒæ”¹é€²ï¼")

    # åˆå§‹åŒ–éŒ¯èª¤åˆ†æå™¨
    if 'mistake_analyzer' not in st.session_state:
        st.session_state.mistake_analyzer = MistakeAnalyzer()

    analyzer = st.session_state.mistake_analyzer

    # ä¸»è¦åŠŸèƒ½å€åŸŸ
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["è¨˜éŒ„éŒ¯èª¤", "éŒ¯èª¤åˆ†æ", "æ”¹é€²è¨ˆåŠƒ", "å­¸ç¿’é€²åº¦", "éŒ¯èª¤ç™¾ç§‘"])

    with tab1:
        st.subheader("ğŸ“ è¨˜éŒ„æ–°éŒ¯èª¤")

        with st.form("mistake_form"):
            col1, col2 = st.columns(2)

            with col1:
                mistake_type = st.selectbox(
                    "éŒ¯èª¤é¡å‹",
                    list(analyzer.mistake_categories.keys()),
                    format_func=lambda x: analyzer.mistake_categories[x]['name']
                )

                cost_impact = st.number_input(
                    "æˆæœ¬å½±éŸ¿ (%)",
                    min_value=-100.0,
                    max_value=100.0,
                    value=0.0,
                    step=0.1,
                    help="è² æ•¸è¡¨ç¤ºæå¤±ï¼Œæ­£æ•¸è¡¨ç¤ºé¿å…çš„æå¤±"
                )

            with col2:
                trade_id = st.text_input("ç›¸é—œäº¤æ˜“IDï¼ˆå¯é¸ï¼‰", placeholder="ä¾‹å¦‚: trade_20231201_001")

                severity_display = analyzer.mistake_categories[mistake_type]['severity']
                st.info(f"åš´é‡ç¨‹åº¦: {severity_display}")

            description = st.text_area(
                "éŒ¯èª¤æè¿°",
                placeholder="è©³ç´°æè¿°ç™¼ç”Ÿäº†ä»€éº¼éŒ¯èª¤...",
                help="è«‹è©³ç´°èªªæ˜éŒ¯èª¤çš„å…·é«”æƒ…æ³"
            )

            lessons_learned = st.text_area(
                "å­¸ç¿’å¿ƒå¾—ï¼ˆå¯é¸ï¼‰",
                placeholder="å¾é€™å€‹éŒ¯èª¤ä¸­å­¸åˆ°äº†ä»€éº¼..."
            )

            if st.form_submit_button("è¨˜éŒ„éŒ¯èª¤"):
                if description:
                    mistake_id = analyzer.record_mistake(
                        mistake_type=mistake_type,
                        description=description,
                        cost_impact=cost_impact / 100,  # è½‰æ›ç‚ºå°æ•¸
                        trade_id=trade_id,
                        lessons_learned=lessons_learned
                    )
                    st.success(f"âœ… éŒ¯èª¤å·²è¨˜éŒ„ï¼ID: {mistake_id}")

                    # é¡¯ç¤ºç›¸é—œå»ºè­°
                    category = analyzer.mistake_categories[mistake_type]
                    st.subheader("ğŸ’¡ é é˜²å»ºè­°")
                    for tip in category['prevention_tips']:
                        st.write(f"â€¢ {tip}")
                else:
                    st.error("è«‹å¡«å¯«éŒ¯èª¤æè¿°")

    with tab2:
        st.subheader("ğŸ“Š éŒ¯èª¤æ¨¡å¼åˆ†æ")

        if analyzer.mistake_records:
            analysis = analyzer.analyze_mistake_patterns()

            # åŸºæœ¬çµ±è¨ˆ
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("ç¸½éŒ¯èª¤æ•¸", analysis['total_mistakes'])
            with col2:
                st.metric("è§£æ±ºç‡", f"{analysis['resolution_rate']:.1%}")
            with col3:
                st.metric("ç¸½æˆæœ¬å½±éŸ¿", f"{analysis['total_cost_impact']:.2%}")
            with col4:
                st.metric("é‡è¤‡éŒ¯èª¤", analysis['recurring_mistakes'])

            # éŒ¯èª¤é¡å‹åˆ†å¸ƒ
            if analysis['type_distribution']:
                st.subheader("ğŸ“ˆ éŒ¯èª¤é¡å‹åˆ†å¸ƒ")

                types = list(analysis['type_distribution'].keys())
                counts = list(analysis['type_distribution'].values())

                # è½‰æ›ç‚ºä¸­æ–‡åç¨±
                type_names = [analyzer.mistake_categories.get(t, {}).get('name', t) for t in types]

                fig = px.bar(x=type_names, y=counts, title="éŒ¯èª¤é¡å‹é »ç‡")
                fig.update_layout(xaxis_title="éŒ¯èª¤é¡å‹", yaxis_title="ç™¼ç”Ÿæ¬¡æ•¸")
                st.plotly_chart(fig, use_container_width=True)

            # åš´é‡ç¨‹åº¦åˆ†æ
            if analysis['severity_distribution']:
                st.subheader("âš ï¸ åš´é‡ç¨‹åº¦åˆ†å¸ƒ")

                severity_names = {
                    'critical': 'åš´é‡',
                    'high': 'é«˜',
                    'medium': 'ä¸­',
                    'low': 'ä½'
                }

                severities = list(analysis['severity_distribution'].keys())
                severity_counts = list(analysis['severity_distribution'].values())
                severity_labels = [severity_names.get(s, s) for s in severities]

                fig = px.pie(values=severity_counts, names=severity_labels, title="éŒ¯èª¤åš´é‡ç¨‹åº¦åˆ†å¸ƒ")
                st.plotly_chart(fig, use_container_width=True)

            # æ”¹é€²å»ºè­°
            st.subheader("ğŸ’¡ æ¨¡å¼åˆ†æå»ºè­°")
            for suggestion in analysis['improvement_suggestions']:
                st.write(f"â€¢ {suggestion}")

        else:
            st.info("å°šç„¡éŒ¯èª¤è¨˜éŒ„ï¼Œè«‹å…ˆè¨˜éŒ„ä¸€äº›éŒ¯èª¤")

    with tab3:
        st.subheader("ğŸ“‹ æ”¹é€²è¨ˆåŠƒ")

        plan_type = st.radio(
            "è¨ˆåŠƒé¡å‹",
            ["ç¶œåˆæ”¹é€²è¨ˆåŠƒ", "ç‰¹å®šéŒ¯èª¤æ”¹é€²"],
            horizontal=True
        )

        if plan_type == "ç‰¹å®šéŒ¯èª¤æ”¹é€²":
            mistake_type = st.selectbox(
                "é¸æ“‡éŒ¯èª¤é¡å‹",
                list(analyzer.mistake_categories.keys()),
                format_func=lambda x: analyzer.mistake_categories[x]['name']
            )

            if st.button("ç”Ÿæˆæ”¹é€²è¨ˆåŠƒ"):
                plan = analyzer.generate_improvement_plan(mistake_type)

                if 'error' in plan:
                    st.error(plan['error'])
                else:
                    st.write(f"**éŒ¯èª¤é¡å‹**: {plan['mistake_name']}")
                    st.write(f"**åš´é‡ç¨‹åº¦**: {plan['severity']}")
                    st.write(f"**ç™¼ç”Ÿæ¬¡æ•¸**: {plan['occurrence_count']}")
                    st.write(f"**ç¸½æˆæœ¬å½±éŸ¿**: {plan['total_cost']:.2%}")

                    st.subheader("ğŸ¯ è¡Œå‹•é …ç›®")
                    for action in plan['action_items']:
                        st.write(f"â€¢ {action}")

                    st.subheader("ğŸ“… æ™‚é–“è¡¨")
                    st.write(plan['timeline'])

                    st.subheader("ğŸ“Š æˆåŠŸæŒ‡æ¨™")
                    for metric in plan['success_metrics']:
                        st.write(f"â€¢ {metric}")

        else:
            if st.button("ç”Ÿæˆç¶œåˆæ”¹é€²è¨ˆåŠƒ"):
                plan = analyzer.generate_improvement_plan()

                if 'message' in plan:
                    st.info(plan['message'])
                else:
                    st.subheader("ğŸ¯ å„ªå…ˆæ”¹é€²éŒ¯èª¤")
                    for i, mistake in enumerate(plan['priority_mistakes'], 1):
                        st.write(f"{i}. **{mistake['mistake_name']}** - ç™¼ç”Ÿ{mistake['frequency']}æ¬¡ï¼Œæˆæœ¬å½±éŸ¿{mistake['total_cost']:.2%}")

                    st.subheader("ğŸ“‹ åˆ†éšæ®µè¡Œå‹•è¨ˆåŠƒ")
                    for phase in plan['action_plan']:
                        with st.expander(phase['phase']):
                            st.write(f"**é‡é»**: {phase['focus']}")
                            st.write("**è¡Œå‹•é …ç›®**:")
                            for action in phase['actions']:
                                st.write(f"â€¢ {action}")

    with tab4:
        st.subheader("ğŸ“ˆ å­¸ç¿’é€²åº¦")

        progress = analyzer.learning_progress

        # é€²åº¦æŒ‡æ¨™
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("ç¸½éŒ¯èª¤æ•¸", progress['total_mistakes'])
            st.metric("å·²è§£æ±ºéŒ¯èª¤", progress['resolved_mistakes'])

        with col2:
            st.metric("æ”¹é€²ç‡", f"{progress['improvement_rate']:.1%}")
            st.metric("ç„¡éŒ¯èª¤å¤©æ•¸", progress['mistake_free_days'])

        with col3:
            if progress['last_mistake_date']:
                last_date = datetime.fromisoformat(progress['last_mistake_date']).strftime('%Y-%m-%d')
                st.metric("æœ€å¾ŒéŒ¯èª¤æ—¥æœŸ", last_date)

        # é€²åº¦åœ–è¡¨
        if analyzer.mistake_records:
            df = pd.DataFrame(analyzer.mistake_records)
            df['date'] = pd.to_datetime(df['timestamp']).dt.date

            # æ¯æ—¥éŒ¯èª¤æ•¸é‡è¶¨å‹¢
            daily_mistakes = df.groupby('date').size().reset_index(name='count')

            fig = px.line(daily_mistakes, x='date', y='count', title='æ¯æ—¥éŒ¯èª¤æ•¸é‡è¶¨å‹¢')
            fig.update_layout(xaxis_title="æ—¥æœŸ", yaxis_title="éŒ¯èª¤æ•¸é‡")
            st.plotly_chart(fig, use_container_width=True)

            # ç´¯ç©è§£æ±ºéŒ¯èª¤æ•¸
            resolved_df = df[df['resolved'] == True]
            if not resolved_df.empty:
                resolved_df['resolve_date'] = pd.to_datetime(resolved_df['resolution_date']).dt.date
                cumulative_resolved = resolved_df.groupby('resolve_date').size().cumsum().reset_index(name='cumulative')

                fig2 = px.line(cumulative_resolved, x='resolve_date', y='cumulative', title='ç´¯ç©è§£æ±ºéŒ¯èª¤æ•¸')
                fig2.update_layout(xaxis_title="æ—¥æœŸ", yaxis_title="ç´¯ç©è§£æ±ºæ•¸")
                st.plotly_chart(fig2, use_container_width=True)

        # æ¨™è¨˜éŒ¯èª¤ç‚ºå·²è§£æ±º
        st.subheader("âœ… æ¨™è¨˜éŒ¯èª¤å·²è§£æ±º")

        unresolved_mistakes = [m for m in analyzer.mistake_records if not m['resolved']]

        if unresolved_mistakes:
            mistake_options = {
                f"{m['id']}: {analyzer.mistake_categories.get(m['mistake_type'], {}).get('name', m['mistake_type'])} ({m['timestamp'][:10]})": m['id']
                for m in unresolved_mistakes[-10:]  # é¡¯ç¤ºæœ€è¿‘10å€‹æœªè§£æ±ºéŒ¯èª¤
            }

            selected_mistake = st.selectbox("é¸æ“‡è¦æ¨™è¨˜çš„éŒ¯èª¤", list(mistake_options.keys()))

            prevention_actions = st.text_area(
                "é é˜²æªæ–½",
                placeholder="æè¿°æ‚¨æ¡å–äº†å“ªäº›æªæ–½ä¾†é é˜²æ­¤éŒ¯èª¤å†æ¬¡ç™¼ç”Ÿ..."
            )

            if st.button("æ¨™è¨˜ç‚ºå·²è§£æ±º"):
                if prevention_actions:
                    mistake_id = mistake_options[selected_mistake]
                    success = analyzer.mark_mistake_resolved(
                        mistake_id,
                        prevention_actions.split('\n')
                    )
                    if success:
                        st.success("âœ… éŒ¯èª¤å·²æ¨™è¨˜ç‚ºè§£æ±º")
                        st.rerun()
                    else:
                        st.error("æ¨™è¨˜å¤±æ•—")
                else:
                    st.error("è«‹æè¿°é é˜²æªæ–½")
        else:
            st.info("æ‰€æœ‰éŒ¯èª¤éƒ½å·²è§£æ±ºï¼")

    with tab5:
        st.subheader("ğŸ“š éŒ¯èª¤ç™¾ç§‘")

        st.markdown("äº†è§£å„ç¨®å¸¸è¦‹çš„äº¤æ˜“éŒ¯èª¤ï¼Œå­¸ç¿’å¦‚ä½•é é˜²ã€‚")

        for mistake_type, category in analyzer.mistake_categories.items():
            with st.expander(f"{category['name']} ({category['severity']} åš´é‡ç¨‹åº¦)"):
                st.write(f"**æè¿°**: {category['description']}")

                st.write("**å¸¸è¦‹åŸå› **:")
                for cause in category['common_causes']:
                    st.write(f"â€¢ {cause}")

                st.write("**é é˜²å»ºè­°**:")
                for tip in category['prevention_tips']:
                    st.write(f"â€¢ {tip}")

                st.write(f"**æˆæœ¬å½±éŸ¿**: {category['cost_impact']}")

                # é¡¯ç¤ºè©²é¡å‹éŒ¯èª¤çš„çµ±è¨ˆ
                related_mistakes = [m for m in analyzer.mistake_records if m['mistake_type'] == mistake_type]
                if related_mistakes:
                    st.write(f"**æ‚¨çš„è¨˜éŒ„**: ç™¼ç”Ÿ {len(related_mistakes)} æ¬¡")
                    total_cost = sum(m['cost_impact'] for m in related_mistakes)
                    st.write(f"**ç¸½æˆæœ¬å½±éŸ¿**: {total_cost:.2%}")

    # å´é‚Šæ¬„ï¼šå¿«é€Ÿçµ±è¨ˆå’Œæ“ä½œ
    with st.sidebar:
        st.subheader("ğŸ“Š å¿«é€Ÿçµ±è¨ˆ")

        if analyzer.mistake_records:
            total_mistakes = len(analyzer.mistake_records)
            resolved_mistakes = len([m for m in analyzer.mistake_records if m['resolved']])
            total_cost = sum(m['cost_impact'] for m in analyzer.mistake_records)

            st.metric("ç¸½éŒ¯èª¤æ•¸", total_mistakes)
            st.metric("å·²è§£æ±º", resolved_mistakes)
            st.metric("ç¸½æˆæœ¬å½±éŸ¿", f"{total_cost:.2%}")

            # æœ€è¿‘éŒ¯èª¤
            recent_mistakes = sorted(
                analyzer.mistake_records,
                key=lambda x: x['timestamp'],
                reverse=True
            )[:3]

            st.subheader("ğŸ•’ æœ€è¿‘éŒ¯èª¤")
            for mistake in recent_mistakes:
                mistake_name = analyzer.mistake_categories.get(
                    mistake['mistake_type'], {}
                ).get('name', mistake['mistake_type'])

                date = mistake['timestamp'][:10]
                status = "âœ…" if mistake['resolved'] else "â³"
                st.write(f"{status} {mistake_name} ({date})")

        else:
            st.info("å°šç„¡éŒ¯èª¤è¨˜éŒ„")

        # å¿«é€Ÿæ“ä½œ
        st.subheader("âš¡ å¿«é€Ÿæ“ä½œ")

        if st.button("ğŸ“¥ åŒ¯å…¥ç¯„ä¾‹éŒ¯èª¤"):
            # æ·»åŠ ä¸€äº›ç¯„ä¾‹éŒ¯èª¤è¨˜éŒ„
            example_mistakes = [
                ('overtrading', 'ä»Šå¤©é€²è¡Œäº†8æ¬¡äº¤æ˜“ï¼Œéæ–¼é »ç¹', -0.015),
                ('poor_timing', 'åœ¨è‚¡åƒ¹é«˜é»è²·å…¥ï¼Œæ²’æœ‰ç­‰å¾…å›èª¿', -0.05),
                ('inadequate_risk_management', 'æ²’æœ‰è¨­å®šåœæï¼Œæå¤±æ“´å¤§', -0.08),
                ('emotional_decisions', 'çœ‹åˆ°æ–°èææ…Œè³£å‡º', -0.03),
                ('insufficient_research', 'è½ä¿¡æœ‹å‹æ¨è–¦å°±è²·å…¥', -0.02)
            ]

            for mistake_type, description, cost in example_mistakes:
                analyzer.record_mistake(mistake_type, description, cost)

            st.success("âœ… ç¯„ä¾‹éŒ¯èª¤å·²åŒ¯å…¥")
            st.rerun()

        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰éŒ¯èª¤"):
            analyzer.mistake_records = []
            analyzer.learning_progress = analyzer._initialize_learning_progress()
            st.success("âœ… éŒ¯èª¤è¨˜éŒ„å·²æ¸…é™¤")
            st.rerun()

        # å­¸ç¿’æé†’
        st.subheader("ğŸ’¡ å­¸ç¿’æé†’")

        if analyzer.mistake_records:
            unresolved_count = len([m for m in analyzer.mistake_records if not m['resolved']])
            if unresolved_count > 0:
                st.warning(f"æ‚¨æœ‰ {unresolved_count} å€‹æœªè§£æ±ºçš„éŒ¯èª¤")

            # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡éŒ¯èª¤
            df = pd.DataFrame(analyzer.mistake_records)
            recent_df = df[pd.to_datetime(df['timestamp']) >= datetime.now() - timedelta(days=7)]

            if not recent_df.empty:
                recurring = recent_df[recent_df['recurrence_count'] > 1]
                if not recurring.empty:
                    st.error("âš ï¸ ç™¼ç¾é‡è¤‡éŒ¯èª¤ï¼Œè«‹åŠ å¼·é é˜²æªæ–½")

        else:
            st.info("é–‹å§‹è¨˜éŒ„éŒ¯èª¤ä»¥ç²å¾—å­¸ç¿’å»ºè­°")