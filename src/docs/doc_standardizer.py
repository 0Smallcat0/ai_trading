"""æ–‡æª”æ¨™æº–åŒ–å·¥å…·

æ­¤æ¨¡çµ„æä¾›æ–‡æª”çš„æ¨™æº–åŒ–å’Œç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æª”æ ¼å¼çµ±ä¸€ï¼ˆMarkdownã€UTF-8 ç·¨ç¢¼ï¼‰
- ç‰ˆæœ¬è™Ÿå’Œè®Šæ›´æ­·å²ç®¡ç†
- æŠ€è¡“æº–ç¢ºæ€§é©—è­‰
- å¤šèªè¨€ä¸€è‡´æ€§æª¢æŸ¥
- æ–‡æª”çµæ§‹å„ªåŒ–

ç¬¦åˆæ–‡æª”æ¨™æº–ï¼šTraditional Chineseã€ç‰ˆæœ¬æ§åˆ¶ã€WCAG 2.1ã€æŠ€è¡“æº–ç¢ºæ€§
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime
import hashlib

# è¨­å®šæ—¥èªŒ
logger = logging.getLogger(__name__)


class DocumentStandardizer:
    """æ–‡æª”æ¨™æº–åŒ–å™¨
    
    æä¾›æ–‡æª”çš„æ¨™æº–åŒ–å’Œç®¡ç†åŠŸèƒ½ã€‚
    """
    
    # æ–‡æª”æ¨™æº–é…ç½®
    REQUIRED_ENCODING = "utf-8"
    REQUIRED_FORMAT = "markdown"
    REQUIRED_LANGUAGE = "zh_TW"  # Traditional Chinese
    
    # å¿…é ˆä¿ç•™çš„æ–‡æª”
    PROTECTED_DOCS = [
        "README.md",
        "docs/æ–°é€²äººå“¡æŒ‡å—.md",
        "docs/ä½¿ç”¨è€…æ‰‹å†Š/",
        "docs/FAQ/",
        "docs/é–‹ç™¼è€…æŒ‡å—/",
        "docs/Todo_list.md"
    ]
    
    # æ–‡æª”ç‰ˆæœ¬æ ¼å¼
    VERSION_PATTERN = r"v\d+\.\d+(\.\d+)?"
    
    def __init__(self, project_root: Optional[str] = None):
        """åˆå§‹åŒ–æ–‡æª”æ¨™æº–åŒ–å™¨
        
        Args:
            project_root: å°ˆæ¡ˆæ ¹ç›®éŒ„è·¯å¾‘
        """
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.docs_dir = self.project_root / "docs"
        
    def analyze_documentation_structure(self) -> Dict[str, Any]:
        """åˆ†ææ–‡æª”çµæ§‹
        
        Returns:
            æ–‡æª”çµæ§‹åˆ†æçµæœ
        """
        logger.info("åˆ†ææ–‡æª”çµæ§‹...")
        
        analysis = {
            "total_files": 0,
            "markdown_files": 0,
            "non_markdown_files": [],
            "encoding_issues": [],
            "missing_versions": [],
            "duplicate_files": [],
            "structure_compliance": {},
            "protected_docs_status": {}
        }
        
        # æª¢æŸ¥å¿…é ˆä¿ç•™çš„æ–‡æª”
        for protected_doc in self.PROTECTED_DOCS:
            doc_path = self.project_root / protected_doc
            analysis["protected_docs_status"][protected_doc] = {
                "exists": doc_path.exists(),
                "path": str(doc_path),
                "is_directory": doc_path.is_dir() if doc_path.exists() else False
            }
        
        # éæ­·æ‰€æœ‰æ–‡æª”æª”æ¡ˆ
        for doc_file in self.docs_dir.rglob("*"):
            if doc_file.is_file():
                analysis["total_files"] += 1
                
                # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
                if doc_file.suffix.lower() in [".md", ".markdown"]:
                    analysis["markdown_files"] += 1
                    
                    # æª¢æŸ¥ç·¨ç¢¼
                    encoding_result = self._check_file_encoding(doc_file)
                    if not encoding_result["is_utf8"]:
                        analysis["encoding_issues"].append({
                            "file": str(doc_file.relative_to(self.project_root)),
                            "detected_encoding": encoding_result["detected_encoding"]
                        })
                    
                    # æª¢æŸ¥ç‰ˆæœ¬è³‡è¨Š
                    version_result = self._check_version_info(doc_file)
                    if not version_result["has_version"]:
                        analysis["missing_versions"].append({
                            "file": str(doc_file.relative_to(self.project_root)),
                            "reason": version_result["reason"]
                        })
                        
                else:
                    analysis["non_markdown_files"].append({
                        "file": str(doc_file.relative_to(self.project_root)),
                        "extension": doc_file.suffix
                    })
        
        # æª¢æŸ¥é‡è¤‡æª”æ¡ˆ
        analysis["duplicate_files"] = self._find_duplicate_files()
        
        # æª¢æŸ¥ç›®éŒ„çµæ§‹åˆè¦æ€§
        analysis["structure_compliance"] = self._check_structure_compliance()
        
        logger.info(f"æ–‡æª”çµæ§‹åˆ†æå®Œæˆ: {analysis}")
        return analysis
    
    def _check_file_encoding(self, file_path: Path) -> Dict[str, Any]:
        """æª¢æŸ¥æª”æ¡ˆç·¨ç¢¼
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            ç·¨ç¢¼æª¢æŸ¥çµæœ
        """
        try:
            # å˜—è©¦ç”¨ UTF-8 è®€å–
            with open(file_path, 'r', encoding='utf-8') as f:
                f.read()
            return {"is_utf8": True, "detected_encoding": "utf-8"}
        except UnicodeDecodeError:
            # å˜—è©¦æª¢æ¸¬ç·¨ç¢¼
            try:
                import chardet
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                detected = chardet.detect(raw_data)
                return {
                    "is_utf8": False,
                    "detected_encoding": detected.get("encoding", "unknown")
                }
            except ImportError:
                return {"is_utf8": False, "detected_encoding": "unknown"}
        except Exception as e:
            logger.error(f"æª¢æŸ¥æª”æ¡ˆç·¨ç¢¼å¤±æ•— {file_path}: {e}")
            return {"is_utf8": False, "detected_encoding": "error"}
    
    def _check_version_info(self, file_path: Path) -> Dict[str, Any]:
        """æª¢æŸ¥ç‰ˆæœ¬è³‡è¨Š
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            ç‰ˆæœ¬æª¢æŸ¥çµæœ
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ç‰ˆæœ¬è™Ÿ
            version_match = re.search(self.VERSION_PATTERN, content)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´æ­·å²
            has_changelog = any(keyword in content.lower() for keyword in [
                "è®Šæ›´æ­·å²", "changelog", "ç‰ˆæœ¬æ­·å²", "æ›´æ–°è¨˜éŒ„"
            ])
            
            if version_match and has_changelog:
                return {
                    "has_version": True,
                    "version": version_match.group(),
                    "has_changelog": True
                }
            elif version_match:
                return {
                    "has_version": True,
                    "version": version_match.group(),
                    "has_changelog": False,
                    "reason": "ç¼ºå°‘è®Šæ›´æ­·å²"
                }
            else:
                return {
                    "has_version": False,
                    "reason": "ç¼ºå°‘ç‰ˆæœ¬è™Ÿå’Œè®Šæ›´æ­·å²"
                }
                
        except Exception as e:
            logger.error(f"æª¢æŸ¥ç‰ˆæœ¬è³‡è¨Šå¤±æ•— {file_path}: {e}")
            return {"has_version": False, "reason": f"è®€å–å¤±æ•—: {e}"}
    
    def _find_duplicate_files(self) -> List[Dict[str, Any]]:
        """å°‹æ‰¾é‡è¤‡æª”æ¡ˆ
        
        Returns:
            é‡è¤‡æª”æ¡ˆåˆ—è¡¨
        """
        file_hashes = {}
        duplicates = []
        
        for doc_file in self.docs_dir.rglob("*.md"):
            if doc_file.is_file():
                try:
                    with open(doc_file, 'rb') as f:
                        file_hash = hashlib.md5(f.read()).hexdigest()
                    
                    if file_hash in file_hashes:
                        duplicates.append({
                            "original": str(file_hashes[file_hash].relative_to(self.project_root)),
                            "duplicate": str(doc_file.relative_to(self.project_root)),
                            "hash": file_hash
                        })
                    else:
                        file_hashes[file_hash] = doc_file
                        
                except Exception as e:
                    logger.error(f"è¨ˆç®—æª”æ¡ˆé›œæ¹Šå¤±æ•— {doc_file}: {e}")
        
        return duplicates
    
    def _check_structure_compliance(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç›®éŒ„çµæ§‹åˆè¦æ€§
        
        Returns:
            çµæ§‹åˆè¦æ€§æª¢æŸ¥çµæœ
        """
        required_structure = {
            "docs/": True,
            "docs/ä½¿ç”¨è€…æ‰‹å†Š/": True,
            "docs/FAQ/": True,
            "docs/é–‹ç™¼è€…æŒ‡å—/": True,
            "docs/modules/": False,  # å¯é¸
            "docs/å…±ç”¨å·¥å…·èªªæ˜/": False,  # å¯é¸
            "docs/Q&Aå¸¸è¦‹å•é¡Œ.md": False,  # å¯é¸
        }
        
        compliance = {}
        
        for path, required in required_structure.items():
            full_path = self.project_root / path
            exists = full_path.exists()
            
            compliance[path] = {
                "exists": exists,
                "required": required,
                "compliant": exists if required else True
            }
        
        return compliance
    
    def standardize_document(self, file_path: Path) -> Dict[str, Any]:
        """æ¨™æº–åŒ–å–®å€‹æ–‡æª”
        
        Args:
            file_path: æ–‡æª”æª”æ¡ˆè·¯å¾‘
            
        Returns:
            æ¨™æº–åŒ–çµæœ
        """
        logger.info(f"æ¨™æº–åŒ–æ–‡æª”: {file_path}")
        
        result = {
            "success": False,
            "changes_made": [],
            "errors": []
        }
        
        try:
            # è®€å–åŸå§‹å…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            modified_content = original_content
            
            # 1. æ·»åŠ ç‰ˆæœ¬è³‡è¨Šï¼ˆå¦‚æœç¼ºå°‘ï¼‰
            if not re.search(self.VERSION_PATTERN, modified_content):
                version_header = self._generate_version_header()
                modified_content = version_header + "\n\n" + modified_content
                result["changes_made"].append("æ·»åŠ ç‰ˆæœ¬è³‡è¨Š")
            
            # 2. æ¨™æº–åŒ– Markdown æ ¼å¼
            modified_content = self._standardize_markdown_format(modified_content)
            if modified_content != original_content:
                result["changes_made"].append("æ¨™æº–åŒ– Markdown æ ¼å¼")
            
            # 3. ç¢ºä¿ UTF-8 ç·¨ç¢¼
            if modified_content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(modified_content)
                result["changes_made"].append("æ›´æ–°æª”æ¡ˆå…§å®¹")
            
            result["success"] = True
            logger.info(f"æ–‡æª”æ¨™æº–åŒ–å®Œæˆ: {file_path}")
            
        except Exception as e:
            error_msg = f"æ¨™æº–åŒ–æ–‡æª”å¤±æ•—: {e}"
            result["errors"].append(error_msg)
            logger.error(error_msg)
        
        return result
    
    def _generate_version_header(self) -> str:
        """ç”Ÿæˆç‰ˆæœ¬æ¨™é ­
        
        Returns:
            ç‰ˆæœ¬æ¨™é ­å­—ä¸²
        """
        current_date = datetime.now().strftime("%Y-%m-%d")
        return f"""<!-- æ–‡æª”ç‰ˆæœ¬è³‡è¨Š -->
<!-- ç‰ˆæœ¬: v1.0 -->
<!-- æœ€å¾Œæ›´æ–°: {current_date} -->
<!-- è®Šæ›´æ­·å²:
- v1.0 ({current_date}): åˆå§‹ç‰ˆæœ¬
-->"""
    
    def _standardize_markdown_format(self, content: str) -> str:
        """æ¨™æº–åŒ– Markdown æ ¼å¼
        
        Args:
            content: åŸå§‹å…§å®¹
            
        Returns:
            æ¨™æº–åŒ–å¾Œçš„å…§å®¹
        """
        # æ¨™æº–åŒ–æ¨™é¡Œæ ¼å¼
        content = re.sub(r'^(#{1,6})\s*(.+)$', r'\1 \2', content, flags=re.MULTILINE)
        
        # æ¨™æº–åŒ–åˆ—è¡¨æ ¼å¼
        content = re.sub(r'^(\s*)-\s+(.+)$', r'\1- \2', content, flags=re.MULTILINE)
        content = re.sub(r'^(\s*)\*\s+(.+)$', r'\1- \2', content, flags=re.MULTILINE)
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºè¡Œ
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        # ç¢ºä¿æª”æ¡ˆçµå°¾æœ‰æ›è¡Œ
        if not content.endswith('\n'):
            content += '\n'
        
        return content
    
    def generate_documentation_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æª”æ¨™æº–åŒ–å ±å‘Š
        
        Returns:
            æ–‡æª”å ±å‘Š
        """
        logger.info("ç”Ÿæˆæ–‡æª”æ¨™æº–åŒ–å ±å‘Š...")
        
        # åˆ†ææ–‡æª”çµæ§‹
        structure_analysis = self.analyze_documentation_structure()
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_documentation_recommendations(structure_analysis)
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "structure_analysis": structure_analysis,
            "recommendations": recommendations,
            "compliance_score": self._calculate_compliance_score(structure_analysis)
        }
        
        # ä¿å­˜å ±å‘Š
        report_file = self.project_root / "documentation_standardization_report.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"æ–‡æª”å ±å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æª”å ±å‘Šå¤±æ•—: {e}")
        
        return report
    
    def _generate_documentation_recommendations(
        self, analysis: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆæ–‡æª”æ”¹é€²å»ºè­°
        
        Args:
            analysis: æ–‡æª”åˆ†æçµæœ
            
        Returns:
            æ”¹é€²å»ºè­°åˆ—è¡¨
        """
        recommendations = []
        
        # ç·¨ç¢¼å•é¡Œ
        if analysis["encoding_issues"]:
            recommendations.append(
                f"æœ‰ {len(analysis['encoding_issues'])} å€‹æª”æ¡ˆç·¨ç¢¼ä¸æ˜¯ UTF-8ï¼Œéœ€è¦è½‰æ›"
            )
        
        # ç‰ˆæœ¬è³‡è¨Šç¼ºå¤±
        if analysis["missing_versions"]:
            recommendations.append(
                f"æœ‰ {len(analysis['missing_versions'])} å€‹æª”æ¡ˆç¼ºå°‘ç‰ˆæœ¬è³‡è¨Šï¼Œéœ€è¦æ·»åŠ "
            )
        
        # é‡è¤‡æª”æ¡ˆ
        if analysis["duplicate_files"]:
            recommendations.append(
                f"ç™¼ç¾ {len(analysis['duplicate_files'])} çµ„é‡è¤‡æª”æ¡ˆï¼Œå»ºè­°åˆä½µæˆ–åˆªé™¤"
            )
        
        # é Markdown æª”æ¡ˆ
        if analysis["non_markdown_files"]:
            recommendations.append(
                f"æœ‰ {len(analysis['non_markdown_files'])} å€‹é Markdown æª”æ¡ˆï¼Œå»ºè­°è½‰æ›æ ¼å¼"
            )
        
        # çµæ§‹åˆè¦æ€§
        non_compliant = [
            path for path, info in analysis["structure_compliance"].items()
            if not info["compliant"]
        ]
        if non_compliant:
            recommendations.append(
                f"ç›®éŒ„çµæ§‹ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(non_compliant)}"
            )
        
        # ä¿è­·æ–‡æª”ç‹€æ…‹
        missing_protected = [
            doc for doc, status in analysis["protected_docs_status"].items()
            if not status["exists"]
        ]
        if missing_protected:
            recommendations.append(
                f"ç¼ºå°‘å¿…é ˆä¿ç•™çš„æ–‡æª”: {', '.join(missing_protected)}"
            )
        
        if not recommendations:
            recommendations.append("æ–‡æª”çµæ§‹è‰¯å¥½ï¼Œç¬¦åˆæ‰€æœ‰æ¨™æº–")
        
        return recommendations
    
    def _calculate_compliance_score(self, analysis: Dict[str, Any]) -> float:
        """è¨ˆç®—åˆè¦è©•åˆ†
        
        Args:
            analysis: æ–‡æª”åˆ†æçµæœ
            
        Returns:
            åˆè¦è©•åˆ† (0-100)
        """
        total_score = 100
        
        # ç·¨ç¢¼å•é¡Œæ‰£åˆ†
        if analysis["encoding_issues"]:
            total_score -= len(analysis["encoding_issues"]) * 5
        
        # ç‰ˆæœ¬è³‡è¨Šç¼ºå¤±æ‰£åˆ†
        if analysis["missing_versions"]:
            total_score -= len(analysis["missing_versions"]) * 3
        
        # é‡è¤‡æª”æ¡ˆæ‰£åˆ†
        if analysis["duplicate_files"]:
            total_score -= len(analysis["duplicate_files"]) * 2
        
        # çµæ§‹ä¸åˆè¦æ‰£åˆ†
        non_compliant = sum(
            1 for info in analysis["structure_compliance"].values()
            if not info["compliant"]
        )
        total_score -= non_compliant * 10
        
        return max(0, total_score)


def main():
    """ä¸»å‡½æ•¸"""
    standardizer = DocumentStandardizer()
    
    print("ğŸš€ é–‹å§‹æ–‡æª”æ¨™æº–åŒ–åˆ†æ...")
    report = standardizer.generate_documentation_report()
    
    print("\nğŸ“Š æ–‡æª”æ¨™æº–åŒ–å ±å‘Š")
    print("=" * 50)
    
    # é¡¯ç¤ºçµæ§‹åˆ†æçµæœ
    analysis = report["structure_analysis"]
    print(f"\nğŸ“ æ–‡æª”çµæ§‹åˆ†æ:")
    print(f"  ç¸½æª”æ¡ˆæ•¸: {analysis['total_files']}")
    print(f"  Markdown æª”æ¡ˆ: {analysis['markdown_files']}")
    print(f"  ç·¨ç¢¼å•é¡Œ: {len(analysis['encoding_issues'])}")
    print(f"  ç¼ºå°‘ç‰ˆæœ¬: {len(analysis['missing_versions'])}")
    print(f"  é‡è¤‡æª”æ¡ˆ: {len(analysis['duplicate_files'])}")
    
    # é¡¯ç¤ºåˆè¦è©•åˆ†
    print(f"\nğŸ“Š åˆè¦è©•åˆ†: {report['compliance_score']:.1f}/100")
    
    # é¡¯ç¤ºå»ºè­°
    print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")
    for i, recommendation in enumerate(report["recommendations"], 1):
        print(f"  {i}. {recommendation}")
    
    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: documentation_standardization_report.json")


if __name__ == "__main__":
    main()
