#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ•¸æ“šæºä¸‹è¼‰å™¨ - å°è‚¡å…è²»æ•¸æ“šæ•´åˆç³»çµ±
==================================

æ•´åˆå¤šå€‹å…è²»å°è‚¡æ•¸æ“šä¾†æºï¼Œæä¾›çµ±ä¸€çš„æ•¸æ“šç²å–æ¥å£ã€‚
æ”¯æ´æŠ€è¡“é¢ã€åŸºæœ¬é¢ã€ç±Œç¢¼é¢ã€äº‹ä»¶é¢å’Œç¸½ç¶“é¢æ•¸æ“šã€‚

åŠŸèƒ½ç‰¹é»ï¼š
- å¤šæ•¸æ“šæºæ•´åˆ (TWSE + TPEX + Yahoo Finance + æ”¿åºœé–‹æ”¾å¹³å°)
- è‡ªå­¸èƒ½åŠ›å’Œæ•¸æ“šé©—è­‰
- è‡ªå‹•é‡è©¦å’ŒéŒ¯èª¤è™•ç†
- çµ±ä¸€çš„æ•¸æ“šæ ¼å¼
- UPSERTè³‡æ–™åº«æ“ä½œ

Author: AI Trading System
Date: 2025-01-28
Version: 1.0
"""

import requests
import pandas as pd
import yfinance as yf
import time
import logging
from datetime import datetime, date, timedelta
from typing import List, Dict, Optional, Any, Union
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
import json
from functools import wraps
import numpy as np

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def handle_with_retry(max_retries: int = 3, delay: float = 2.0):
    """éŒ¯èª¤è™•ç†è£é£¾å™¨ï¼Œæ”¯æ´é‡è©¦æ©Ÿåˆ¶"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"å‡½æ•¸ {func.__name__} åŸ·è¡Œå¤±æ•—ï¼Œå·²é‡è©¦ {max_retries} æ¬¡: {e}")
                        raise e
                    else:
                        logger.warning(f"å‡½æ•¸ {func.__name__} ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}ï¼Œ{delay}ç§’å¾Œé‡è©¦")
                        time.sleep(delay)
            return None
        return wrapper
    return decorator

class MultiSourceDownloader:
    """å¤šæ•¸æ“šæºä¸‹è¼‰å™¨ä¸»é¡"""
    
    def __init__(self, db_url: str = "sqlite:///multi_source_data.db"):
        self.db_url = db_url
        self.engine = create_engine(db_url)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # åˆå§‹åŒ–å„å€‹æ•¸æ“šæºæ¨¡çµ„
        self.technical = TechnicalDataCrawler(self.session, self.engine)
        self.fundamental = FundamentalDataCrawler(self.session, self.engine)
        self.chip = ChipDataCrawler(self.session, self.engine)
        self.event = EventDataCrawler(self.session, self.engine)
        self.macro = MacroDataCrawler(self.session, self.engine)
        
        logger.info("âœ… å¤šæ•¸æ“šæºä¸‹è¼‰å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_all_data(self, symbol: str, start_date: str = None, end_date: str = None) -> Dict[str, pd.DataFrame]:
        """ç²å–æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰é¡å‹æ•¸æ“š"""
        if start_date is None:
            start_date = (date.today() - timedelta(days=30)).strftime('%Y%m%d')
        if end_date is None:
            end_date = date.today().strftime('%Y%m%d')
        
        logger.info(f"ğŸš€ é–‹å§‹ç²å– {symbol} çš„å…¨æ–¹ä½æ•¸æ“š ({start_date} - {end_date})")
        
        results = {}
        
        try:
            # æŠ€è¡“é¢æ•¸æ“š
            results['technical'] = self.technical.get_comprehensive_data(symbol, start_date, end_date)
            
            # åŸºæœ¬é¢æ•¸æ“š
            results['fundamental'] = self.fundamental.get_comprehensive_data(symbol, start_date, end_date)
            
            # ç±Œç¢¼é¢æ•¸æ“š
            results['chip'] = self.chip.get_comprehensive_data(symbol, start_date, end_date)
            
            # äº‹ä»¶é¢æ•¸æ“š
            results['event'] = self.event.get_comprehensive_data(symbol, start_date, end_date)
            
            # ç¸½ç¶“é¢æ•¸æ“š
            results['macro'] = self.macro.get_comprehensive_data(start_date, end_date)
            
            logger.info(f"âœ… {symbol} å…¨æ–¹ä½æ•¸æ“šç²å–å®Œæˆ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç²å– {symbol} æ•¸æ“šå¤±æ•—: {e}")
            return {}

class TechnicalDataCrawler:
    """æŠ€è¡“é¢æ•¸æ“šçˆ¬èŸ²"""
    
    def __init__(self, session: requests.Session, engine):
        self.session = session
        self.engine = engine
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_twse_zero_share(self, date: str = None) -> pd.DataFrame:
        """çˆ¬å–TWSEé›¶è‚¡æˆäº¤è³‡è¨Š"""
        if date is None:
            date = datetime.now().strftime('%Y%m%d')
        
        url = f"https://www.twse.com.tw/exchangeReport/TWTASU?response=csv&date={date}"
        
        try:
            logger.info(f"æ­£åœ¨ç²å– {date} é›¶è‚¡æˆäº¤è³‡è¨Š...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # è§£æCSVæ•¸æ“š
            lines = response.text.strip().split('\n')
            if len(lines) < 2:
                logger.warning(f"é›¶è‚¡æ•¸æ“šç‚ºç©º: {date}")
                return pd.DataFrame()
            
            # è·³éæ¨™é¡Œè¡Œï¼Œå¾ç¬¬äºŒè¡Œé–‹å§‹
            df = pd.read_csv(pd.io.StringIO('\n'.join(lines[1:])))
            
            if not df.empty:
                df['date'] = pd.to_datetime(date)
                df['data_source'] = 'TWSE_é›¶è‚¡'
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('zero_share_trading', self.engine, if_exists='append', index=False)
                
                logger.info(f"âœ… é›¶è‚¡æˆäº¤è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ é›¶è‚¡æˆäº¤è³‡è¨Šç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_twse_backtest_index(self, start_date: str = None, end_date: str = None) -> pd.DataFrame:
        """çˆ¬å–TWSEå›æ¸¬åŸºæº–æŒ‡æ•¸"""
        url = "https://openapi.twse.com.tw/v1/indicesReport/MI_5MINS_HIST"
        
        try:
            logger.info("æ­£åœ¨ç²å–å›æ¸¬åŸºæº–æŒ‡æ•¸...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            df = pd.DataFrame(data)
            
            if not df.empty:
                df['Date'] = pd.to_datetime(df['Date'])
                
                # æ—¥æœŸç¯©é¸
                if start_date:
                    df = df[df['Date'] >= pd.to_datetime(start_date)]
                if end_date:
                    df = df[df['Date'] <= pd.to_datetime(end_date)]
                
                df['data_source'] = 'TWSE_æŒ‡æ•¸'
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('backtest_index', self.engine, if_exists='append', index=False)
                
                logger.info(f"âœ… å›æ¸¬åŸºæº–æŒ‡æ•¸ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ å›æ¸¬åŸºæº–æŒ‡æ•¸ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_yahoo_adjusted_price(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """çˆ¬å–Yahoo Financeé‚„åŸæ¬Šå€¼è‚¡åƒ¹"""
        try:
            logger.info(f"æ­£åœ¨ç²å– {symbol} Yahoo Financeé‚„åŸè‚¡åƒ¹...")
            
            # è½‰æ›æ—¥æœŸæ ¼å¼
            start = pd.to_datetime(start_date).strftime('%Y-%m-%d')
            end = pd.to_datetime(end_date).strftime('%Y-%m-%d')
            
            # ä½¿ç”¨yfinanceç²å–èª¿æ•´å¾Œè‚¡åƒ¹
            df = yf.download(symbol, start=start, end=end, auto_adjust=True, progress=False)

            if not df.empty:
                # ä¿®å¾©å¤šå±¤æ¬„ä½çµæ§‹å•é¡Œ
                if isinstance(df.columns, pd.MultiIndex):
                    # å¦‚æœæ˜¯å¤šå±¤ç´¢å¼•ï¼Œå–ç¬¬ä¸€å±¤
                    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]

                df.reset_index(inplace=True)
                df['symbol'] = symbol
                df['data_source'] = 'Yahoo_é‚„åŸè‚¡åƒ¹'

                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('adjusted_prices', self.engine, if_exists='append', index=False)

                logger.info(f"âœ… {symbol} é‚„åŸè‚¡åƒ¹ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ {symbol} é‚„åŸè‚¡åƒ¹ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_tpex_cb_trading(self) -> pd.DataFrame:
        """çˆ¬å–TPEXå¯è½‰æ›å…¬å¸å‚µæˆäº¤è³‡è¨Š"""
        url = "https://www.tpex.org.tw/web/regular_emerging/corporateInfo/regular_bond_trading.php"
        
        try:
            logger.info("æ­£åœ¨ç²å–å¯è½‰æ›å…¬å¸å‚µæˆäº¤è³‡è¨Š...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                df = pd.read_html(str(tables[0]))[0]
                df['date'] = datetime.now().date()
                df['data_source'] = 'TPEX_å¯è½‰å‚µ'
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('cb_trading', self.engine, if_exists='append', index=False)
                
                logger.info(f"âœ… å¯è½‰æ›å…¬å¸å‚µæˆäº¤è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                logger.warning("æœªæ‰¾åˆ°å¯è½‰æ›å…¬å¸å‚µäº¤æ˜“è¡¨æ ¼")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"âŒ å¯è½‰æ›å…¬å¸å‚µæˆäº¤è³‡è¨Šç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_market_indicators(self) -> pd.DataFrame:
        """çˆ¬å–å¤§ç›¤å¸‚æ³æŒ‡æ¨™"""
        url = "https://openapi.twse.com.tw/v1/indicesReport/MI_INDEX"

        try:
            logger.info("æ­£åœ¨ç²å–å¤§ç›¤å¸‚æ³æŒ‡æ¨™...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            data = response.json()
            df = pd.DataFrame(data)

            if not df.empty:
                df['crawl_date'] = datetime.now().date()
                df['data_source'] = 'TWSE_å¤§ç›¤æŒ‡æ¨™'

                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('market_indicators', self.engine, if_exists='append', index=False)

                logger.info(f"âœ… å¤§ç›¤å¸‚æ³æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")

            return df

        except Exception as e:
            logger.error(f"âŒ å¤§ç›¤å¸‚æ³æŒ‡æ¨™ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()

    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_disposal_stocks(self) -> pd.DataFrame:
        """çˆ¬å–è™•ç½®è‚¡æ¸…å–®"""
        url = "https://www.tpex.org.tw/zh-tw/announce/market/disposal.html"

        try:
            logger.info("æ­£åœ¨ç²å–è™•ç½®è‚¡æ¸…å–®...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                df = pd.read_html(str(tables[0]))[0]
                df['crawl_date'] = datetime.now().date()
                df['data_source'] = 'TPEX_è™•ç½®è‚¡'

                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('disposal_stocks', self.engine, if_exists='append', index=False)

                logger.info(f"âœ… è™•ç½®è‚¡æ¸…å–®ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                logger.warning("æœªæ‰¾åˆ°è™•ç½®è‚¡è¡¨æ ¼")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ è™•ç½®è‚¡æ¸…å–®ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()

    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_attention_stocks(self) -> pd.DataFrame:
        """çˆ¬å–æ³¨æ„è‚¡æ¸…å–®"""
        url = "https://www.twse.com.tw/zh/page/trading/attention.html"

        try:
            logger.info("æ­£åœ¨ç²å–æ³¨æ„è‚¡æ¸…å–®...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                df = pd.read_html(str(tables[0]))[0]
                df['crawl_date'] = datetime.now().date()
                df['data_source'] = 'TWSE_æ³¨æ„è‚¡'

                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('attention_stocks', self.engine, if_exists='append', index=False)

                logger.info(f"âœ… æ³¨æ„è‚¡æ¸…å–®ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                logger.warning("æœªæ‰¾åˆ°æ³¨æ„è‚¡è¡¨æ ¼")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ æ³¨æ„è‚¡æ¸…å–®ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()

    def get_comprehensive_data(self, symbol: str, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ç²å–æŠ€è¡“é¢ç¶œåˆæ•¸æ“š"""
        logger.info(f"ğŸ“Š ç²å– {symbol} æŠ€è¡“é¢æ•¸æ“š...")

        results = {}

        # é›¶è‚¡æˆäº¤è³‡è¨Š
        results['zero_share'] = self.crawl_twse_zero_share(end_date)

        # å›æ¸¬åŸºæº–æŒ‡æ•¸
        results['backtest_index'] = self.crawl_twse_backtest_index(start_date, end_date)

        # é‚„åŸæ¬Šå€¼è‚¡åƒ¹
        results['adjusted_price'] = self.crawl_yahoo_adjusted_price(symbol, start_date, end_date)

        # å¯è½‰æ›å…¬å¸å‚µ
        results['cb_trading'] = self.crawl_tpex_cb_trading()

        # å¤§ç›¤å¸‚æ³æŒ‡æ¨™
        results['market_indicators'] = self.crawl_market_indicators()

        # è™•ç½®è‚¡æ¸…å–®
        results['disposal_stocks'] = self.crawl_disposal_stocks()

        # æ³¨æ„è‚¡æ¸…å–®
        results['attention_stocks'] = self.crawl_attention_stocks()

        # æ·»åŠ å»¶é²é¿å…è¢«å°é–
        time.sleep(3)

        return results

class FundamentalDataCrawler:
    """åŸºæœ¬é¢æ•¸æ“šçˆ¬èŸ²"""
    
    def __init__(self, session: requests.Session, engine):
        self.session = session
        self.engine = engine
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_dividend_announcement(self) -> pd.DataFrame:
        """çˆ¬å–è‘£äº‹æœƒæ±ºæ“¬è­°åˆ†é…è‚¡åˆ©å…¬å‘Š"""
        url = "https://www.twse.com.tw/zh/page/announcement/dividend.html"
        
        try:
            logger.info("æ­£åœ¨ç²å–è‚¡åˆ©å…¬å‘Š...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                df = pd.read_html(str(tables[0]))[0]
                df['crawl_date'] = datetime.now().date()
                df['data_source'] = 'TWSE_è‚¡åˆ©å…¬å‘Š'
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('dividend_announcements', self.engine, if_exists='append', index=False)
                
                logger.info(f"âœ… è‚¡åˆ©å…¬å‘Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                logger.warning("æœªæ‰¾åˆ°è‚¡åˆ©å…¬å‘Šè¡¨æ ¼")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"âŒ è‚¡åˆ©å…¬å‘Šç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_monthly_revenue(self) -> pd.DataFrame:
        """çˆ¬å–ä¸Šå¸‚æ«ƒæœˆç‡Ÿæ”¶"""
        url = "https://www.twse.com.tw/zh/page/announcement/monthly_revenue.html"
        
        try:
            logger.info("æ­£åœ¨ç²å–æœˆç‡Ÿæ”¶è³‡æ–™...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                df = pd.read_html(str(tables[0]))[0]
                df['crawl_date'] = datetime.now().date()
                df['data_source'] = 'TWSE_æœˆç‡Ÿæ”¶'
                
                # ä¿å­˜åˆ°è³‡æ–™åº«
                df.to_sql('monthly_revenue', self.engine, if_exists='append', index=False)
                
                logger.info(f"âœ… æœˆç‡Ÿæ”¶è³‡æ–™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                logger.warning("æœªæ‰¾åˆ°æœˆç‡Ÿæ”¶è¡¨æ ¼")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"âŒ æœˆç‡Ÿæ”¶è³‡æ–™ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def get_comprehensive_data(self, symbol: str, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ç²å–åŸºæœ¬é¢ç¶œåˆæ•¸æ“š"""
        logger.info(f"ğŸ“ˆ ç²å– {symbol} åŸºæœ¬é¢æ•¸æ“š...")
        
        results = {}
        
        # è‚¡åˆ©å…¬å‘Š
        results['dividend'] = self.crawl_dividend_announcement()
        
        # æœˆç‡Ÿæ”¶
        results['monthly_revenue'] = self.crawl_monthly_revenue()
        
        # æ·»åŠ å»¶é²
        time.sleep(2)
        
        return results

class ChipDataCrawler:
    """ç±Œç¢¼é¢æ•¸æ“šçˆ¬èŸ²"""
    
    def __init__(self, session: requests.Session, engine):
        self.session = session
        self.engine = engine
    
    def get_comprehensive_data(self, symbol: str, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ç²å–ç±Œç¢¼é¢ç¶œåˆæ•¸æ“š"""
        logger.info(f"ğŸ’° ç²å– {symbol} ç±Œç¢¼é¢æ•¸æ“š...")
        # é€™è£¡å¯ä»¥æ·»åŠ å…·é«”çš„ç±Œç¢¼é¢æ•¸æ“šçˆ¬å–é‚è¼¯
        return {}

class EventDataCrawler:
    """äº‹ä»¶é¢æ•¸æ“šçˆ¬èŸ²"""
    
    def __init__(self, session: requests.Session, engine):
        self.session = session
        self.engine = engine
    
    def get_comprehensive_data(self, symbol: str, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ç²å–äº‹ä»¶é¢ç¶œåˆæ•¸æ“š"""
        logger.info(f"ğŸ“… ç²å– {symbol} äº‹ä»¶é¢æ•¸æ“š...")
        # é€™è£¡å¯ä»¥æ·»åŠ å…·é«”çš„äº‹ä»¶é¢æ•¸æ“šçˆ¬å–é‚è¼¯
        return {}

class MacroDataCrawler:
    """ç¸½ç¶“é¢æ•¸æ“šçˆ¬èŸ²"""
    
    def __init__(self, session: requests.Session, engine):
        self.session = session
        self.engine = engine
    
    def get_comprehensive_data(self, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ç²å–ç¸½ç¶“é¢ç¶œåˆæ•¸æ“š"""
        logger.info(f"ğŸŒ ç²å–ç¸½ç¶“é¢æ•¸æ“š...")
        # é€™è£¡å¯ä»¥æ·»åŠ å…·é«”çš„ç¸½ç¶“é¢æ•¸æ“šçˆ¬å–é‚è¼¯
        return {}

if __name__ == "__main__":
    # æ¸¬è©¦å¤šæ•¸æ“šæºä¸‹è¼‰å™¨
    downloader = MultiSourceDownloader()
    
    # æ¸¬è©¦ç²å–å°ç©é›»çš„æ•¸æ“š
    test_symbol = "2330.TW"
    start_date = "20250701"
    end_date = "20250728"
    
    results = downloader.get_all_data(test_symbol, start_date, end_date)
    
    print("ğŸ‰ å¤šæ•¸æ“šæºä¸‹è¼‰å™¨æ¸¬è©¦å®Œæˆ")
    for category, data in results.items():
        if isinstance(data, dict):
            print(f"ğŸ“Š {category}: {len(data)} å€‹å­é¡åˆ¥")
            for sub_cat, df in data.items():
                if not df.empty:
                    print(f"   - {sub_cat}: {len(df)} ç­†è¨˜éŒ„")
        else:
            print(f"ğŸ“Š {category}: {len(data) if not data.empty else 0} ç­†è¨˜éŒ„")
