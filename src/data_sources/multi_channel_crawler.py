#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¸ é“è‚¡ç¥¨æ•¸æ“šçˆ¬èŸ²ç³»çµ±
====================

æ•´åˆTWSEã€TPEXã€Yahoo Financeç­‰å¤šå€‹æ•¸æ“šæºï¼Œæä¾›é«˜å¯é æ€§çš„è‚¡ç¥¨æ•¸æ“šç²å–æœå‹™ã€‚
æ”¯æ´è‡ªå‹•å‚™æ´åˆ‡æ›ã€ä¸¦è¡Œè™•ç†ã€æ•¸æ“šå“è³ªé©—è­‰å’Œè‡ªå­¸å„ªåŒ–åŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹é»ï¼š
- å¤šæ¸ é“æ•¸æ“šæºæ•´åˆ (TWSE + TPEX + Yahoo Finance)
- è‡ªå‹•å‚™æ´åˆ‡æ›æ©Ÿåˆ¶
- ä¸¦è¡Œè™•ç†æå‡æ•ˆç‡
- æ™ºèƒ½æ•¸æ“šå“è³ªé©—è­‰
- è‡ªå­¸èƒ½åŠ›å’Œæº–ç¢ºæ€§å ±å‘Š
- UPSERTè³‡æ–™åº«æ“ä½œ
- æ”¯æ´PostgreSQLå’ŒSQLite

Author: AI Trading System
Date: 2025-01-28
Version: 1.0
"""

import requests
import pandas as pd
import yfinance as yf
import time
import logging
from datetime import datetime, date, timedelta
from typing import List, Dict, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps
import sqlite3
from sqlalchemy import create_engine, text
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.dialects.sqlite import insert as sqlite_insert
import numpy as np

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def handle_with_retry(max_retries: int = 3, delay: float = 2.0):
    """éŒ¯èª¤è™•ç†è£é£¾å™¨ï¼Œæ”¯æ´é‡è©¦æ©Ÿåˆ¶"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"å‡½æ•¸ {func.__name__} åŸ·è¡Œå¤±æ•—ï¼Œå·²é‡è©¦ {max_retries} æ¬¡: {e}")
                        raise e
                    else:
                        logger.warning(f"å‡½æ•¸ {func.__name__} ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—: {e}ï¼Œ{delay}ç§’å¾Œé‡è©¦")
                        time.sleep(delay)
            return None
        return wrapper
    return decorator

class DataValidator:
    """æ•¸æ“šé©—è­‰å™¨"""
    
    @staticmethod
    def validate_ohlcv_data(df: pd.DataFrame) -> pd.DataFrame:
        """é©—è­‰OHLCVæ•¸æ“šçš„é‚è¼¯æ­£ç¢ºæ€§"""
        if df.empty:
            return df
        
        # åŸºæœ¬æ•¸å€¼æª¢æŸ¥
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # OHLCé‚è¼¯æª¢æŸ¥
        valid_mask = (
            (df['open'] > 0) & 
            (df['high'] >= df['open']) & 
            (df['high'] >= df['close']) & 
            (df['low'] <= df['open']) & 
            (df['low'] <= df['close']) & 
            (df['high'] >= df['low']) &
            (df['volume'] >= 0)
        )
        
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            logger.warning(f"ç™¼ç¾ {invalid_count} ç­†ç„¡æ•ˆæ•¸æ“šï¼Œå·²è‡ªå‹•éæ¿¾")
        
        return df[valid_mask].copy()
    
    @staticmethod
    def check_data_completeness(df: pd.DataFrame, expected_days: int = 20) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
        actual_days = len(df)
        completeness_ratio = actual_days / expected_days if expected_days > 0 else 0
        
        return {
            'actual_records': actual_days,
            'expected_records': expected_days,
            'completeness_ratio': completeness_ratio,
            'is_complete': completeness_ratio >= 0.8,  # 80%ä»¥ä¸Šè¦–ç‚ºå®Œæ•´
            'missing_days': max(0, expected_days - actual_days)
        }

class TPEXCrawler:
    """æ«ƒè²·ä¸­å¿ƒ(TPEX)æ•¸æ“šçˆ¬èŸ² - ä½¿ç”¨OpenAPI"""

    def __init__(self):
        self.openapi_url = "https://www.tpex.org.tw/openapi/v1/tpex_mainboard_quotes"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        # ç·©å­˜ç•¶æ—¥æ•¸æ“šä»¥æå‡æ•ˆç‡
        self._daily_cache = {}
        self._cache_date = None

    @handle_with_retry(max_retries=3, delay=2.0)
    def crawl_monthly_data(self, symbol: str, year: int, month: int) -> pd.DataFrame:
        """çˆ¬å–TPEXè‚¡ç¥¨æ•¸æ“š - ä½¿ç”¨OpenAPIç²å–ç•¶æ—¥æ•¸æ“š"""
        try:
            # ç§»é™¤.TWå¾Œç¶´
            stock_code = symbol.replace('.TW', '')

            logger.info(f"æ­£åœ¨å¾TPEX OpenAPIçˆ¬å– {symbol} æ•¸æ“š...")

            # ç²å–ç•¶æ—¥æ‰€æœ‰ä¸Šæ«ƒè‚¡ç¥¨æ•¸æ“š
            all_data = self._get_daily_data()

            if not all_data:
                logger.warning(f"TPEX OpenAPIæœªè¿”å›æ•¸æ“š")
                return pd.DataFrame()

            # å°‹æ‰¾ç›®æ¨™è‚¡ç¥¨
            target_stock = None
            for stock in all_data:
                if isinstance(stock, dict) and stock.get('SecuritiesCompanyCode') == stock_code:
                    target_stock = stock
                    break

            if not target_stock:
                logger.warning(f"TPEXæœªæ‰¾åˆ°è‚¡ç¥¨ {symbol} ({stock_code}) çš„æ•¸æ“š")
                return pd.DataFrame()

            # è½‰æ›ç‚ºDataFrameæ ¼å¼
            date_str = target_stock.get('Date', '')
            # ä¿®å¾©æ—¥æœŸè§£æå•é¡Œ
            try:
                if date_str:
                    # TPEXæ—¥æœŸæ ¼å¼: 1140728 (æ°‘åœ‹å¹´æœˆæ—¥)
                    if len(date_str) == 7:  # 1140728
                        year = int(date_str[:3]) + 1911  # 114 + 1911 = 2025
                        month = int(date_str[3:5])       # 07
                        day = int(date_str[5:7])         # 28
                        parsed_date = pd.to_datetime(f"{year}-{month:02d}-{day:02d}")
                    else:
                        parsed_date = pd.to_datetime(date_str)
                else:
                    parsed_date = pd.to_datetime('today')
            except Exception as e:
                logger.warning(f"æ—¥æœŸè§£æå¤±æ•— {date_str}: {e}ï¼Œä½¿ç”¨ä»Šæ—¥æ—¥æœŸ")
                parsed_date = pd.to_datetime('today')

            df_data = {
                'date': [parsed_date],
                'open': [self._safe_float(target_stock.get('Open', 0))],
                'high': [self._safe_float(target_stock.get('High', 0))],
                'low': [self._safe_float(target_stock.get('Low', 0))],
                'close': [self._safe_float(target_stock.get('Close', 0))],
                'volume': [self._safe_int(target_stock.get('TradingShares', 0))],
                'symbol': [symbol],
                'source': ['TPEX']
                # ç§»é™¤company_nameä»¥ä¿æŒèˆ‡æ¨™æº–æ ¼å¼ä¸€è‡´
            }

            df = pd.DataFrame(df_data)

            # æ•¸æ“šé©—è­‰
            df = DataValidator.validate_ohlcv_data(df)

            if not df.empty:
                logger.info(f"âœ… TPEXæˆåŠŸçˆ¬å– {symbol} æ•¸æ“š: {len(df)} ç­†è¨˜éŒ„")
                logger.info(f"   å…¬å¸åç¨±: {target_stock.get('CompanyName', 'Unknown')}")
                logger.info(f"   æ”¶ç›¤åƒ¹: {target_stock.get('Close', 'Unknown')}")
            else:
                logger.warning(f"âš ï¸ TPEXæ•¸æ“šé©—è­‰å¤±æ•—: {symbol}")

            return df

        except Exception as e:
            logger.error(f"âŒ TPEXçˆ¬å– {symbol} å¤±æ•—: {e}")
            return pd.DataFrame()

    def _get_daily_data(self) -> List[Dict]:
        """ç²å–ç•¶æ—¥æ‰€æœ‰ä¸Šæ«ƒè‚¡ç¥¨æ•¸æ“šï¼Œä½¿ç”¨ç·©å­˜æå‡æ•ˆç‡"""
        try:
            from datetime import date
            today = date.today()

            # æª¢æŸ¥ç·©å­˜
            if self._cache_date == today and self._daily_cache:
                logger.debug("ä½¿ç”¨TPEXæ•¸æ“šç·©å­˜")
                return self._daily_cache

            # ç²å–æ–°æ•¸æ“š
            response = self.session.get(self.openapi_url, timeout=30)
            response.raise_for_status()

            data = response.json()

            if isinstance(data, list):
                # æ›´æ–°ç·©å­˜
                self._daily_cache = data
                self._cache_date = today
                logger.info(f"âœ… TPEX OpenAPIç²å– {len(data)} ç­†è‚¡ç¥¨æ•¸æ“š")
                return data
            else:
                logger.warning(f"TPEX OpenAPIæ•¸æ“šæ ¼å¼ç•°å¸¸: {type(data)}")
                return []

        except Exception as e:
            logger.error(f"âŒ TPEX OpenAPIè«‹æ±‚å¤±æ•—: {e}")
            return []

    def _safe_float(self, value) -> float:
        """å®‰å…¨è½‰æ›ç‚ºæµ®é»æ•¸"""
        try:
            if isinstance(value, str):
                # ç§»é™¤é€—è™Ÿå’Œå…¶ä»–éæ•¸å­—å­—ç¬¦
                cleaned = value.replace(',', '').replace(' ', '')
                return float(cleaned) if cleaned else 0.0
            return float(value) if value is not None else 0.0
        except (ValueError, TypeError):
            return 0.0

    def _safe_int(self, value) -> int:
        """å®‰å…¨è½‰æ›ç‚ºæ•´æ•¸"""
        try:
            if isinstance(value, str):
                # ç§»é™¤é€—è™Ÿå’Œå…¶ä»–éæ•¸å­—å­—ç¬¦
                cleaned = value.replace(',', '').replace(' ', '')
                return int(float(cleaned)) if cleaned else 0
            return int(float(value)) if value is not None else 0
        except (ValueError, TypeError):
            return 0

    def get_available_symbols(self) -> List[str]:
        """ç²å–æ‰€æœ‰å¯ç”¨çš„ä¸Šæ«ƒè‚¡ç¥¨ä»£ç¢¼"""
        try:
            all_data = self._get_daily_data()
            symbols = []

            for stock in all_data:
                if isinstance(stock, dict):
                    code = stock.get('SecuritiesCompanyCode', '')
                    if code and code.isdigit():  # åªå–æ•¸å­—ä»£ç¢¼
                        symbols.append(f"{code}.TW")

            return sorted(symbols)

        except Exception as e:
            logger.error(f"âŒ ç²å–TPEXè‚¡ç¥¨åˆ—è¡¨å¤±æ•—: {e}")
            return []

class YahooFinanceCrawler:
    """Yahoo Financeæ•¸æ“šçˆ¬èŸ²"""
    
    @handle_with_retry(max_retries=3, delay=1.0)
    def crawl_historical_data(self, symbol: str, start_date: date, end_date: date) -> pd.DataFrame:
        """çˆ¬å–Yahoo Financeæ­·å²æ•¸æ“š"""
        try:
            logger.info(f"æ­£åœ¨å¾Yahoo Financeçˆ¬å– {symbol} {start_date} è‡³ {end_date} æ•¸æ“š...")
            
            # ä½¿ç”¨yfinanceç²å–æ•¸æ“š
            ticker = yf.Ticker(symbol)
            df = ticker.history(start=start_date, end=end_date, auto_adjust=False)
            
            if df.empty:
                logger.warning(f"Yahoo Financeæœªè¿”å› {symbol} çš„æ•¸æ“š")
                return pd.DataFrame()
            
            # é‡ç½®ç´¢å¼•ä¸¦é‡å‘½åæ¬„ä½
            df.reset_index(inplace=True)
            df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()
            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']
            
            # æ·»åŠ å…ƒæ•¸æ“š
            df['symbol'] = symbol
            df['source'] = 'Yahoo Finance'
            
            # æ•¸æ“šé©—è­‰
            df = DataValidator.validate_ohlcv_data(df)
            
            logger.info(f"âœ… Yahoo FinanceæˆåŠŸçˆ¬å– {symbol} æ•¸æ“š: {len(df)} ç­†è¨˜éŒ„")
            return df
            
        except Exception as e:
            logger.error(f"âŒ Yahoo Financeçˆ¬å– {symbol} å¤±æ•—: {e}")
            return pd.DataFrame()

class AutoDataManager:
    """è‡ªå‹•æ•¸æ“šç®¡ç†å™¨ - æ”¯æ´è‡ªå­¸èƒ½åŠ›"""
    
    def __init__(self, db_url: str = "sqlite:///stock_data.db"):
        self.db_url = db_url
        self.engine = create_engine(db_url)
        self.tpex_crawler = TPEXCrawler()
        self.yahoo_crawler = YahooFinanceCrawler()
        
    def crawl_multi_channel_data(self, symbols: List[str], start_date: date, end_date: date, 
                                max_workers: int = 2) -> Dict[str, pd.DataFrame]:
        """å¤šæ¸ é“ä¸¦è¡Œçˆ¬å–æ•¸æ“š"""
        results = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_symbol = {}
            
            for symbol in symbols:
                # Yahoo Financeä»»å‹™
                future_yahoo = executor.submit(
                    self.yahoo_crawler.crawl_historical_data, 
                    symbol, start_date, end_date
                )
                future_to_symbol[future_yahoo] = (symbol, 'yahoo')
                
                # TPEXä»»å‹™ (æŒ‰æœˆä»½æ‹†åˆ†)
                current_date = start_date
                while current_date <= end_date:
                    future_tpex = executor.submit(
                        self.tpex_crawler.crawl_monthly_data,
                        symbol, current_date.year, current_date.month
                    )
                    future_to_symbol[future_tpex] = (symbol, f'tpex_{current_date.year}_{current_date.month}')
                    
                    # ç§»å‹•åˆ°ä¸‹å€‹æœˆ
                    if current_date.month == 12:
                        current_date = current_date.replace(year=current_date.year + 1, month=1)
                    else:
                        current_date = current_date.replace(month=current_date.month + 1)
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_symbol):
                symbol, source_key = future_to_symbol[future]
                try:
                    df = future.result()
                    if not df.empty:
                        if symbol not in results:
                            results[symbol] = []
                        results[symbol].append(df)
                except Exception as e:
                    logger.error(f"ä»»å‹™åŸ·è¡Œå¤±æ•— {symbol} ({source_key}): {e}")
        
        # åˆä½µåŒä¸€è‚¡ç¥¨çš„å¤šå€‹æ•¸æ“šæº
        final_results = {}
        for symbol, dfs in results.items():
            if dfs:
                combined_df = pd.concat(dfs, ignore_index=True)
                # å»é‡ä¸¦æ’åº
                combined_df = combined_df.drop_duplicates(subset=['date', 'symbol']).sort_values('date')
                final_results[symbol] = combined_df
        
        return final_results

    def generate_accuracy_report(self, symbol_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•¸æ“šæº–ç¢ºæ€§å ±å‘Š"""
        report = {
            'timestamp': datetime.now(),
            'symbols_processed': len(symbol_data),
            'symbol_reports': {},
            'overall_quality': 0.0
        }

        total_quality_score = 0

        for symbol, df in symbol_data.items():
            if df.empty:
                continue

            # è¨ˆç®—æ•¸æ“šå“è³ªæŒ‡æ¨™
            completeness = DataValidator.check_data_completeness(df)

            # æª¢æŸ¥æ•¸æ“šæºå¤šæ¨£æ€§
            sources = df['source'].unique() if 'source' in df.columns else ['Unknown']

            # è¨ˆç®—åƒ¹æ ¼è®Šå‹•åˆç†æ€§
            df_sorted = df.sort_values('date')
            price_changes = df_sorted['close'].pct_change().abs()
            extreme_changes = (price_changes > 0.1).sum()  # è¶…é10%è®Šå‹•çš„å¤©æ•¸

            # è¨ˆç®—å“è³ªåˆ†æ•¸
            quality_score = (
                completeness['completeness_ratio'] * 0.4 +  # å®Œæ•´æ€§40%
                (len(sources) / 3) * 0.3 +  # æ•¸æ“šæºå¤šæ¨£æ€§30%
                max(0, 1 - extreme_changes / len(df)) * 0.3  # åƒ¹æ ¼åˆç†æ€§30%
            )

            symbol_report = {
                'symbol': symbol,
                'total_records': len(df),
                'date_range': {
                    'start': df['date'].min().strftime('%Y-%m-%d') if not df.empty else None,
                    'end': df['date'].max().strftime('%Y-%m-%d') if not df.empty else None
                },
                'completeness': completeness,
                'data_sources': sources.tolist(),
                'extreme_price_changes': int(extreme_changes),
                'quality_score': round(quality_score, 3),
                'recommendations': []
            }

            # ç”Ÿæˆå»ºè­°
            if completeness['completeness_ratio'] < 0.8:
                symbol_report['recommendations'].append("æ•¸æ“šå®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè­°å¢åŠ å‚™æ´æ•¸æ“šæº")

            if len(sources) == 1:
                symbol_report['recommendations'].append("åƒ…æœ‰å–®ä¸€æ•¸æ“šæºï¼Œå»ºè­°å•Ÿç”¨å¤šæ¸ é“å‚™æ´")

            if extreme_changes > len(df) * 0.1:
                symbol_report['recommendations'].append("ç™¼ç¾ç•°å¸¸åƒ¹æ ¼è®Šå‹•ï¼Œå»ºè­°é€²è¡Œæ•¸æ“šé©—è­‰")

            report['symbol_reports'][symbol] = symbol_report
            total_quality_score += quality_score

        # è¨ˆç®—æ•´é«”å“è³ªåˆ†æ•¸
        if symbol_data:
            report['overall_quality'] = round(total_quality_score / len(symbol_data), 3)

        return report

    def save_to_database_with_upsert(self, df: pd.DataFrame, table_name: str = 'stock_data') -> bool:
        """ä½¿ç”¨UPSERTé‚è¼¯ä¿å­˜æ•¸æ“šåˆ°è³‡æ–™åº«"""
        if df.empty:
            return False

        try:
            # æª¢æŸ¥è³‡æ–™åº«é¡å‹
            if 'postgresql' in self.db_url:
                return self._upsert_postgresql(df, table_name)
            else:
                return self._upsert_sqlite(df, table_name)

        except Exception as e:
            logger.error(f"æ•¸æ“šåº«ä¿å­˜å¤±æ•—: {e}")
            return False

    def _upsert_postgresql(self, df: pd.DataFrame, table_name: str) -> bool:
        """PostgreSQL UPSERTæ“ä½œ"""
        try:
            # å‰µå»ºè¡¨æ ¼ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            df.head(0).to_sql(table_name, self.engine, if_exists='append', index=False)

            # åŸ·è¡ŒUPSERT
            with self.engine.begin() as conn:
                for _, row in df.iterrows():
                    stmt = pg_insert(table_name).values(**row.to_dict())
                    stmt = stmt.on_conflict_do_update(
                        index_elements=['symbol', 'date'],
                        set_={col: stmt.excluded[col] for col in df.columns if col not in ['symbol', 'date']}
                    )
                    conn.execute(stmt)

            logger.info(f"âœ… PostgreSQL UPSERTå®Œæˆ: {len(df)} ç­†è¨˜éŒ„")
            return True

        except Exception as e:
            logger.error(f"PostgreSQL UPSERTå¤±æ•—: {e}")
            return False

    def _upsert_sqlite(self, df: pd.DataFrame, table_name: str) -> bool:
        """SQLite UPSERTæ“ä½œ - ä¿®å¾©ç‰ˆ"""
        try:
            if df.empty:
                logger.warning("DataFrameç‚ºç©ºï¼Œè·³éUPSERTæ“ä½œ")
                return True

            # å‰µå»ºå¸¶æœ‰ä¸»éµç´„æŸçš„è¡¨æ ¼
            self._create_table_with_constraints(df, table_name)

            # ä½¿ç”¨çœŸæ­£çš„UPSERTæ“ä½œ
            with self.engine.begin() as conn:
                # ç‚ºæ¯ä¸€è¡Œæ•¸æ“šåŸ·è¡ŒUPSERT
                for _, row in df.iterrows():
                    # æ§‹å»ºåƒæ•¸å­—å…¸
                    row_dict = row.to_dict()

                    # è™•ç†NaNå€¼å’Œç‰¹æ®Šé¡å‹
                    for key, value in row_dict.items():
                        if pd.isna(value):
                            row_dict[key] = None
                        elif isinstance(value, pd.Timestamp):
                            # ä¿®å¾©æ™‚å€å•é¡Œï¼šè½‰æ›ç‚ºä¸å¸¶æ™‚å€çš„æ—¥æœŸå­—ç¬¦ä¸²
                            row_dict[key] = value.strftime('%Y-%m-%d')
                        elif hasattr(value, 'item'):  # numpyé¡å‹
                            row_dict[key] = value.item()

                    # é¦–å…ˆå˜—è©¦æ›´æ–°
                    if 'symbol' in row_dict and 'date' in row_dict:
                        # æª¢æŸ¥è¨˜éŒ„æ˜¯å¦å­˜åœ¨
                        check_sql = f"""
                        SELECT COUNT(*) FROM {table_name}
                        WHERE symbol = :symbol AND date = :date
                        """

                        result = conn.execute(text(check_sql), {
                            'symbol': row_dict['symbol'],
                            'date': row_dict['date']
                        })

                        exists = result.fetchone()[0] > 0

                        if exists:
                            # æ›´æ–°ç¾æœ‰è¨˜éŒ„
                            update_columns = [f"{col} = :{col}" for col in row_dict.keys()
                                            if col not in ['symbol', 'date']]
                            if update_columns:
                                update_sql = f"""
                                UPDATE {table_name}
                                SET {', '.join(update_columns)}
                                WHERE symbol = :symbol AND date = :date
                                """
                                conn.execute(text(update_sql), row_dict)
                        else:
                            # æ’å…¥æ–°è¨˜éŒ„
                            columns = list(row_dict.keys())
                            placeholders = ', '.join([f':{col}' for col in columns])
                            columns_str = ', '.join(columns)

                            insert_sql = f"""
                            INSERT INTO {table_name} ({columns_str})
                            VALUES ({placeholders})
                            """
                            conn.execute(text(insert_sql), row_dict)
                    else:
                        # å¦‚æœæ²’æœ‰symbolå’Œdateï¼Œç›´æ¥æ’å…¥
                        columns = list(row_dict.keys())
                        placeholders = ', '.join([f':{col}' for col in columns])
                        columns_str = ', '.join(columns)

                        insert_sql = f"""
                        INSERT INTO {table_name} ({columns_str})
                        VALUES ({placeholders})
                        """
                        conn.execute(text(insert_sql), row_dict)

            logger.info(f"âœ… SQLite UPSERTå®Œæˆ: {len(df)} ç­†è¨˜éŒ„")
            return True

        except Exception as e:
            logger.error(f"SQLite UPSERTå¤±æ•—: {e}")
            logger.error(f"DataFrame info: shape={df.shape}, columns={list(df.columns)}")
            return False

    def _create_table_with_constraints(self, df: pd.DataFrame, table_name: str):
        """å‰µå»ºå¸¶æœ‰ç´„æŸçš„è¡¨æ ¼"""
        try:
            with self.engine.begin() as conn:
                # æª¢æŸ¥è¡¨æ ¼æ˜¯å¦å·²å­˜åœ¨
                check_table_sql = f"""
                SELECT name FROM sqlite_master
                WHERE type='table' AND name='{table_name}'
                """

                result = conn.execute(text(check_table_sql))
                table_exists = result.fetchone() is not None

                if not table_exists:
                    # å‰µå»ºè¡¨æ ¼çµæ§‹
                    columns_def = []
                    for col in df.columns:
                        if col in ['symbol', 'date']:
                            columns_def.append(f"{col} TEXT NOT NULL")
                        elif col in ['open', 'high', 'low', 'close', 'volume']:
                            columns_def.append(f"{col} REAL")
                        else:
                            columns_def.append(f"{col} TEXT")

                    # æ·»åŠ ä¸»éµç´„æŸ
                    if 'symbol' in df.columns and 'date' in df.columns:
                        columns_def.append("PRIMARY KEY (symbol, date)")

                    create_table_sql = f"""
                    CREATE TABLE IF NOT EXISTS {table_name} (
                        {', '.join(columns_def)}
                    )
                    """

                    conn.execute(text(create_table_sql))
                    logger.debug(f"å‰µå»ºè¡¨æ ¼ {table_name} æˆåŠŸ")

        except Exception as e:
            logger.warning(f"å‰µå»ºè¡¨æ ¼ç´„æŸå¤±æ•—ï¼Œä½¿ç”¨é»˜èªæ–¹å¼: {e}")
            # å›é€€åˆ°pandasé»˜èªå‰µå»ºæ–¹å¼
            df.head(0).to_sql(table_name, self.engine, if_exists='append', index=False)

    def auto_detect_and_retry(self, symbols: List[str], start_date: date, end_date: date) -> Dict[str, Any]:
        """è‡ªå‹•åµæ¸¬æ•¸æ“šç¼ºå¤±ä¸¦é‡è©¦"""
        logger.info("ğŸ¤– å•Ÿå‹•è‡ªå­¸æ•¸æ“šç®¡ç†æ¨¡å¼")

        # ç¬¬ä¸€æ¬¡çˆ¬å–
        data_results = self.crawl_multi_channel_data(symbols, start_date, end_date)

        # ç”Ÿæˆåˆå§‹å ±å‘Š
        initial_report = self.generate_accuracy_report(data_results)

        # è­˜åˆ¥éœ€è¦é‡è©¦çš„è‚¡ç¥¨
        retry_symbols = []
        for symbol, report in initial_report['symbol_reports'].items():
            if (report['completeness']['completeness_ratio'] < 0.8 or
                report['quality_score'] < 0.7):
                retry_symbols.append(symbol)

        # åŸ·è¡Œé‡è©¦
        if retry_symbols:
            logger.info(f"ğŸ”„ æª¢æ¸¬åˆ° {len(retry_symbols)} å€‹è‚¡ç¥¨éœ€è¦é‡è©¦: {retry_symbols}")

            # å»¶é²å¾Œé‡è©¦
            time.sleep(5)
            retry_results = self.crawl_multi_channel_data(retry_symbols, start_date, end_date)

            # æ™ºèƒ½åˆä½µçµæœ
            for symbol, retry_df in retry_results.items():
                if not retry_df.empty:
                    original_df = data_results.get(symbol, pd.DataFrame())

                    if original_df.empty:
                        # å¦‚æœåŸå§‹æ•¸æ“šç‚ºç©ºï¼Œç›´æ¥ä½¿ç”¨é‡è©¦æ•¸æ“š
                        data_results[symbol] = retry_df
                        logger.info(f"âœ… {symbol} é‡è©¦ç²å¾—æ–°æ•¸æ“š: {len(retry_df)} ç­†è¨˜éŒ„")
                    else:
                        # åˆä½µæ•¸æ“šä¸¦å»é‡
                        combined_df = pd.concat([original_df, retry_df], ignore_index=True)

                        # æ ¹æ“šæ—¥æœŸå’Œè‚¡ç¥¨ä»£ç¢¼å»é‡ï¼Œä¿ç•™æœ€æ–°æ•¸æ“š
                        if 'date' in combined_df.columns and 'symbol' in combined_df.columns:
                            combined_df = combined_df.drop_duplicates(
                                subset=['date', 'symbol'],
                                keep='last'
                            ).sort_values('date')

                        # åªæœ‰åœ¨åˆä½µå¾Œæ•¸æ“šæ›´å¥½æ™‚æ‰æ›¿æ›
                        if len(combined_df) > len(original_df):
                            data_results[symbol] = combined_df
                            logger.info(f"âœ… {symbol} é‡è©¦æ”¹å–„æ•¸æ“š: {len(original_df)} â†’ {len(combined_df)} ç­†è¨˜éŒ„")
                        else:
                            logger.info(f"âš ï¸ {symbol} é‡è©¦æœªæ”¹å–„æ•¸æ“šå“è³ªï¼Œä¿æŒåŸå§‹æ•¸æ“š")

        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = self.generate_accuracy_report(data_results)

        # ä¿å­˜åˆ°è³‡æ–™åº«
        saved_count = 0
        for symbol, df in data_results.items():
            if self.save_to_database_with_upsert(df):
                saved_count += 1

        return {
            'data_results': data_results,
            'initial_report': initial_report,
            'final_report': final_report,
            'retry_symbols': retry_symbols,
            'saved_to_db': saved_count,
            'total_symbols': len(symbols)
        }
