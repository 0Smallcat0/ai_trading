#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·²é©—è­‰æ•¸æ“šçˆ¬èŸ²ç³»çµ±
================

å°ˆæ³¨æ–¼å·²é©—è­‰å¯ç”¨çš„å°è‚¡å…è²»æ•¸æ“šæºï¼Œç¢ºä¿æ¯å€‹åŠŸèƒ½éƒ½ç©©å®šå¯ç”¨ã€‚
åŸºæ–¼æ¸¬è©¦çµæœï¼Œå„ªå…ˆå¯¦ç¾æˆåŠŸç‡é«˜çš„æ•¸æ“šæºã€‚
"""

import requests
import pandas as pd
import yfinance as yf
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
from io import StringIO
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VerifiedCrawler:
    """å·²é©—è­‰æ•¸æ“šçˆ¬èŸ²ç³»çµ±"""

    def __init__(self, db_url: str = "sqlite:///unified_stock_database.db",
                 auto_save: bool = True):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.request_delay = 2.0
        self.auto_save = auto_save

        # åˆå§‹åŒ–çµ±ä¸€å­˜å„²
        if auto_save:
            try:
                from ..database.unified_storage import UnifiedStorage
                self.storage = UnifiedStorage(db_url)
                logger.info("âœ… çµ±ä¸€å­˜å„²ç³»çµ±å·²å•Ÿç”¨")
            except Exception as e:
                logger.warning(f"âš ï¸ çµ±ä¸€å­˜å„²ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
                self.storage = None
        else:
            self.storage = None

    def _save_data_if_enabled(self, df: pd.DataFrame, table_name: str,
                              data_source: str, data_type: str = 'REAL_DATA') -> Dict:
        """å¦‚æœå•Ÿç”¨è‡ªå‹•å­˜å„²ï¼Œå‰‡ä¿å­˜æ•¸æ“šåˆ°è³‡æ–™åº«"""
        if not self.auto_save or not self.storage or df.empty:
            return {'success': False, 'reason': 'Auto save disabled or no data'}

        try:
            result = self.storage.save_to_database(df, table_name, data_source, data_type)
            return result
        except Exception as e:
            logger.error(f"è‡ªå‹•å­˜å„²å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    def _safe_request(self, url: str, params: Dict = None, timeout: int = 30) -> Optional[requests.Response]:
        """å®‰å…¨çš„HTTPè«‹æ±‚"""
        try:
            time.sleep(self.request_delay)
            response = self.session.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            return response
        except Exception as e:
            logger.error(f"è«‹æ±‚å¤±æ•— {url}: {e}")
            return None
    
    # ========================================================================
    # å·²é©—è­‰å¯ç”¨çš„æŠ€è¡“é¢æ•¸æ“šæº
    # ========================================================================
    
    def crawl_twse_backtest_index(self) -> pd.DataFrame:
        """å›æ¸¬åŸºæº–æŒ‡æ•¸ - TWSE OpenAPI JSON âœ…"""
        url = "https://openapi.twse.com.tw/v1/indicesReport/MI_5MINS_HIST"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if data:
                df = pd.DataFrame(data)
                df['data_source'] = 'TWSE_å›æ¸¬æŒ‡æ•¸'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… å›æ¸¬åŸºæº–æŒ‡æ•¸ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å›æ¸¬åŸºæº–æŒ‡æ•¸è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_yahoo_adjusted_price(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """é‚„åŸæ¬Šå€¼è‚¡åƒ¹ - Yahoo Finance API âœ…"""
        try:
            start = pd.to_datetime(start_date).strftime('%Y-%m-%d')
            end = pd.to_datetime(end_date).strftime('%Y-%m-%d')

            df = yf.download(symbol, start=start, end=end, auto_adjust=True, progress=False)

            if not df.empty:
                # ä¿®å¾©å¤šå±¤æ¬„ä½çµæ§‹å•é¡Œ
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]

                df.reset_index(inplace=True)
                df['symbol'] = symbol

                # æ¨™æº–åŒ–æ¬„ä½åç¨±ä»¥ç¬¦åˆè³‡æ–™åº«çµæ§‹
                df = df.rename(columns={
                    'Date': 'date',
                    'Open': 'open_price',
                    'High': 'high_price',
                    'Low': 'low_price',
                    'Close': 'close_price',
                    'Volume': 'volume'
                })

                # å¦‚æœæœ‰èª¿æ•´å¾Œæ”¶ç›¤åƒ¹ï¼Œæ·»åŠ åˆ°adjusted_closeæ¬„ä½
                if 'close_price' in df.columns:
                    df['adjusted_close'] = df['close_price']

                logger.info(f"âœ… {symbol} é‚„åŸè‚¡åƒ¹ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")

                # è‡ªå‹•å­˜å„²åˆ°è³‡æ–™åº«
                storage_result = self._save_data_if_enabled(
                    df, 'stock_daily_prices', 'Yahoo_Finance', 'REAL_DATA'
                )
                if storage_result.get('success'):
                    logger.info(f"âœ… {symbol} è‚¡åƒ¹æ•¸æ“šå·²å­˜å„²åˆ°è³‡æ–™åº«")

                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"é‚„åŸè‚¡åƒ¹ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_market_indicators(self) -> pd.DataFrame:
        """å¤§ç›¤å¸‚æ³æŒ‡æ¨™ - TWSE OpenAPI JSON âœ…"""
        url = "https://openapi.twse.com.tw/v1/indicesReport/MI_INDEX"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if data:
                df = pd.DataFrame(data)

                # æ¨™æº–åŒ–æ¬„ä½åç¨±
                df = df.rename(columns={
                    'æ—¥æœŸ': 'date',
                    'æŒ‡æ•¸': 'index_name',
                    'æ”¶ç›¤æŒ‡æ•¸': 'index_value',
                    'æ¼²è·Œé»æ•¸': 'change_points',
                    'æ¼²è·Œç™¾åˆ†æ¯”': 'change_percent',
                    'ç‰¹æ®Šè™•ç†è¨»è¨˜': 'special_note'
                })

                # è™•ç†æ—¥æœŸæ ¼å¼
                if 'date' in df.columns:
                    try:
                        df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')
                    except Exception:
                        df['date'] = datetime.now().date()
                else:
                    df['date'] = datetime.now().date()

                # è™•ç†æ•¸å€¼æ¬„ä½
                for col in ['index_value', 'change_points', 'change_percent']:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                logger.info(f"âœ… å¤§ç›¤å¸‚æ³æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")

                # è‡ªå‹•å­˜å„²åˆ°è³‡æ–™åº«
                storage_result = self._save_data_if_enabled(
                    df, 'market_indicators', 'TWSE_OpenAPI', 'REAL_DATA'
                )
                if storage_result.get('success'):
                    logger.info(f"âœ… å¤§ç›¤æŒ‡æ¨™æ•¸æ“šå·²å­˜å„²åˆ°è³‡æ–™åº«")

                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å¤§ç›¤å¸‚æ³æŒ‡æ¨™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_tpex_mainboard_quotes(self) -> pd.DataFrame:
        """TPEXä¸Šæ«ƒè‚¡ç¥¨å³æ™‚å ±åƒ¹ âœ…"""
        url = "https://www.tpex.org.tw/openapi/v1/tpex_mainboard_quotes"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if isinstance(data, list) and data:
                df = pd.DataFrame(data)
                df['data_source'] = 'TPEX_ä¸Šæ«ƒè‚¡ç¥¨'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… TPEXä¸Šæ«ƒè‚¡ç¥¨ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"TPEXä¸Šæ«ƒè‚¡ç¥¨è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    # ========================================================================
    # å·²é©—è­‰å¯ç”¨çš„åŸºæœ¬é¢æ•¸æ“šæº
    # ========================================================================
    
    def crawl_gov_company_info(self, api_key: str = None) -> pd.DataFrame:
        """ä¼æ¥­åŸºæœ¬è³‡è¨Š - æ”¿åºœé–‹æ”¾å¹³å° JSON âœ…"""
        if not api_key:
            logger.info("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            return pd.DataFrame({
                'company_id': ['12345678', '87654321'],
                'company_name': ['å°ç©é›» [æ¨¡æ“¬æ•¸æ“š]', 'é´»æµ·ç²¾å¯† [æ¨¡æ“¬æ•¸æ“š]'],
                'industry': ['åŠå°é«”æ¥­ [æ¨¡æ“¬æ•¸æ“š]', 'é›»å­è£½é€ æ¥­ [æ¨¡æ“¬æ•¸æ“š]'],
                'capital': [259303956330, 138460264120],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_ä¼æ¥­è³‡è¨Š_æ¨¡æ“¬æ•¸æ“š', 'æ”¿åºœé–‹æ”¾å¹³å°_ä¼æ¥­è³‡è¨Š_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now(), datetime.now()],
                'data_type': ['MOCK_DATA', 'MOCK_DATA']
            })
        
        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000157-053"
        params = {
            'Authorization': api_key,
            'limit': 1000
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_ä¼æ¥­è³‡è¨Š'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… ä¼æ¥­åŸºæœ¬è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ä¼æ¥­åŸºæœ¬è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_finmind_financial_data(self, symbol: str = "2330") -> pd.DataFrame:
        """è²¡å‹™æŒ‡æ¨™ - FinMind API JSON âœ…"""
        url = "https://api.finmindtrade.com/api/v4/data"
        params = {
            'dataset': 'TaiwanStockFinancialStatements',
            'data_id': symbol,
            'start_date': '2024-01-01'
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'FinMind_è²¡å‹™æŒ‡æ¨™'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… è²¡å‹™æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"è²¡å‹™æŒ‡æ¨™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    # ========================================================================
    # å·²é©—è­‰å¯ç”¨çš„ç±Œç¢¼é¢æ•¸æ“šæº
    # ========================================================================
    
    def crawl_twse_broker_trading(self) -> pd.DataFrame:
        """åˆ¸å•†åˆ†é»è²·è³£è¶…å‰15å¤§åˆ¸å•†æ˜ç´° - TWSE JSON âœ…"""
        url = "https://www.twse.com.tw/fund/T86"
        params = {
            'response': 'json',
            'date': datetime.now().strftime('%Y%m%d'),
            'selectType': 'ALLBUT0999'
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_åˆ¸å•†è²·è³£è¶…'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… åˆ¸å•†è²·è³£è¶…æ˜ç´°ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"åˆ¸å•†è²·è³£è¶…æ˜ç´°è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_foreign_holding(self) -> pd.DataFrame:
        """å¤–è³‡æŒè‚¡æ¯”ç‡ - TWSE JSON âœ…"""
        url = "https://www.twse.com.tw/fund/MI_QFIIS"
        params = {
            'response': 'json',
            'date': datetime.now().strftime('%Y%m%d')
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_å¤–è³‡æŒè‚¡'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… å¤–è³‡æŒè‚¡æ¯”ç‡ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å¤–è³‡æŒè‚¡æ¯”ç‡è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    # ========================================================================
    # å·²é©—è­‰å¯ç”¨çš„ç¸½ç¶“é¢æ•¸æ“šæº
    # ========================================================================
    
    def crawl_gov_economic_indicators(self, api_key: str = None) -> pd.DataFrame:
        """å°ç£æ™¯æ°£æŒ‡æ¨™ - æ”¿åºœé–‹æ”¾å¹³å° JSON âœ…"""
        if not api_key:
            logger.info("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            return pd.DataFrame({
                'indicator_name': ['æ™¯æ°£å°ç­–ä¿¡è™Ÿ [æ¨¡æ“¬æ•¸æ“š]', 'æ™¯æ°£é ˜å…ˆæŒ‡æ¨™ [æ¨¡æ“¬æ•¸æ“š]', 'æ™¯æ°£åŒæ™‚æŒ‡æ¨™ [æ¨¡æ“¬æ•¸æ“š]'],
                'value': [25, 102.5, 98.7],
                'period': ['2025-01', '2025-01', '2025-01'],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_æ™¯æ°£æŒ‡æ¨™_æ¨¡æ“¬æ•¸æ“š', 'æ”¿åºœé–‹æ”¾å¹³å°_æ™¯æ°£æŒ‡æ¨™_æ¨¡æ“¬æ•¸æ“š', 'æ”¿åºœé–‹æ”¾å¹³å°_æ™¯æ°£æŒ‡æ¨™_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now(), datetime.now(), datetime.now()],
                'data_type': ['MOCK_DATA', 'MOCK_DATA', 'MOCK_DATA']
            })
        
        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000151-001"
        params = {
            'Authorization': api_key,
            'limit': 100
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_æ™¯æ°£æŒ‡æ¨™'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… æ™¯æ°£æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"æ™¯æ°£æŒ‡æ¨™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_yahoo_world_indices(self) -> pd.DataFrame:
        """ä¸–ç•ŒæŒ‡æ•¸ - Yahoo Finance API âœ…"""
        try:
            indices = {
                '^GSPC': 'S&P 500',
                '^DJI': 'Dow Jones',
                '^IXIC': 'NASDAQ',
                '^N225': 'Nikkei 225',
                '^HSI': 'Hang Seng'
            }
            
            world_data = []
            for symbol, name in indices.items():
                try:
                    ticker = yf.Ticker(symbol)
                    info = ticker.info
                    
                    world_data.append({
                        'symbol': symbol,
                        'name': name,
                        'current_price': info.get('regularMarketPrice', 0),
                        'previous_close': info.get('previousClose', 0),
                        'change': info.get('regularMarketChange', 0),
                        'change_percent': info.get('regularMarketChangePercent', 0),
                        'data_source': 'Yahoo_ä¸–ç•ŒæŒ‡æ•¸',
                        'crawl_time': datetime.now()
                    })
                    
                except Exception as e:
                    logger.warning(f"ç²å– {name} æ•¸æ“šå¤±æ•—: {e}")
                    continue
            
            if world_data:
                df = pd.DataFrame(world_data)
                logger.info(f"âœ… ä¸–ç•ŒæŒ‡æ•¸ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ä¸–ç•ŒæŒ‡æ•¸è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    # ========================================================================
    # çµ±ä¸€æ¥å£
    # ========================================================================
    
    def get_verified_sources(self) -> Dict[str, List[str]]:
        """ç²å–å·²é©—è­‰å¯ç”¨çš„æ•¸æ“šæºæ¸…å–®"""
        return {
            'technical': [
                'crawl_twse_backtest_index',
                'crawl_yahoo_adjusted_price', 
                'crawl_twse_market_indicators',
                'crawl_tpex_mainboard_quotes'
            ],
            'fundamental': [
                'crawl_gov_company_info',
                'crawl_finmind_financial_data'
            ],
            'chip': [
                'crawl_twse_broker_trading',
                'crawl_twse_foreign_holding'
            ],
            'macro': [
                'crawl_gov_economic_indicators',
                'crawl_yahoo_world_indices'
            ]
        }
    
    def crawl_all_verified_data(self, **kwargs) -> Dict[str, Dict[str, pd.DataFrame]]:
        """çˆ¬å–æ‰€æœ‰å·²é©—è­‰çš„æ•¸æ“šæº"""
        verified_sources = self.get_verified_sources()
        all_results = {}
        
        for category, methods in verified_sources.items():
            logger.info(f"ğŸ”„ é–‹å§‹çˆ¬å– {category} å·²é©—è­‰æ•¸æ“š...")
            category_results = {}
            
            for method_name in methods:
                try:
                    method = getattr(self, method_name)
                    
                    # æ ¹æ“šæ–¹æ³•éœ€è¦çš„åƒæ•¸èª¿ç”¨
                    if method_name == 'crawl_yahoo_adjusted_price':
                        if 'symbol' in kwargs and 'start_date' in kwargs and 'end_date' in kwargs:
                            df = method(kwargs['symbol'], kwargs['start_date'], kwargs['end_date'])
                        else:
                            df = method('2330.TW', '2025-07-20', '2025-07-25')  # é»˜èªåƒæ•¸
                    elif method_name in ['crawl_gov_company_info', 'crawl_gov_economic_indicators']:
                        df = method(kwargs.get('api_key'))
                    elif method_name == 'crawl_finmind_financial_data':
                        df = method(kwargs.get('symbol', '2330'))
                    else:
                        df = method()
                    
                    if not df.empty:
                        category_results[method_name] = df
                        logger.info(f"âœ… {method_name} æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                    else:
                        logger.warning(f"âš ï¸ {method_name} ç„¡æ•¸æ“š")
                        
                except Exception as e:
                    logger.error(f"âŒ {method_name} åŸ·è¡Œå¤±æ•—: {e}")
                    continue
            
            all_results[category] = category_results
            success_count = len(category_results)
            total_count = len(methods)
            logger.info(f"ğŸ“Š {category} å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")
        
        return all_results
