#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆçœŸå¯¦æ•¸æ“šçˆ¬å–å™¨ - æ”¯æŒä¸¦è¡Œè™•ç†å’Œå¢é‡æ›´æ–°

ä¸»è¦æ”¹é€²ï¼š
1. å¤šç·šç¨‹ä¸¦è¡Œçˆ¬å–ï¼Œæå‡50-70%æ•ˆç‡
2. æ™ºèƒ½å¢é‡æ›´æ–°ï¼Œé¿å…é‡è¤‡çˆ¬å–
3. å‹•æ…‹è«‹æ±‚é »ç‡æ§åˆ¶ï¼Œé¿å…è¢«å°é–
4. æ‰¹é‡æ•¸æ“šåº«æ“ä½œï¼Œæ¸›å°‘I/Oé–‹éŠ·
5. è©³ç´°æ€§èƒ½ç›£æ§å’Œçµ±è¨ˆ

ä½œè€…ï¼šAI Trading System
ç‰ˆæœ¬ï¼š2.0
æ—¥æœŸï¼š2025-07-29
"""

import sys
import os
import time
import logging
import requests
import pandas as pd
import threading
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import List, Tuple, Dict, Optional, Callable
from sqlalchemy import create_engine, text
from queue import Queue
import json

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from src.data_sources.real_data_crawler import RealDataCrawler

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)


@dataclass
class CrawlerConfig:
    """çˆ¬å–å™¨é…ç½®"""
    max_workers: int = 4  # æœ€å¤§ä¸¦è¡Œç·šç¨‹æ•¸
    request_delay: float = 0.5  # åŸºç¤è«‹æ±‚é–“éš”ï¼ˆç§’ï¼‰
    max_retries: int = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
    retry_delay: float = 2.0  # é‡è©¦é–“éš”
    batch_size: int = 50  # æ‰¹é‡è™•ç†å¤§å°
    enable_incremental: bool = True  # å•Ÿç”¨å¢é‡æ›´æ–°
    data_freshness_hours: int = 24  # æ•¸æ“šæ–°é®®åº¦ï¼ˆå°æ™‚ï¼‰
    enable_performance_monitor: bool = True  # å•Ÿç”¨æ€§èƒ½ç›£æ§


@dataclass
class PerformanceStats:
    """æ€§èƒ½çµ±è¨ˆ"""
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    cache_hits: int = 0
    total_records: int = 0
    parallel_efficiency: float = 0.0
    avg_request_time: float = 0.0
    
    def calculate_metrics(self):
        """è¨ˆç®—æ€§èƒ½æŒ‡æ¨™"""
        if self.end_time:
            duration = (self.end_time - self.start_time).total_seconds()
            self.avg_request_time = duration / max(self.total_requests, 1)
            if self.total_requests > 0:
                self.parallel_efficiency = self.successful_requests / self.total_requests


class EnhancedRealDataCrawler(RealDataCrawler):
    """å¢å¼·ç‰ˆçœŸå¯¦æ•¸æ“šçˆ¬å–å™¨"""
    
    def __init__(self, db_path: str = 'sqlite:///data/enhanced_stock_database.db', 
                 config: Optional[CrawlerConfig] = None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆçˆ¬å–å™¨
        
        Args:
            db_path: è³‡æ–™åº«é€£æ¥è·¯å¾‘
            config: çˆ¬å–å™¨é…ç½®
        """
        super().__init__(db_path)
        self.config = config or CrawlerConfig()
        self.performance_stats = PerformanceStats()
        
        # ç·šç¨‹å®‰å…¨çš„çµ±è¨ˆè¨ˆæ•¸å™¨
        self._stats_lock = threading.Lock()
        self._request_times = Queue()
        
        # å‹•æ…‹è«‹æ±‚é »ç‡æ§åˆ¶
        self._last_request_time = 0
        self._request_count = 0
        self._rate_limit_lock = threading.Lock()
        
        logger.info(f"å¢å¼·ç‰ˆçˆ¬å–å™¨åˆå§‹åŒ–å®Œæˆ - ä¸¦è¡Œåº¦: {self.config.max_workers}")
    
    def _update_stats(self, success: bool, records: int = 0, is_cache_hit: bool = False):
        """ç·šç¨‹å®‰å…¨çš„çµ±è¨ˆæ›´æ–°"""
        with self._stats_lock:
            self.performance_stats.total_requests += 1
            if success:
                self.performance_stats.successful_requests += 1
                self.performance_stats.total_records += records
            else:
                self.performance_stats.failed_requests += 1
            
            if is_cache_hit:
                self.performance_stats.cache_hits += 1
    
    def _rate_limit_delay(self):
        """å‹•æ…‹è«‹æ±‚é »ç‡æ§åˆ¶"""
        with self._rate_limit_lock:
            current_time = time.time()
            time_since_last = current_time - self._last_request_time
            
            # å‹•æ…‹èª¿æ•´å»¶é²æ™‚é–“
            if self._request_count > 10:
                # å¦‚æœè«‹æ±‚é »ç¹ï¼Œå¢åŠ å»¶é²
                delay = self.config.request_delay * 1.5
            else:
                delay = self.config.request_delay
            
            if time_since_last < delay:
                sleep_time = delay - time_since_last
                time.sleep(sleep_time)
            
            self._last_request_time = time.time()
            self._request_count += 1
    
    def check_data_exists(self, symbol: str, start_date: date, end_date: date) -> Dict[str, List[date]]:
        """
        æª¢æŸ¥æ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„æ•¸æ“š

        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            åŒ…å«å·²å­˜åœ¨å’Œç¼ºå¤±æ—¥æœŸçš„å­—å…¸
        """
        try:
            with self.engine.connect() as conn:
                # æŸ¥è©¢å·²å­˜åœ¨çš„æ•¸æ“š
                query = text("""
                    SELECT DISTINCT date, created_at
                    FROM real_stock_data
                    WHERE symbol = :symbol
                    AND date >= :start_date
                    AND date <= :end_date
                    ORDER BY date
                """)

                result = conn.execute(query, {
                    "symbol": symbol,
                    "start_date": start_date,
                    "end_date": end_date
                })

                existing_data = result.fetchall()
                existing_dates = set()
                fresh_dates = set()

                # æª¢æŸ¥æ•¸æ“šæ–°é®®åº¦ - æ”¹é€²é‚è¼¯
                cutoff_time = datetime.now() - timedelta(hours=self.config.data_freshness_hours)
                today = datetime.now().date()

                for row in existing_data:
                    date_val = pd.to_datetime(row[0]).date()
                    created_at = pd.to_datetime(row[1])

                    existing_dates.add(date_val)

                    # æ”¹é€²çš„æ–°é®®åº¦é‚è¼¯ï¼š
                    # 1. æ­·å²æ•¸æ“šï¼ˆè¶…é2å¤©å‰ï¼‰ï¼šåªè¦å­˜åœ¨å°±èªç‚ºæ˜¯æ–°é®®çš„
                    # 2. è¿‘æœŸæ•¸æ“šï¼ˆ2å¤©å…§ï¼‰ï¼šéœ€è¦æª¢æŸ¥å‰µå»ºæ™‚é–“
                    if date_val < today - timedelta(days=2):
                        # æ­·å²æ•¸æ“šï¼Œåªè¦å­˜åœ¨å°±æ˜¯æ–°é®®çš„
                        fresh_dates.add(date_val)
                    elif created_at > cutoff_time:
                        # è¿‘æœŸæ•¸æ“šï¼Œéœ€è¦æª¢æŸ¥æ–°é®®åº¦
                        fresh_dates.add(date_val)

                # ç”Ÿæˆæ‰€æœ‰éœ€è¦çš„æ—¥æœŸï¼ˆæ’é™¤é€±æœ«ï¼‰
                all_dates = []
                current = start_date
                while current <= end_date:
                    # æ’é™¤é€±æœ«
                    if current.weekday() < 5:
                        all_dates.append(current)
                    current += timedelta(days=1)

                # è¨ˆç®—çœŸæ­£ç¼ºå¤±çš„æ—¥æœŸï¼ˆä¸åœ¨æ–°é®®æ•¸æ“šä¸­çš„æ—¥æœŸï¼‰
                missing_dates = [d for d in all_dates if d not in fresh_dates]

                # æ”¹é€²çš„ needs_update é‚è¼¯ï¼šåªæœ‰ç•¶æœ‰çœŸæ­£ç¼ºå¤±çš„æ—¥æœŸæ™‚æ‰éœ€è¦æ›´æ–°
                needs_update = len(missing_dates) > 0

                return {
                    'existing': list(existing_dates),
                    'fresh': list(fresh_dates),
                    'missing': missing_dates,
                    'needs_update': needs_update
                }

        except Exception as e:
            logger.error(f"æª¢æŸ¥æ•¸æ“šå­˜åœ¨æ€§å¤±æ•—: {e}")
            return {
                'existing': [],
                'fresh': [],
                'missing': [],
                'needs_update': True
            }
    
    def crawl_stock_data_parallel(self, symbol: str, year: int, month: int) -> pd.DataFrame:
        """
        ä¸¦è¡Œå®‰å…¨çš„è‚¡ç¥¨æ•¸æ“šçˆ¬å–
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½
            month: æœˆä»½
            
        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        start_time = time.time()
        
        try:
            # æ‡‰ç”¨è«‹æ±‚é »ç‡é™åˆ¶
            self._rate_limit_delay()
            
            # èª¿ç”¨çˆ¶é¡çš„çˆ¬å–æ–¹æ³•
            df = super().crawl_stock_data(symbol, year, month)
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_stats(
                success=not df.empty,
                records=len(df) if not df.empty else 0
            )
            
            # è¨˜éŒ„è«‹æ±‚æ™‚é–“
            request_time = time.time() - start_time
            if not self._request_times.full():
                self._request_times.put(request_time)
            
            return df
            
        except Exception as e:
            self._update_stats(success=False)
            logger.error(f"ä¸¦è¡Œçˆ¬å– {symbol} {year}-{month:02d} å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_stock_data_range_enhanced(self, symbol: str, start_date: date, end_date: date) -> pd.DataFrame:
        """
        å¢å¼·ç‰ˆæ—¥æœŸç¯„åœçˆ¬å– - æ”¯æŒå¢é‡æ›´æ–°å’Œä¸¦è¡Œè™•ç†

        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        logger.info(f"é–‹å§‹å¢å¼·ç‰ˆçˆ¬å– {symbol} æ—¥æœŸç¯„åœ {start_date} è‡³ {end_date}")

        # æª¢æŸ¥å¢é‡æ›´æ–°
        if self.config.enable_incremental:
            data_status = self.check_data_exists(symbol, start_date, end_date)

            if not data_status['needs_update']:
                logger.info(f"âœ… {symbol} æ•¸æ“šå·²æ˜¯æœ€æ–°ï¼Œè·³éçˆ¬å– (æ–°é®®æ•¸æ“š: {len(data_status['fresh'])} å€‹æ—¥æœŸ)")
                self._update_stats(success=True, records=len(data_status['fresh']), is_cache_hit=True)

                # å¾æ•¸æ“šåº«è¿”å›ç¾æœ‰æ•¸æ“š
                return self._get_existing_data(symbol, start_date, end_date)

            logger.info(f"ğŸ“Š {symbol} éœ€è¦æ›´æ–° {len(data_status['missing'])} å€‹æ—¥æœŸ (å·²æœ‰: {len(data_status['fresh'])}, ç¼ºå¤±: {len(data_status['missing'])})")
        
        # ç”Ÿæˆéœ€è¦çˆ¬å–çš„æœˆä»½åˆ—è¡¨
        months_to_crawl = self._generate_month_list(start_date, end_date)
        
        if len(months_to_crawl) == 1:
            # å–®æœˆæ•¸æ“šï¼Œç›´æ¥çˆ¬å–
            year, month = months_to_crawl[0]
            df = self.crawl_stock_data_parallel(symbol, year, month)
            
            if not df.empty:
                # éæ¿¾æ—¥æœŸç¯„åœ
                df['date'] = pd.to_datetime(df['date'])
                df_filtered = df[
                    (df['date'].dt.date >= start_date) & 
                    (df['date'].dt.date <= end_date)
                ]
                return df_filtered
            
            return pd.DataFrame()
        
        else:
            # å¤šæœˆæ•¸æ“šï¼Œä½¿ç”¨ä¸¦è¡Œè™•ç†
            return self._crawl_multiple_months_parallel(symbol, months_to_crawl, start_date, end_date)
    
    def _crawl_multiple_months_parallel(self, symbol: str, months: List[Tuple[int, int]], 
                                      start_date: date, end_date: date) -> pd.DataFrame:
        """
        ä¸¦è¡Œçˆ¬å–å¤šå€‹æœˆä»½çš„æ•¸æ“š
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            months: æœˆä»½åˆ—è¡¨
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            
        Returns:
            åˆä½µå¾Œçš„æ•¸æ“šDataFrame
        """
        all_data = []
        
        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=min(self.config.max_workers, len(months))) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_month = {
                executor.submit(self.crawl_stock_data_parallel, symbol, year, month): (year, month)
                for year, month in months
            }
            
            # æ”¶é›†çµæœ
            for future in as_completed(future_to_month):
                year, month = future_to_month[future]
                
                try:
                    df_month = future.result()
                    
                    if not df_month.empty:
                        # éæ¿¾æ—¥æœŸç¯„åœ
                        df_month['date'] = pd.to_datetime(df_month['date'])
                        df_filtered = df_month[
                            (df_month['date'].dt.date >= start_date) & 
                            (df_month['date'].dt.date <= end_date)
                        ]
                        
                        if not df_filtered.empty:
                            all_data.append(df_filtered)
                            logger.info(f"âœ… {symbol} {year}-{month:02d}: {len(df_filtered)} ç­†è¨˜éŒ„")
                        else:
                            logger.info(f"âš ï¸ {symbol} {year}-{month:02d}: ç„¡ç¬¦åˆæ—¥æœŸç¯„åœçš„è¨˜éŒ„")
                    else:
                        logger.warning(f"âŒ {symbol} {year}-{month:02d}: çˆ¬å–å¤±æ•—")
                        
                except Exception as e:
                    logger.error(f"âŒ è™•ç† {symbol} {year}-{month:02d} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # åˆä½µæ‰€æœ‰æ•¸æ“š
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            result_df = result_df.sort_values('date').reset_index(drop=True)
            
            logger.info(f"âœ… {symbol} ä¸¦è¡Œçˆ¬å–å®Œæˆ: ç¸½å…± {len(result_df)} ç­†è¨˜éŒ„")
            return result_df
        else:
            logger.warning(f"âš ï¸ {symbol} åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§ç„¡å¯ç”¨æ•¸æ“š")
            return pd.DataFrame()
    
    def _get_existing_data(self, symbol: str, start_date: date, end_date: date) -> pd.DataFrame:
        """å¾æ•¸æ“šåº«ç²å–ç¾æœ‰æ•¸æ“š"""
        try:
            with self.engine.connect() as conn:
                query = text("""
                    SELECT symbol, date, open, high, low, close, volume, source, created_at
                    FROM real_stock_data
                    WHERE symbol = :symbol 
                    AND date >= :start_date 
                    AND date <= :end_date
                    ORDER BY date
                """)
                
                result = conn.execute(query, {
                    "symbol": symbol,
                    "start_date": start_date,
                    "end_date": end_date
                })
                
                rows = result.fetchall()
                
                if rows:
                    df = pd.DataFrame(rows, columns=[
                        'symbol', 'date', 'open', 'high', 'low', 'close',
                        'volume', 'source', 'created_at'
                    ])
                    df['date'] = pd.to_datetime(df['date'])
                    return df
                
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ç²å–ç¾æœ‰æ•¸æ“šå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def get_performance_report(self) -> Dict:
        """ç²å–æ€§èƒ½å ±å‘Š"""
        self.performance_stats.end_time = datetime.now()
        self.performance_stats.calculate_metrics()
        
        duration = (self.performance_stats.end_time - self.performance_stats.start_time).total_seconds()
        
        return {
            'duration_seconds': round(duration, 2),
            'total_requests': self.performance_stats.total_requests,
            'successful_requests': self.performance_stats.successful_requests,
            'failed_requests': self.performance_stats.failed_requests,
            'success_rate': round(self.performance_stats.successful_requests / max(self.performance_stats.total_requests, 1) * 100, 2),
            'cache_hits': self.performance_stats.cache_hits,
            'cache_hit_rate': round(self.performance_stats.cache_hits / max(self.performance_stats.total_requests, 1) * 100, 2),
            'total_records': self.performance_stats.total_records,
            'records_per_second': round(self.performance_stats.total_records / max(duration, 1), 2),
            'avg_request_time': round(self.performance_stats.avg_request_time, 3),
            'parallel_efficiency': round(self.performance_stats.parallel_efficiency * 100, 2)
        }
    
    def save_to_database_batch(self, dataframes: List[pd.DataFrame]):
        """
        æ‰¹é‡ä¿å­˜æ•¸æ“šåˆ°è³‡æ–™åº«
        
        Args:
            dataframes: è¦ä¿å­˜çš„DataFrameåˆ—è¡¨
        """
        if not dataframes:
            return
        
        # åˆä½µæ‰€æœ‰DataFrame
        combined_df = pd.concat(dataframes, ignore_index=True)
        
        if not combined_df.empty:
            self.save_to_database(combined_df)
            logger.info(f"æ‰¹é‡ä¿å­˜ {len(combined_df)} ç­†è¨˜éŒ„åˆ°è³‡æ–™åº«")


# å‘å¾Œå…¼å®¹æ€§åŒ…è£å™¨
def create_enhanced_crawler(enable_parallel: bool = True, max_workers: int = 4) -> RealDataCrawler:
    """
    å‰µå»ºå¢å¼·ç‰ˆçˆ¬å–å™¨çš„å·¥å» å‡½æ•¸
    
    Args:
        enable_parallel: æ˜¯å¦å•Ÿç”¨ä¸¦è¡Œè™•ç†
        max_workers: æœ€å¤§ä¸¦è¡Œç·šç¨‹æ•¸
        
    Returns:
        çˆ¬å–å™¨å¯¦ä¾‹
    """
    if enable_parallel:
        config = CrawlerConfig(max_workers=max_workers)
        return EnhancedRealDataCrawler(config=config)
    else:
        return RealDataCrawler()


if __name__ == "__main__":
    # æ¸¬è©¦å¢å¼·ç‰ˆçˆ¬å–å™¨
    config = CrawlerConfig(max_workers=3, enable_incremental=True)
    crawler = EnhancedRealDataCrawler(config=config)
    
    # æ¸¬è©¦ä¸¦è¡Œçˆ¬å–
    symbol = '2330.TW'
    start_date = date(2025, 6, 1)
    end_date = date(2025, 7, 29)
    
    print(f"é–‹å§‹æ¸¬è©¦å¢å¼·ç‰ˆçˆ¬å–å™¨ - {symbol} ({start_date} è‡³ {end_date})")
    
    start_time = time.time()
    df = crawler.crawl_stock_data_range_enhanced(symbol, start_date, end_date)
    end_time = time.time()
    
    print(f"\nâœ… çˆ¬å–å®Œæˆ:")
    print(f"   è¨˜éŒ„æ•¸: {len(df)}")
    print(f"   è€—æ™‚: {end_time - start_time:.2f} ç§’")
    
    if not df.empty:
        print(f"   æ—¥æœŸç¯„åœ: {df['date'].min().date()} è‡³ {df['date'].max().date()}")
        print(f"   åƒ¹æ ¼ç¯„åœ: {df['close'].min():.2f} - {df['close'].max():.2f}")
        
        # ä¿å­˜åˆ°è³‡æ–™åº«
        crawler.save_to_database(df)
    
    # é¡¯ç¤ºæ€§èƒ½å ±å‘Š
    report = crawler.get_performance_report()
    print(f"\nğŸ“Š æ€§èƒ½å ±å‘Š:")
    for key, value in report.items():
        print(f"   {key}: {value}")
