#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå¯¦æ•¸æ“šçˆ¬å–å™¨ - åŸºæ–¼å®˜æ–¹äº¤æ˜“æ‰€æ¸ é“
=====================================

ä½¿ç”¨TWSEå®˜æ–¹APIå’Œæ«ƒè²·ä¸­å¿ƒç²å–æº–ç¢ºçš„è‚¡ç¥¨æ•¸æ“šï¼Œ
æ›¿ä»£ä¹‹å‰ä¸æº–ç¢ºçš„æ¨¡æ“¬æ•¸æ“šã€‚

æ•¸æ“šä¾†æºå„ªå…ˆç´šï¼š
1. TWSEå®˜æ–¹API (æœ€é«˜æº–ç¢ºæ€§)
2. æ«ƒè²·ä¸­å¿ƒ (ä¸Šæ«ƒè‚¡ç¥¨)
3. Yahoo Finance (å‚™æ´)

Author: AI Trading System
Date: 2025-01-28
Version: 1.0
"""

import requests
import pandas as pd
import yfinance as yf
from sqlalchemy import create_engine, text
from datetime import datetime, date, timedelta
import time
import logging
from typing import Dict, List, Optional, Tuple
import json

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealDataCrawler:
    """çœŸå¯¦æ•¸æ“šçˆ¬å–å™¨ - åŸºæ–¼å®˜æ–¹äº¤æ˜“æ‰€æ¸ é“"""
    
    def __init__(self, db_path: str = 'sqlite:///data/real_stock_database.db'):
        """
        åˆå§‹åŒ–çœŸå¯¦æ•¸æ“šçˆ¬å–å™¨
        
        Args:
            db_path: è³‡æ–™åº«é€£æ¥è·¯å¾‘
        """
        self.db_path = db_path
        self.engine = create_engine(db_path)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'twse_requests': 0,
            'tpex_requests': 0,
            'yahoo_requests': 0,
            'total_records': 0
        }

        # æ™ºèƒ½çˆ¬å–çµ±è¨ˆ
        self.scan_stats = {
            'total_symbols': 0,
            'existing_symbols': 0,
            'missing_symbols': 0,
            'partial_symbols': 0,
            'skipped_records': 0,
            'new_records_needed': 0
        }
        
        # å‰µå»ºè³‡æ–™åº«è¡¨
        self._create_tables()
        
        logger.info("RealDataCrawler åˆå§‹åŒ–å®Œæˆ")

    def scan_existing_data(self, symbols: List[str], start_date: date, end_date: date) -> Dict:
        """
        æƒææ•¸æ“šåº«ä¸­å·²å­˜åœ¨çš„æ•¸æ“šï¼Œè­˜åˆ¥éœ€è¦çˆ¬å–çš„ç¼ºå¤±æ•¸æ“š

        Args:
            symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            æƒæçµæœçµ±è¨ˆ
        """
        scan_result = {
            'existing_symbols': [],
            'missing_symbols': [],
            'partial_symbols': [],
            'complete_symbols': [],
            'total_existing_records': 0,
            'total_missing_records': 0,
            'scan_summary': {}
        }

        logger.info(f"é–‹å§‹æƒææ•¸æ“šåº«ï¼Œç›®æ¨™æœŸé–“: {start_date} åˆ° {end_date}")

        # è¨ˆç®—æœŸé–“å…§çš„äº¤æ˜“æ—¥æ•¸é‡ï¼ˆç°¡åŒ–ä¼°ç®—ï¼‰
        total_days = (end_date - start_date).days + 1
        expected_records_per_symbol = int(total_days * 0.7)  # å‡è¨­70%ç‚ºäº¤æ˜“æ—¥

        with self.engine.connect() as conn:
            for symbol in symbols:
                try:
                    # æŸ¥è©¢è©²è‚¡ç¥¨åœ¨æŒ‡å®šæœŸé–“çš„æ•¸æ“š
                    query = text("""
                        SELECT COUNT(*) as count, MIN(date) as min_date, MAX(date) as max_date
                        FROM real_stock_data
                        WHERE symbol = :symbol
                        AND date >= :start_date
                        AND date <= :end_date
                    """)

                    result = conn.execute(query, {
                        'symbol': symbol,
                        'start_date': start_date.strftime('%Y-%m-%d'),
                        'end_date': end_date.strftime('%Y-%m-%d')
                    }).fetchone()

                    record_count = result[0] if result else 0

                    if record_count == 0:
                        # å®Œå…¨æ²’æœ‰æ•¸æ“š
                        scan_result['missing_symbols'].append(symbol)
                        scan_result['total_missing_records'] += expected_records_per_symbol
                    elif record_count < expected_records_per_symbol * 0.8:  # å°‘æ–¼80%èªç‚ºæ˜¯éƒ¨åˆ†æ•¸æ“š
                        # éƒ¨åˆ†æ•¸æ“š
                        scan_result['partial_symbols'].append(symbol)
                        missing_records = expected_records_per_symbol - record_count
                        scan_result['total_missing_records'] += missing_records
                        scan_result['total_existing_records'] += record_count
                    else:
                        # æ•¸æ“šå®Œæ•´
                        scan_result['complete_symbols'].append(symbol)
                        scan_result['total_existing_records'] += record_count

                    # è¨˜éŒ„è©³ç´°ä¿¡æ¯
                    scan_result['scan_summary'][symbol] = {
                        'existing_records': record_count,
                        'expected_records': expected_records_per_symbol,
                        'completeness': record_count / expected_records_per_symbol if expected_records_per_symbol > 0 else 0,
                        'min_date': result[1] if result and result[1] else None,
                        'max_date': result[2] if result and result[2] else None
                    }

                except Exception as e:
                    logger.error(f"æƒæ {symbol} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    scan_result['missing_symbols'].append(symbol)

        # æ›´æ–°çµ±è¨ˆ
        self.scan_stats.update({
            'total_symbols': len(symbols),
            'existing_symbols': len(scan_result['complete_symbols']),
            'missing_symbols': len(scan_result['missing_symbols']),
            'partial_symbols': len(scan_result['partial_symbols']),
            'skipped_records': scan_result['total_existing_records'],
            'new_records_needed': scan_result['total_missing_records']
        })

        logger.info(f"æ•¸æ“šåº«æƒæå®Œæˆ: å®Œæ•´ {len(scan_result['complete_symbols'])} æª”, "
                   f"éƒ¨åˆ† {len(scan_result['partial_symbols'])} æª”, "
                   f"ç¼ºå¤± {len(scan_result['missing_symbols'])} æª”")

        return scan_result

    def crawl_missing_data_only(self, symbols: List[str], start_date: date, end_date: date) -> Dict:
        """
        æ™ºèƒ½çˆ¬å–ï¼šåªçˆ¬å–æ•¸æ“šåº«ä¸­ç¼ºå¤±çš„æ•¸æ“š

        Args:
            symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        # å…ˆæƒæç¾æœ‰æ•¸æ“š
        scan_result = self.scan_existing_data(symbols, start_date, end_date)

        # ç¢ºå®šéœ€è¦çˆ¬å–çš„è‚¡ç¥¨
        symbols_to_crawl = scan_result['missing_symbols'] + scan_result['partial_symbols']

        if not symbols_to_crawl:
            logger.info("æ‰€æœ‰è‚¡ç¥¨æ•¸æ“šå·²å­˜åœ¨ï¼Œç„¡éœ€çˆ¬å–")
            return {
                'success': True,
                'skipped_symbols': len(scan_result['complete_symbols']),
                'crawled_symbols': 0,
                'total_new_records': 0,
                'failed_symbols': [],
                'scan_result': scan_result
            }

        logger.info(f"ç™¼ç¾ {len(scan_result['complete_symbols'])} æª”è‚¡ç¥¨å·²æœ‰å®Œæ•´æ•¸æ“šï¼Œ"
                   f"å°‡çˆ¬å– {len(symbols_to_crawl)} æª”æ–°è‚¡ç¥¨")

        # çˆ¬å–ç¼ºå¤±çš„æ•¸æ“š
        crawl_result = {
            'success': True,
            'skipped_symbols': len(scan_result['complete_symbols']),
            'crawled_symbols': 0,
            'total_new_records': 0,
            'failed_symbols': [],
            'scan_result': scan_result
        }

        for symbol in symbols_to_crawl:
            try:
                logger.info(f"çˆ¬å–ç¼ºå¤±æ•¸æ“š: {symbol}")

                # å°æ–¼éƒ¨åˆ†æ•¸æ“šçš„è‚¡ç¥¨ï¼Œå¯ä»¥é€²ä¸€æ­¥å„ªåŒ–åªçˆ¬å–ç¼ºå¤±çš„æ—¥æœŸç¯„åœ
                if symbol in scan_result['partial_symbols']:
                    # é€™è£¡å¯ä»¥å¯¦ç¾æ›´ç²¾ç´°çš„æ—¥æœŸç¯„åœçˆ¬å–
                    # æš«æ™‚ä½¿ç”¨å®Œæ•´ç¯„åœçˆ¬å–ï¼Œä¾è³´æ•¸æ“šåº«çš„UNIQUEç´„æŸé¿å…é‡è¤‡
                    pass

                # çˆ¬å–æ•¸æ“š
                df = self.crawl_stock_data_range(symbol, start_date, end_date)

                if not df.empty:
                    # ä¿å­˜åˆ°æ•¸æ“šåº«ï¼ˆæœƒè‡ªå‹•è·³éé‡è¤‡è¨˜éŒ„ï¼‰
                    self.save_to_database(df)
                    crawl_result['crawled_symbols'] += 1
                    crawl_result['total_new_records'] += len(df)
                    logger.info(f"âœ… {symbol}: çˆ¬å– {len(df)} ç­†è¨˜éŒ„")
                else:
                    crawl_result['failed_symbols'].append(symbol)
                    logger.warning(f"âš ï¸ {symbol}: çˆ¬å–å¤±æ•—æˆ–ç„¡æ•¸æ“š")

            except Exception as e:
                crawl_result['failed_symbols'].append(symbol)
                logger.error(f"âŒ {symbol}: çˆ¬å–éŒ¯èª¤ - {e}")

        logger.info(f"æ™ºèƒ½çˆ¬å–å®Œæˆ: è·³é {crawl_result['skipped_symbols']} æª”, "
                   f"çˆ¬å– {crawl_result['crawled_symbols']} æª”, "
                   f"æ–°å¢ {crawl_result['total_new_records']} ç­†è¨˜éŒ„")

        return crawl_result

    def _create_tables(self):
        """å‰µå»ºè³‡æ–™åº«è¡¨"""
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS real_stock_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            symbol TEXT NOT NULL,
            date DATE NOT NULL,
            open REAL NOT NULL,
            high REAL NOT NULL,
            low REAL NOT NULL,
            close REAL NOT NULL,
            volume INTEGER NOT NULL,
            source TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(symbol, date)
        )
        """
        
        with self.engine.connect() as conn:
            conn.execute(text(create_table_sql))
            conn.commit()
    
    def validate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é©—è­‰è‚¡åƒ¹æ•¸æ“šæº–ç¢ºæ€§
        
        Args:
            df: åŸå§‹æ•¸æ“šDataFrame
            
        Returns:
            é©—è­‰å¾Œçš„æœ‰æ•ˆæ•¸æ“š
        """
        if df.empty:
            return df
        
        # æª¢æŸ¥åƒ¹æ ¼ > 0 å’Œ OHLC é‚è¼¯
        valid_mask = (
            (df['open'] > 0) & 
            (df['high'] >= df['open']) & 
            (df['high'] >= df['close']) & 
            (df['low'] <= df['open']) & 
            (df['low'] <= df['close']) & 
            (df['volume'] >= 0) &
            (df['high'] >= df['low'])  # æœ€é«˜åƒ¹ >= æœ€ä½åƒ¹
        )
        
        valid_df = df[valid_mask].copy()
        
        if len(valid_df) < len(df):
            logger.warning(f"æ•¸æ“šé©—è­‰ï¼šç§»é™¤äº† {len(df) - len(valid_df)} ç­†ç„¡æ•ˆè¨˜éŒ„")
        
        return valid_df
    
    def crawl_twse_data(self, symbol: str, year: int, month: int) -> pd.DataFrame:
        """
        å¾TWSEçˆ¬å–å–®æœˆè‚¡åƒ¹æ•¸æ“š
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼ (å¦‚ '2330.TW')
            year: å¹´ä»½
            month: æœˆä»½
            
        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        stock_code = symbol.split('.')[0]  # ç§»é™¤.TWå¾Œç¶´
        url = f"https://www.twse.com.tw/exchangeReport/STOCK_DAY?response=json&date={year}{month:02d}01&stockNo={stock_code}&_={int(time.time())}"
        
        try:
            logger.info(f"æ­£åœ¨å¾TWSEçˆ¬å– {symbol} {year}-{month:02d} æ•¸æ“š...")
            
            self.stats['total_requests'] += 1
            self.stats['twse_requests'] += 1
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if 'data' in data and data['data']:
                columns = data['fields']
                df = pd.DataFrame(data['data'], columns=columns)
                
                # è½‰æ›æ°‘åœ‹æ—¥æœŸç‚ºè¥¿å…ƒæ—¥æœŸ
                df['date'] = pd.to_datetime(df['æ—¥æœŸ'].apply(
                    lambda x: f"{int(x.split('/')[0]) + 1911}-{x.split('/')[1]}-{x.split('/')[2]}"
                ))
                
                # é¸å–OHLCVæ¬„ä½
                df = df[['date', 'é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹', 'æ”¶ç›¤åƒ¹', 'æˆäº¤è‚¡æ•¸']].copy()
                df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']
                
                # è½‰æ›æ•¸æ“šé¡å‹ï¼Œè™•ç†é€—è™Ÿåˆ†éš”çš„æ•¸å­—å’Œç‰¹æ®Šå€¼
                for col in ['open', 'high', 'low', 'close']:
                    # è™•ç† "--" å’Œå…¶ä»–ç„¡æ•ˆå€¼
                    df[col] = df[col].str.replace(',', '').replace('--', '0').replace('', '0')
                    # è½‰æ›ç‚ºæ•¸å€¼ï¼Œç„¡æ•ˆå€¼è¨­ç‚ºNaN
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    # å°‡NaNå’Œ0å€¼çš„è¡Œç§»é™¤
                    df = df[df[col].notna() & (df[col] > 0)]

                # è™•ç†æˆäº¤é‡
                df['volume'] = df['volume'].str.replace(',', '').replace('--', '0').replace('', '0')
                df['volume'] = pd.to_numeric(df['volume'], errors='coerce').fillna(0).astype(int)
                df['symbol'] = symbol
                df['source'] = 'TWSE'
                
                # é©—è­‰æ•¸æ“š
                df = self.validate_data(df)
                
                self.stats['successful_requests'] += 1
                self.stats['total_records'] += len(df)
                
                logger.info(f"âœ… TWSEæˆåŠŸçˆ¬å– {symbol} æ•¸æ“š: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                raise ValueError("TWSE APIè¿”å›ç©ºæ•¸æ“š")
                
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ TWSEçˆ¬å– {symbol} å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_tpex_data(self, symbol: str, year: int, month: int) -> pd.DataFrame:
        """
        å¾æ«ƒè²·ä¸­å¿ƒçˆ¬å–ä¸Šæ«ƒè‚¡ç¥¨æ•¸æ“š
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½
            month: æœˆä»½
            
        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        stock_code = symbol.split('.')[0]
        url = f"https://www.tpex.org.tw/web/stock/aftertrading/daily_trading_info/st43_result.php?l=zh-tw&d={year}/{month:02d}&stkno={stock_code}&_={int(time.time())}"
        
        try:
            logger.info(f"æ­£åœ¨å¾TPEXçˆ¬å– {symbol} {year}-{month:02d} æ•¸æ“š...")
            
            self.stats['total_requests'] += 1
            self.stats['tpex_requests'] += 1
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            # æª¢æŸ¥éŸ¿æ‡‰å…§å®¹æ˜¯å¦ç‚ºæœ‰æ•ˆJSON
            response_text = response.text.strip()
            if not response_text or response_text.startswith('<'):
                raise ValueError("TPEXè¿”å›ç„¡æ•ˆéŸ¿æ‡‰ï¼ˆå¯èƒ½æ˜¯HTMLéŒ¯èª¤é é¢ï¼‰")

            try:
                data = response.json()
            except json.JSONDecodeError as e:
                raise ValueError(f"TPEXéŸ¿æ‡‰JSONè§£æå¤±æ•—: {e}")
            
            if 'aaData' in data and data['aaData']:
                df_data = []
                for row in data['aaData']:
                    # TPEXæ•¸æ“šæ ¼å¼è™•ç†ï¼Œè·³éåŒ…å« "--" çš„è¡Œ
                    try:
                        # æª¢æŸ¥é—œéµåƒ¹æ ¼æ¬„ä½æ˜¯å¦æœ‰æ•ˆ
                        if any(val in ['--', '', None] for val in [row[2], row[4], row[5], row[6]]):
                            continue

                        date_parts = row[0].split('/')
                        date_str = f"{int(date_parts[0]) + 1911}-{date_parts[1]}-{date_parts[2]}"

                        df_data.append({
                            'date': pd.to_datetime(date_str),
                            'open': float(row[4].replace(',', '')),
                            'high': float(row[5].replace(',', '')),
                            'low': float(row[6].replace(',', '')),
                            'close': float(row[2].replace(',', '')),
                            'volume': int(row[8].replace(',', '') if row[8] not in ['--', ''] else '0'),
                            'symbol': symbol,
                            'source': 'TPEX'
                        })
                    except (ValueError, IndexError) as e:
                        logger.warning(f"è·³éç„¡æ•ˆçš„TPEXæ•¸æ“šè¡Œ: {row}, éŒ¯èª¤: {e}")
                        continue
                
                df = pd.DataFrame(df_data)
                df = self.validate_data(df)
                
                self.stats['successful_requests'] += 1
                self.stats['total_records'] += len(df)
                
                logger.info(f"âœ… TPEXæˆåŠŸçˆ¬å– {symbol} æ•¸æ“š: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                raise ValueError("TPEX APIè¿”å›ç©ºæ•¸æ“š")
                
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ TPEXçˆ¬å– {symbol} å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_yahoo_finance_backup(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        Yahoo Financeå‚™æ´æ•¸æ“šæº
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        try:
            logger.info(f"æ­£åœ¨å¾Yahoo Financeçˆ¬å– {symbol} å‚™æ´æ•¸æ“š...")
            
            self.stats['total_requests'] += 1
            self.stats['yahoo_requests'] += 1
            
            # è½‰æ›è‚¡ç¥¨ä»£ç¢¼æ ¼å¼ - ä¿æŒ.TWæ ¼å¼ï¼Œä¸è½‰æ›ç‚º.TWO
            yahoo_symbol = symbol if symbol.endswith('.TW') else f"{symbol}.TW"
            
            df = yf.download(yahoo_symbol, start=start_date, end=end_date, progress=False)
            
            if not df.empty:
                df.reset_index(inplace=True)
                df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].copy()
                df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']
                df['symbol'] = symbol
                df['source'] = 'Yahoo Finance'
                
                # è½‰æ›æ•¸æ“šé¡å‹
                df['volume'] = df['volume'].astype(int)
                
                df = self.validate_data(df)
                
                self.stats['successful_requests'] += 1
                self.stats['total_records'] += len(df)
                
                logger.info(f"âœ… Yahoo FinanceæˆåŠŸçˆ¬å– {symbol} æ•¸æ“š: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                raise ValueError("Yahoo Financeè¿”å›ç©ºæ•¸æ“š")
                
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ Yahoo Financeçˆ¬å– {symbol} å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_stock_data(self, symbol: str, year: int, month: int) -> pd.DataFrame:
        """
        çˆ¬å–è‚¡ç¥¨æ•¸æ“š - å¤šæ•¸æ“šæºç­–ç•¥
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            year: å¹´ä»½
            month: æœˆä»½
            
        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        # é¦–å…ˆå˜—è©¦TWSE
        df = self.crawl_twse_data(symbol, year, month)
        
        if not df.empty:
            return df
        
        # å¦‚æœTWSEå¤±æ•—ï¼Œå˜—è©¦TPEX (é©ç”¨æ–¼ä¸Šæ«ƒè‚¡ç¥¨)
        logger.info(f"TWSEå¤±æ•—ï¼Œå˜—è©¦TPEX...")
        time.sleep(2)  # è«‹æ±‚é–“éš”
        df = self.crawl_tpex_data(symbol, year, month)
        
        if not df.empty:
            return df
        
        # æœ€å¾Œä½¿ç”¨Yahoo Financeå‚™æ´
        logger.info(f"TPEXå¤±æ•—ï¼Œä½¿ç”¨Yahoo Financeå‚™æ´...")
        time.sleep(2)  # è«‹æ±‚é–“éš”
        
        start_date = f"{year}-{month:02d}-01"
        if month == 12:
            end_date = f"{year + 1}-01-01"
        else:
            end_date = f"{year}-{month + 1:02d}-01"
        
        df = self.crawl_yahoo_finance_backup(symbol, start_date, end_date)
        
        return df

    def crawl_stock_data_range(self, symbol: str, start_date: date, end_date: date) -> pd.DataFrame:
        """
        çˆ¬å–æŒ‡å®šæ—¥æœŸç¯„åœçš„è‚¡ç¥¨æ•¸æ“š

        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            è‚¡åƒ¹æ•¸æ“šDataFrame
        """
        logger.info(f"é–‹å§‹çˆ¬å– {symbol} æ—¥æœŸç¯„åœ {start_date} è‡³ {end_date} çš„æ•¸æ“š")

        # ç”Ÿæˆéœ€è¦çˆ¬å–çš„æœˆä»½åˆ—è¡¨
        months_to_crawl = self._generate_month_list(start_date, end_date)

        all_data = []
        total_records = 0

        for year, month in months_to_crawl:
            try:
                logger.info(f"æ­£åœ¨çˆ¬å– {symbol} {year}-{month:02d} æ•¸æ“š...")

                # çˆ¬å–å–®æœˆæ•¸æ“š
                df_month = self.crawl_stock_data(symbol, year, month)

                if not df_month.empty:
                    # éæ¿¾æ—¥æœŸç¯„åœ
                    df_month['date'] = pd.to_datetime(df_month['date'])
                    df_filtered = df_month[
                        (df_month['date'].dt.date >= start_date) &
                        (df_month['date'].dt.date <= end_date)
                    ]

                    if not df_filtered.empty:
                        all_data.append(df_filtered)
                        total_records += len(df_filtered)
                        logger.info(f"âœ… {symbol} {year}-{month:02d}: {len(df_filtered)} ç­†è¨˜éŒ„")
                    else:
                        logger.info(f"âš ï¸ {symbol} {year}-{month:02d}: ç„¡ç¬¦åˆæ—¥æœŸç¯„åœçš„è¨˜éŒ„")
                else:
                    logger.warning(f"âŒ {symbol} {year}-{month:02d}: çˆ¬å–å¤±æ•—")

                # è«‹æ±‚é–“éš”ï¼Œé¿å…è¢«å°é–
                time.sleep(1)

            except Exception as e:
                logger.error(f"âŒ çˆ¬å– {symbol} {year}-{month:02d} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        # åˆä½µæ‰€æœ‰æ•¸æ“š
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            result_df = result_df.sort_values('date').reset_index(drop=True)

            logger.info(f"âœ… {symbol} æ—¥æœŸç¯„åœçˆ¬å–å®Œæˆ: ç¸½å…± {total_records} ç­†è¨˜éŒ„")
            return result_df
        else:
            logger.warning(f"âš ï¸ {symbol} åœ¨æŒ‡å®šæ—¥æœŸç¯„åœå…§ç„¡å¯ç”¨æ•¸æ“š")
            return pd.DataFrame()

    def _generate_month_list(self, start_date: date, end_date: date) -> List[Tuple[int, int]]:
        """
        ç”Ÿæˆéœ€è¦çˆ¬å–çš„æœˆä»½åˆ—è¡¨

        Args:
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            (å¹´ä»½, æœˆä»½) çš„åˆ—è¡¨
        """
        months = []
        current_date = start_date.replace(day=1)  # å¾æœˆåˆé–‹å§‹

        while current_date <= end_date:
            months.append((current_date.year, current_date.month))

            # ç§»å‹•åˆ°ä¸‹å€‹æœˆ
            if current_date.month == 12:
                current_date = current_date.replace(year=current_date.year + 1, month=1)
            else:
                current_date = current_date.replace(month=current_date.month + 1)

        return months

    def save_to_database(self, df: pd.DataFrame):
        """
        å­˜å…¥è³‡æ–™åº« - ä½¿ç”¨UPSERTé‚è¼¯
        
        Args:
            df: è¦å­˜å„²çš„æ•¸æ“šDataFrame
        """
        if df.empty:
            logger.warning("æ²’æœ‰æ•¸æ“šå¯å­˜å„²")
            return
        
        try:
            # æº–å‚™æ•¸æ“š
            df_to_save = df[['symbol', 'date', 'open', 'high', 'low', 'close', 'volume', 'source']].copy()
            
            records_inserted = 0
            records_updated = 0
            
            with self.engine.connect() as conn:
                for _, row in df_to_save.iterrows():
                    # æª¢æŸ¥è¨˜éŒ„æ˜¯å¦å·²å­˜åœ¨
                    check_sql = text("""
                        SELECT COUNT(*) as count FROM real_stock_data 
                        WHERE symbol = :symbol AND date = :date
                    """)
                    
                    result = conn.execute(check_sql, {
                        'symbol': row['symbol'],
                        'date': row['date'].strftime('%Y-%m-%d')
                    }).fetchone()
                    
                    if result[0] == 0:
                        # æ’å…¥æ–°è¨˜éŒ„
                        insert_sql = text("""
                            INSERT INTO real_stock_data (symbol, date, open, high, low, close, volume, source)
                            VALUES (:symbol, :date, :open, :high, :low, :close, :volume, :source)
                        """)
                        
                        row_dict = row.to_dict()
                        row_dict['date'] = row['date'].strftime('%Y-%m-%d')
                        conn.execute(insert_sql, row_dict)
                        records_inserted += 1
                    else:
                        # æ›´æ–°ç¾æœ‰è¨˜éŒ„
                        update_sql = text("""
                            UPDATE real_stock_data
                            SET open = :open, high = :high, low = :low, close = :close,
                                volume = :volume, source = :source
                            WHERE symbol = :symbol AND date = :date
                        """)

                        row_dict = row.to_dict()
                        row_dict['date'] = row['date'].strftime('%Y-%m-%d')
                        conn.execute(update_sql, row_dict)
                        records_updated += 1
                
                conn.commit()
            
            logger.info(f"âœ… æ•¸æ“šå­˜å„²å®Œæˆ: æ–°å¢ {records_inserted} ç­†, æ›´æ–° {records_updated} ç­†")
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šå­˜å„²å¤±æ•—: {e}")
    
    def get_stats(self) -> Dict:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return self.stats.copy()
    
    def print_stats(self):
        """æ‰“å°çµ±è¨ˆä¿¡æ¯"""
        print("\nğŸ“Š çˆ¬å–çµ±è¨ˆä¿¡æ¯:")
        print("=" * 50)
        print(f"ç¸½è«‹æ±‚æ•¸: {self.stats['total_requests']}")
        print(f"æˆåŠŸè«‹æ±‚æ•¸: {self.stats['successful_requests']}")
        print(f"å¤±æ•—è«‹æ±‚æ•¸: {self.stats['failed_requests']}")
        print(f"TWSEè«‹æ±‚æ•¸: {self.stats['twse_requests']}")
        print(f"TPEXè«‹æ±‚æ•¸: {self.stats['tpex_requests']}")
        print(f"Yahooè«‹æ±‚æ•¸: {self.stats['yahoo_requests']}")
        print(f"ç¸½è¨˜éŒ„æ•¸: {self.stats['total_records']}")
        
        if self.stats['total_requests'] > 0:
            success_rate = (self.stats['successful_requests'] / self.stats['total_requests']) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")

if __name__ == "__main__":
    # æ¸¬è©¦ç¯„ä¾‹
    crawler = RealDataCrawler()
    
    # æ¸¬è©¦çˆ¬å–2330.TW (å°ç©é›») 2025å¹´7æœˆæ•¸æ“š
    symbol = '2330.TW'
    year = 2025
    month = 7
    
    df = crawler.crawl_stock_data(symbol, year, month)
    
    if not df.empty:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {symbol} {year}-{month} æ•¸æ“š:")
        print(f"è¨˜éŒ„æ•¸: {len(df)}")
        print(f"æ•¸æ“šä¾†æº: {df['source'].iloc[0]}")
        print(f"åƒ¹æ ¼ç¯„åœ: {df['low'].min():.2f} - {df['high'].max():.2f}")
        print(f"å¹³å‡æ”¶ç›¤åƒ¹: {df['close'].mean():.2f}")
        
        # å­˜å…¥è³‡æ–™åº«
        crawler.save_to_database(df)
        
        # é¡¯ç¤ºå‰5ç­†æ•¸æ“š
        print("\nğŸ“‹ å‰5ç­†æ•¸æ“š:")
        print(df[['date', 'open', 'high', 'low', 'close', 'volume']].head().to_string(index=False))
    else:
        print(f"âŒ ç„¡æ³•ç²å– {symbol} æ•¸æ“š")
    
    # æ‰“å°çµ±è¨ˆä¿¡æ¯
    crawler.print_stats()
