#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆæ‰¹é‡è‚¡ç¥¨æ•¸æ“šæ›´æ–°å™¨ - æ”¯æŒä¸¦è¡Œè™•ç†å’Œæ™ºèƒ½æ›´æ–°

ä¸»è¦æ”¹é€²ï¼š
1. ä¸¦è¡Œè‚¡ç¥¨æ›´æ–°ï¼Œå¤§å¹…æå‡è™•ç†é€Ÿåº¦
2. æ™ºèƒ½å¢é‡æ›´æ–°ï¼Œé¿å…é‡è¤‡è™•ç†
3. å‹•æ…‹è² è¼‰å‡è¡¡ï¼Œå„ªåŒ–è³‡æºåˆ©ç”¨
4. è©³ç´°æ€§èƒ½ç›£æ§å’Œé€²åº¦è¿½è¹¤
5. éŒ¯èª¤æ¢å¾©å’Œé‡è©¦æ©Ÿåˆ¶

ä½œè€…ï¼šAI Trading System
ç‰ˆæœ¬ï¼š2.0
æ—¥æœŸï¼š2025-07-29
"""

import sys
import os
import time
import logging
import threading
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from src.data_sources.batch_stock_updater import BatchStockUpdater, BatchConfig, BatchProgress, BatchStatus
from src.data_sources.enhanced_real_data_crawler import EnhancedRealDataCrawler, CrawlerConfig
from src.data_sources.taiwan_stock_list_manager import TaiwanStockListManager

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)


@dataclass
class EnhancedBatchConfig(BatchConfig):
    """å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–°é…ç½®"""
    # ä¸¦è¡Œè™•ç†é…ç½®
    max_workers: int = 4  # æœ€å¤§ä¸¦è¡Œç·šç¨‹æ•¸
    enable_parallel: bool = True  # å•Ÿç”¨ä¸¦è¡Œè™•ç†
    
    # å¢é‡æ›´æ–°é…ç½®
    enable_incremental: bool = True  # å•Ÿç”¨å¢é‡æ›´æ–°
    data_freshness_hours: int = 24  # æ•¸æ“šæ–°é®®åº¦ï¼ˆå°æ™‚ï¼‰
    
    # æ€§èƒ½å„ªåŒ–é…ç½®
    batch_db_operations: bool = True  # æ‰¹é‡æ•¸æ“šåº«æ“ä½œ
    dynamic_rate_limit: bool = True  # å‹•æ…‹è«‹æ±‚é »ç‡æ§åˆ¶
    
    # ç›£æ§é…ç½®
    enable_performance_monitor: bool = True  # å•Ÿç”¨æ€§èƒ½ç›£æ§
    progress_update_interval: float = 1.0  # é€²åº¦æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰


@dataclass
class EnhancedBatchProgress(BatchProgress):
    """å¢å¼·ç‰ˆæ‰¹é‡é€²åº¦"""
    # ä¸¦è¡Œè™•ç†çµ±è¨ˆ
    active_workers: int = 0
    parallel_efficiency: float = 0.0
    
    # å¢é‡æ›´æ–°çµ±è¨ˆ
    cache_hits: int = 0
    incremental_updates: int = 0
    
    # æ€§èƒ½çµ±è¨ˆ
    avg_processing_time: float = 0.0
    records_per_second: float = 0.0
    estimated_completion: Optional[datetime] = None


class EnhancedBatchStockUpdater(BatchStockUpdater):
    """å¢å¼·ç‰ˆæ‰¹é‡è‚¡ç¥¨æ•¸æ“šæ›´æ–°å™¨"""
    
    def __init__(self, db_path: str = 'sqlite:///data/enhanced_stock_database.db',
                 config: Optional[EnhancedBatchConfig] = None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–°å™¨
        
        Args:
            db_path: è³‡æ–™åº«è·¯å¾‘
            config: å¢å¼·ç‰ˆé…ç½®
        """
        # åˆå§‹åŒ–åŸºé¡
        super().__init__(db_path)
        
        # è¨­ç½®å¢å¼·ç‰ˆé…ç½®
        self.enhanced_config = config or EnhancedBatchConfig()
        
        # å‰µå»ºå¢å¼·ç‰ˆçˆ¬å–å™¨
        crawler_config = CrawlerConfig(
            max_workers=self.enhanced_config.max_workers,
            enable_incremental=self.enhanced_config.enable_incremental,
            data_freshness_hours=self.enhanced_config.data_freshness_hours
        )
        self.enhanced_crawler = EnhancedRealDataCrawler(db_path=db_path, config=crawler_config)
        
        # ä¸¦è¡Œè™•ç†ç‹€æ…‹
        self._worker_stats = {}
        self._stats_lock = threading.Lock()
        self._processing_times = []
        
        # å¢å¼·ç‰ˆé€²åº¦
        self.enhanced_progress = None
        
        logger.info(f"å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–°å™¨åˆå§‹åŒ–å®Œæˆ - ä¸¦è¡Œåº¦: {self.enhanced_config.max_workers}")
    
    def update_all_stocks_enhanced(self, start_date: date, end_date: date,
                                 config: Optional[EnhancedBatchConfig] = None,
                                 stock_list: Optional[List] = None) -> Dict:
        """
        å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–°æ‰€æœ‰è‚¡ç¥¨æ•¸æ“š
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            config: å¢å¼·ç‰ˆé…ç½®
            stock_list: è‡ªå®šç¾©è‚¡ç¥¨æ¸…å–®ï¼ˆç”¨æ–¼æ¸¬è©¦æ¨¡å¼ï¼‰
            
        Returns:
            Dict: æ›´æ–°çµæœçµ±è¨ˆ
        """
        try:
            # ä½¿ç”¨æä¾›çš„é…ç½®æˆ–é»˜èªé…ç½®
            if config:
                self.enhanced_config = config
            
            # å¦‚æœæä¾›äº†è‡ªå®šç¾©è‚¡ç¥¨æ¸…å–®ï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰ï¼Œå¼·åˆ¶é‡ç½®é€²åº¦
            if stock_list is not None:
                self.progress = None
                self.enhanced_progress = None
                logger.info(f"ğŸ§ª æ¸¬è©¦æ¨¡å¼å•Ÿç”¨ï¼šå°‡æ›´æ–° {len(stock_list)} å€‹è‚¡ç¥¨")
            
            # ç²å–è‚¡ç¥¨æ¸…å–®
            if stock_list is not None:
                stocks = stock_list
                logger.info(f"ğŸ§ª ä½¿ç”¨æ¸¬è©¦æ¨¡å¼è‚¡ç¥¨æ¸…å–®: {len(stocks)} å€‹è‚¡ç¥¨")
            else:
                stocks = self.stock_manager.get_all_stocks()
                if not stocks:
                    self.stock_manager.update_stock_list()
                    stocks = self.stock_manager.get_all_stocks()
                if not stocks:
                    raise Exception("ç„¡æ³•ç²å–è‚¡ç¥¨æ¸…å–®")
            
            # åˆå§‹åŒ–å¢å¼·ç‰ˆé€²åº¦
            self.enhanced_progress = EnhancedBatchProgress(
                total_stocks=len(stocks),
                completed_stocks=0,
                failed_stocks=0,
                current_stock="",
                status=BatchStatus.RUNNING,
                start_time=datetime.now(),
                total_batches=1,  # ä¸¦è¡Œè™•ç†ä¸éœ€è¦æ‰¹æ¬¡æ¦‚å¿µ
                active_workers=0,
                cache_hits=0,
                incremental_updates=0
            )
            
            mode_text = "æ¸¬è©¦æ¨¡å¼" if stock_list is not None else "æ­£å¼æ¨¡å¼"
            logger.info(f"ğŸš€ é–‹å§‹å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–° {len(stocks)} å€‹è‚¡ç¥¨ ({mode_text})")
            
            # åŸ·è¡Œä¸¦è¡Œæ›´æ–°
            if self.enhanced_config.enable_parallel and len(stocks) > 1:
                result = self._update_stocks_parallel(stocks, start_date, end_date)
            else:
                result = self._update_stocks_sequential(stocks, start_date, end_date)
            
            # å®Œæˆçµ±è¨ˆ
            self.enhanced_progress.status = BatchStatus.COMPLETED
            self.enhanced_progress.end_time = datetime.now()
            
            # ç²å–æ€§èƒ½å ±å‘Š
            performance_report = self.enhanced_crawler.get_performance_report()
            
            # åˆä½µçµæœ
            final_result = {
                **result,
                'performance': performance_report,
                'mode': mode_text,
                'parallel_enabled': self.enhanced_config.enable_parallel,
                'incremental_enabled': self.enhanced_config.enable_incremental
            }
            
            logger.info(f"âœ… å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–°å®Œæˆ: {final_result}")
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–°å¤±æ•—: {e}")
            if self.enhanced_progress:
                self.enhanced_progress.status = BatchStatus.FAILED
            return {
                'status': 'failed',
                'error': str(e),
                'completed_stocks': self.enhanced_progress.completed_stocks if self.enhanced_progress else 0,
                'failed_stocks': self.enhanced_progress.failed_stocks if self.enhanced_progress else 0
            }
    
    def _update_stocks_parallel(self, stocks: List, start_date: date, end_date: date) -> Dict:
        """
        ä¸¦è¡Œæ›´æ–°è‚¡ç¥¨æ•¸æ“š

        Args:
            stocks: è‚¡ç¥¨æ¸…å–®
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            æ›´æ–°çµæœçµ±è¨ˆ
        """
        logger.info(f"ğŸ”„ å•Ÿå‹•ä¸¦è¡Œæ›´æ–°æ¨¡å¼ - {self.enhanced_config.max_workers} å€‹ç·šç¨‹")

        completed_stocks = 0
        failed_stocks = 0
        total_records = 0
        failed_symbols = []

        # çµ±è¨ˆå¿«å–å‘½ä¸­
        total_cache_hits = 0
        total_requests = 0

        # ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡Œè™•ç†
        with ThreadPoolExecutor(max_workers=self.enhanced_config.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_stock = {
                executor.submit(self._update_single_stock_enhanced, stock, start_date, end_date): stock
                for stock in stocks
            }

            # æ›´æ–°æ´»èºç·šç¨‹æ•¸
            with self._stats_lock:
                self.enhanced_progress.active_workers = len(future_to_stock)

            # æ”¶é›†çµæœ
            for future in as_completed(future_to_stock):
                stock = future_to_stock[future]
                symbol = stock.symbol if hasattr(stock, 'symbol') else str(stock)

                try:
                    start_time = time.time()
                    success, records, is_cache_hit = future.result()
                    processing_time = time.time() - start_time

                    # æ›´æ–°çµ±è¨ˆ
                    with self._stats_lock:
                        total_requests += 1
                        if is_cache_hit:
                            total_cache_hits += 1
                            self.enhanced_progress.cache_hits += 1

                        if success:
                            completed_stocks += 1
                            total_records += records
                            self.enhanced_progress.completed_stocks = completed_stocks
                        else:
                            failed_stocks += 1
                            failed_symbols.append(symbol)
                            self.enhanced_progress.failed_stocks = failed_stocks

                        # è¨˜éŒ„è™•ç†æ™‚é–“
                        self._processing_times.append(processing_time)
                        if len(self._processing_times) > 100:  # ä¿æŒæœ€è¿‘100å€‹è¨˜éŒ„
                            self._processing_times.pop(0)
                        
                        # æ›´æ–°é€²åº¦ä¿¡æ¯
                        self.enhanced_progress.current_stock = symbol
                        self.enhanced_progress.avg_processing_time = sum(self._processing_times) / len(self._processing_times)
                        
                        # è¨ˆç®—é ä¼°å®Œæˆæ™‚é–“
                        remaining_stocks = len(stocks) - completed_stocks - failed_stocks
                        if remaining_stocks > 0 and self.enhanced_progress.avg_processing_time > 0:
                            estimated_seconds = remaining_stocks * self.enhanced_progress.avg_processing_time / self.enhanced_config.max_workers
                            self.enhanced_progress.estimated_completion = datetime.now() + timedelta(seconds=estimated_seconds)
                    
                    # èª¿ç”¨é€²åº¦å›èª¿
                    self._notify_progress()
                    
                    status = "âœ…" if success else "âŒ"
                    logger.info(f"{status} {symbol}: {records} ç­†è¨˜éŒ„ ({processing_time:.2f}s)")
                    
                except Exception as e:
                    failed_stocks += 1
                    failed_symbols.append(symbol)
                    logger.error(f"âŒ è™•ç† {symbol} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    
                    with self._stats_lock:
                        self.enhanced_progress.failed_stocks = failed_stocks
                    
                    self._notify_progress()
        
        # è¨ˆç®—æˆåŠŸç‡å’Œå¿«å–å‘½ä¸­ç‡
        total_processed = completed_stocks + failed_stocks
        success_rate = completed_stocks / total_processed if total_processed > 0 else 0
        cache_hit_rate = (total_cache_hits / total_requests * 100) if total_requests > 0 else 0

        return {
            'status': 'completed',
            'total_stocks': len(stocks),
            'completed_stocks': completed_stocks,
            'failed_stocks': failed_stocks,
            'success_rate': success_rate,
            'total_records': total_records,
            'failed_symbols': failed_symbols,
            'processing_mode': 'parallel',
            'cache_hits': total_cache_hits,
            'cache_hit_rate': cache_hit_rate
        }
    
    def _update_stocks_sequential(self, stocks: List, start_date: date, end_date: date) -> Dict:
        """
        åºåˆ—æ›´æ–°è‚¡ç¥¨æ•¸æ“šï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        
        Args:
            stocks: è‚¡ç¥¨æ¸…å–®
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            
        Returns:
            æ›´æ–°çµæœçµ±è¨ˆ
        """
        logger.info("ğŸ”„ ä½¿ç”¨åºåˆ—æ›´æ–°æ¨¡å¼")
        
        completed_stocks = 0
        failed_stocks = 0
        total_records = 0
        failed_symbols = []
        
        for i, stock in enumerate(stocks):
            symbol = stock.symbol if hasattr(stock, 'symbol') else str(stock)
            
            try:
                start_time = time.time()
                success, records = self._update_single_stock_enhanced(stock, start_date, end_date)
                processing_time = time.time() - start_time
                
                if success:
                    completed_stocks += 1
                    total_records += records
                else:
                    failed_stocks += 1
                    failed_symbols.append(symbol)
                
                # æ›´æ–°é€²åº¦
                self.enhanced_progress.completed_stocks = completed_stocks
                self.enhanced_progress.failed_stocks = failed_stocks
                self.enhanced_progress.current_stock = symbol
                
                # è¨˜éŒ„è™•ç†æ™‚é–“
                self._processing_times.append(processing_time)
                if len(self._processing_times) > 50:
                    self._processing_times.pop(0)
                
                self.enhanced_progress.avg_processing_time = sum(self._processing_times) / len(self._processing_times)
                
                self._notify_progress()
                
                status = "âœ…" if success else "âŒ"
                logger.info(f"{status} {symbol}: {records} ç­†è¨˜éŒ„ ({processing_time:.2f}s)")
                
            except Exception as e:
                failed_stocks += 1
                failed_symbols.append(symbol)
                logger.error(f"âŒ è™•ç† {symbol} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # è¨ˆç®—æˆåŠŸç‡
        total_processed = completed_stocks + failed_stocks
        success_rate = completed_stocks / total_processed if total_processed > 0 else 0
        
        return {
            'status': 'completed',
            'total_stocks': len(stocks),
            'completed_stocks': completed_stocks,
            'failed_stocks': failed_stocks,
            'success_rate': success_rate,
            'total_records': total_records,
            'failed_symbols': failed_symbols,
            'processing_mode': 'sequential'
        }
    
    def _update_single_stock_enhanced(self, stock, start_date: date, end_date: date) -> tuple:
        """
        å¢å¼·ç‰ˆå–®ä¸€è‚¡ç¥¨æ›´æ–°

        Args:
            stock: è‚¡ç¥¨ä¿¡æ¯
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            (æ˜¯å¦æˆåŠŸ, è¨˜éŒ„æ•¸, æ˜¯å¦å¿«å–å‘½ä¸­)
        """
        symbol = stock.symbol if hasattr(stock, 'symbol') else str(stock)

        for attempt in range(self.enhanced_config.max_retries):
            try:
                # è¨˜éŒ„çˆ¬å–å‰çš„çµ±è¨ˆ
                initial_cache_hits = self.enhanced_crawler.performance_stats.cache_hits

                # ä½¿ç”¨å¢å¼·ç‰ˆçˆ¬å–å™¨
                df = self.enhanced_crawler.crawl_stock_data_range_enhanced(symbol, start_date, end_date)

                # æª¢æŸ¥æ˜¯å¦æœ‰å¿«å–å‘½ä¸­
                final_cache_hits = self.enhanced_crawler.performance_stats.cache_hits
                is_cache_hit = final_cache_hits > initial_cache_hits

                if df is not None and not df.empty:
                    # ä¿å­˜æ•¸æ“š
                    self.enhanced_crawler.save_to_database(df)
                    return True, len(df), is_cache_hit
                else:
                    raise Exception("çˆ¬å–çµæœç‚ºç©º")

            except Exception as e:
                if attempt < self.enhanced_config.max_retries - 1:
                    time.sleep(self.enhanced_config.retry_delay)
                else:
                    logger.warning(f"âŒ {symbol} æ›´æ–°å¤±æ•—: {e}")

        return False, 0, False
    
    def _notify_progress(self):
        """é€šçŸ¥é€²åº¦æ›´æ–°"""
        if self.enhanced_progress:
            # è¨ˆç®—ä¸¦è¡Œæ•ˆç‡
            if self.enhanced_progress.total_stocks > 0:
                self.enhanced_progress.parallel_efficiency = (
                    self.enhanced_progress.completed_stocks / self.enhanced_progress.total_stocks
                )
            
            # è¨ˆç®—è™•ç†é€Ÿåº¦
            if self.enhanced_progress.start_time:
                elapsed = (datetime.now() - self.enhanced_progress.start_time).total_seconds()
                if elapsed > 0:
                    self.enhanced_progress.records_per_second = (
                        self.enhanced_crawler.performance_stats.total_records / elapsed
                    )
            
            # èª¿ç”¨æ‰€æœ‰å›èª¿
            for callback in self.progress_callbacks:
                try:
                    callback(self.enhanced_progress)
                except Exception as e:
                    logger.error(f"é€²åº¦å›èª¿éŒ¯èª¤: {e}")
    
    def get_enhanced_progress(self) -> Optional[EnhancedBatchProgress]:
        """ç²å–å¢å¼·ç‰ˆé€²åº¦ä¿¡æ¯"""
        return self.enhanced_progress
    
    # å‘å¾Œå…¼å®¹æ€§æ–¹æ³•
    def update_all_stocks(self, year: int = None, month: int = None,
                         config: Optional[BatchConfig] = None,
                         resume: bool = False,
                         stock_list: Optional[List] = None,
                         start_date: Optional[date] = None,
                         end_date: Optional[date] = None) -> Dict:
        """
        å‘å¾Œå…¼å®¹çš„æ‰¹é‡æ›´æ–°æ–¹æ³•
        
        è‡ªå‹•æª¢æ¸¬æ˜¯å¦ä½¿ç”¨å¢å¼·ç‰ˆåŠŸèƒ½
        """
        # åƒæ•¸è™•ç†é‚è¼¯ï¼ˆèˆ‡åŸç‰ˆç›¸åŒï¼‰
        if start_date and end_date:
            if start_date > end_date:
                raise ValueError("é–‹å§‹æ—¥æœŸä¸èƒ½æ™šæ–¼çµæŸæ—¥æœŸ")
            logger.info(f"ğŸ“… ä½¿ç”¨æ—¥æœŸç¯„åœæ¨¡å¼: {start_date} è‡³ {end_date}")
        elif year and month:
            start_date = date(year, month, 1)
            if month == 12:
                end_date = date(year + 1, 1, 1) - timedelta(days=1)
            else:
                end_date = date(year, month + 1, 1) - timedelta(days=1)
            logger.info(f"ğŸ“… ä½¿ç”¨æœˆä»½æ¨¡å¼: {year}-{month:02d}")
        else:
            raise ValueError("å¿…é ˆæä¾› start_date/end_date æˆ– year/month åƒæ•¸")
        
        # å¦‚æœå•Ÿç”¨äº†å¢å¼·åŠŸèƒ½ï¼Œä½¿ç”¨å¢å¼·ç‰ˆæ–¹æ³•
        if self.enhanced_config.enable_parallel or self.enhanced_config.enable_incremental:
            enhanced_config = self.enhanced_config
            if config:
                # å°‡èˆŠé…ç½®è½‰æ›ç‚ºå¢å¼·ç‰ˆé…ç½®
                enhanced_config.batch_size = config.batch_size
                enhanced_config.request_delay = config.request_delay
                enhanced_config.max_retries = config.max_retries
                enhanced_config.retry_delay = config.retry_delay
            
            return self.update_all_stocks_enhanced(start_date, end_date, enhanced_config, stock_list)
        else:
            # ä½¿ç”¨åŸç‰ˆæ–¹æ³•
            return super().update_all_stocks(year, month, config, resume, stock_list, start_date, end_date)


# å·¥å» å‡½æ•¸
def create_enhanced_updater(enable_parallel: bool = True, max_workers: int = 4,
                          enable_incremental: bool = True) -> BatchStockUpdater:
    """
    å‰µå»ºå¢å¼·ç‰ˆæ›´æ–°å™¨çš„å·¥å» å‡½æ•¸
    
    Args:
        enable_parallel: æ˜¯å¦å•Ÿç”¨ä¸¦è¡Œè™•ç†
        max_workers: æœ€å¤§ä¸¦è¡Œç·šç¨‹æ•¸
        enable_incremental: æ˜¯å¦å•Ÿç”¨å¢é‡æ›´æ–°
        
    Returns:
        æ›´æ–°å™¨å¯¦ä¾‹
    """
    if enable_parallel or enable_incremental:
        config = EnhancedBatchConfig(
            enable_parallel=enable_parallel,
            max_workers=max_workers,
            enable_incremental=enable_incremental
        )
        return EnhancedBatchStockUpdater(config=config)
    else:
        return BatchStockUpdater()


if __name__ == "__main__":
    # æ¸¬è©¦å¢å¼·ç‰ˆæ›´æ–°å™¨
    logging.basicConfig(level=logging.INFO)
    
    config = EnhancedBatchConfig(
        max_workers=3,
        enable_parallel=True,
        enable_incremental=True
    )
    
    updater = EnhancedBatchStockUpdater(config=config)
    
    # è¨­ç½®é€²åº¦å›èª¿
    def progress_callback(progress):
        print(f"é€²åº¦: {progress.completed_stocks}/{progress.total_stocks} "
              f"({progress.parallel_efficiency*100:.1f}%) - {progress.current_stock}")
    
    updater.add_progress_callback(progress_callback)
    
    # æ¸¬è©¦æ—¥æœŸç¯„åœ
    start_date = date(2025, 7, 22)
    end_date = date(2025, 7, 29)
    
    print(f"é–‹å§‹æ¸¬è©¦å¢å¼·ç‰ˆæ‰¹é‡æ›´æ–° ({start_date} è‡³ {end_date})")
    
    # åŸ·è¡Œæ›´æ–°ï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰
    result = updater.update_all_stocks_enhanced(start_date, end_date)
    print(f"æ›´æ–°çµæœ: {result}")
