#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¶œåˆæ•¸æ“šçˆ¬èŸ²ç³»çµ±
==============

å¯¦ç¾æ‰€æœ‰33å€‹å°è‚¡å…è²»æ•¸æ“šæºçš„çˆ¬èŸ²æ¨¡çµ„ï¼Œç¢ºä¿æ¯å€‹åŠŸèƒ½éƒ½æœ‰å¯ç”¨çš„å¯¦ç¾ã€‚
æŒ‰ç…§æŠ€è¡“é¢ã€åŸºæœ¬é¢ã€ç±Œç¢¼é¢ã€äº‹ä»¶é¢ã€ç¸½ç¶“é¢åˆ†é¡çµ„ç¹”ã€‚
"""

import requests
import pandas as pd
import yfinance as yf
import time
import logging
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import warnings
warnings.filterwarnings('ignore')

try:
    from .enhanced_html_parser import EnhancedHTMLParser
except ImportError:
    # å¦‚æœå°å…¥å¤±æ•—ï¼Œä½¿ç”¨ç©ºé¡ä½œç‚ºå‚™ç”¨
    class EnhancedHTMLParser:
        def parse_twse_page(self, url, data_type, **kwargs):
            return pd.DataFrame()

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveCrawler:
    """ç¶œåˆæ•¸æ“šçˆ¬èŸ²ç³»çµ±"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.request_delay = 2.0  # è«‹æ±‚é–“éš”

        # åˆå§‹åŒ–å¢å¼·HTMLè§£æå™¨
        try:
            self.html_parser = EnhancedHTMLParser()
            logger.info("âœ… å¢å¼·HTMLè§£æå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"å¢å¼·HTMLè§£æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            self.html_parser = None
        
    def _safe_request(self, url: str, params: Dict = None, timeout: int = 30) -> Optional[requests.Response]:
        """å®‰å…¨çš„HTTPè«‹æ±‚"""
        try:
            time.sleep(self.request_delay)
            response = self.session.get(url, params=params, timeout=timeout)
            response.raise_for_status()
            return response
        except Exception as e:
            logger.error(f"è«‹æ±‚å¤±æ•— {url}: {e}")
            return None
    
    # ========================================================================
    # æŠ€è¡“é¢æ•¸æ“šçˆ¬èŸ² (10å€‹æ•¸æ“šæº)
    # ========================================================================
    
    def crawl_twse_zero_share(self, date_str: str = None) -> pd.DataFrame:
        """1. ä¸Šå¸‚æ«ƒç›¤å¾Œé›¶è‚¡æˆäº¤è³‡è¨Š - TWSE CSV"""
        if not date_str:
            date_str = datetime.now().strftime('%Y%m%d')
            
        url = "https://www.twse.com.tw/exchangeReport/TWTASU"
        params = {
            'response': 'csv',
            'date': date_str
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            # è§£æCSVæ•¸æ“š
            lines = response.text.strip().split('\n')
            if len(lines) < 2:
                return pd.DataFrame()

            # æ‰¾åˆ°æ•¸æ“šé–‹å§‹è¡Œ
            data_lines = []
            for line in lines:
                if 'è­‰åˆ¸ä»£è™Ÿ' in line or line.startswith('"'):
                    data_lines.append(line)

            if data_lines:
                from io import StringIO
                df = pd.read_csv(StringIO('\n'.join(data_lines)))
                df['data_source'] = 'TWSE_é›¶è‚¡'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… é›¶è‚¡æˆäº¤è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"é›¶è‚¡æˆäº¤è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_backtest_index(self) -> pd.DataFrame:
        """2. å›æ¸¬åŸºæº–æŒ‡æ•¸ - TWSE OpenAPI JSON"""
        url = "https://openapi.twse.com.tw/v1/indicesReport/MI_5MINS_HIST"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if data:
                df = pd.DataFrame(data)
                df['data_source'] = 'TWSE_å›æ¸¬æŒ‡æ•¸'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… å›æ¸¬åŸºæº–æŒ‡æ•¸ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å›æ¸¬åŸºæº–æŒ‡æ•¸è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_tpex_cb_trading(self) -> pd.DataFrame:
        """3. å¯è½‰æ›å…¬å¸å‚µæˆäº¤è³‡è¨Š - TPEX HTML"""
        url = "https://www.tpex.org.tw/web/regular_emerging/corporateInfo/regular_bond_trading.php"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°åŒ…å«å‚µåˆ¸æ•¸æ“šçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and 'å‚µåˆ¸' in table.get_text():
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TPEX_å¯è½‰å‚µ'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… å¯è½‰æ›å…¬å¸å‚µè³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"å¯è½‰æ›å…¬å¸å‚µè³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_trading_change(self) -> pd.DataFrame:
        """4. ä¸Šå¸‚æ«ƒè®Šæ›´äº¤æ˜“ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTAUU.html"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°è®Šæ›´äº¤æ˜“çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('è®Šæ›´' in table.get_text() or 'äº¤æ˜“' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_è®Šæ›´äº¤æ˜“'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… è®Šæ›´äº¤æ˜“è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"è®Šæ›´äº¤æ˜“è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_yahoo_adjusted_price(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """5. é‚„åŸæ¬Šå€¼è‚¡åƒ¹ - Yahoo Finance API"""
        try:
            # è½‰æ›æ—¥æœŸæ ¼å¼
            start = pd.to_datetime(start_date).strftime('%Y-%m-%d')
            end = pd.to_datetime(end_date).strftime('%Y-%m-%d')
            
            # ä½¿ç”¨yfinanceç²å–èª¿æ•´å¾Œè‚¡åƒ¹
            df = yf.download(symbol, start=start, end=end, auto_adjust=True, progress=False)
            
            if not df.empty:
                # ä¿®å¾©å¤šå±¤æ¬„ä½çµæ§‹å•é¡Œ
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
                
                df.reset_index(inplace=True)
                df['symbol'] = symbol
                df['data_source'] = 'Yahoo_é‚„åŸè‚¡åƒ¹'
                df['crawl_time'] = datetime.now()
                
                logger.info(f"âœ… {symbol} é‚„åŸè‚¡åƒ¹ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"é‚„åŸè‚¡åƒ¹ç²å–å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_tpex_disposal_stocks(self) -> pd.DataFrame:
        """6. æ’é™¤è™•ç½®è‚¡ - TPEX HTML"""
        url = "https://www.tpex.org.tw/web/stock/aftertrading/daily_trading_info/st43.php"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°è™•ç½®è‚¡çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('è™•ç½®' in table.get_text() or 'è‚¡ç¥¨' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TPEX_è™•ç½®è‚¡'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… è™•ç½®è‚¡è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"è™•ç½®è‚¡è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_market_indicators(self) -> pd.DataFrame:
        """7. å¤§ç›¤å¸‚æ³æŒ‡æ¨™ - TWSE OpenAPI JSON âœ…"""
        url = "https://openapi.twse.com.tw/v1/indicesReport/MI_INDEX"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            data = response.json()
            if data:
                df = pd.DataFrame(data)
                df['data_source'] = 'TWSE_å¤§ç›¤æŒ‡æ¨™'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… å¤§ç›¤å¸‚æ³æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å¤§ç›¤å¸‚æ³æŒ‡æ¨™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_attention_stocks(self) -> pd.DataFrame:
        """8. æ’é™¤æ³¨æ„è‚¡ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB4U.html"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°æ³¨æ„è‚¡çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('æ³¨æ„' in table.get_text() or 'è‚¡ç¥¨' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_æ³¨æ„è‚¡'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… æ³¨æ„è‚¡è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"æ³¨æ„è‚¡è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_taifex_futures_daily(self, date_str: str = None) -> pd.DataFrame:
        """9. æœŸè²¨æ—¥æˆäº¤è³‡è¨Š - TAIFEX CSV"""
        if not date_str:
            date_str = datetime.now().strftime('%Y%m%d')
            
        url = "https://www.taifex.com.tw/cht/3/dailyFXRate"
        params = {
            'queryDate': date_str
        }
        
        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°æœŸè²¨æ•¸æ“šçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('æœŸè²¨' in table.get_text() or 'æˆäº¤' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TAIFEX_æœŸè²¨'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… æœŸè²¨æ—¥æˆäº¤è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"æœŸè²¨æ—¥æˆäº¤è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_twse_intraday_zero_share(self) -> pd.DataFrame:
        """10. ä¸Šå¸‚æ«ƒç›¤ä¸­é›¶è‚¡æˆäº¤è³‡è¨Š - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTASU.html"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°ç›¤ä¸­é›¶è‚¡çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('é›¶è‚¡' in table.get_text() or 'ç›¤ä¸­' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_ç›¤ä¸­é›¶è‚¡'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… ç›¤ä¸­é›¶è‚¡æˆäº¤è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"ç›¤ä¸­é›¶è‚¡æˆäº¤è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    # ========================================================================
    # åŸºæœ¬é¢æ•¸æ“šçˆ¬èŸ² (7å€‹æ•¸æ“šæº)
    # ========================================================================
    
    def crawl_twse_dividend_announcement(self) -> pd.DataFrame:
        """1. è‘£äº‹æœƒæ±ºæ“¬è­°åˆ†é…è‚¡åˆ©å…¬å‘Š - TWSE HTML (å¢å¼·è§£æ)"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB7U.html"

        # å„ªå…ˆä½¿ç”¨å¢å¼·HTMLè§£æå™¨
        if self.html_parser:
            try:
                df = self.html_parser.parse_twse_page(url, 'è‚¡åˆ©å…¬å‘Š')
                if not df.empty:
                    logger.info(f"âœ… è‚¡åˆ©å…¬å‘Šç²å–æˆåŠŸ (å¢å¼·è§£æ): {len(df)} ç­†è¨˜éŒ„")
                    return df
            except Exception as e:
                logger.warning(f"å¢å¼·è§£æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•: {e}")

        # å‚™ç”¨è§£ææ–¹æ³•
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°è‚¡åˆ©å…¬å‘Šçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('è‚¡åˆ©' in table.get_text() or 'åˆ†é…' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_è‚¡åˆ©å…¬å‘Š'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… è‚¡åˆ©å…¬å‘Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"è‚¡åˆ©å…¬å‘Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()
    
    def crawl_tpex_capital_reduction(self) -> pd.DataFrame:
        """2. ä¸Šæ«ƒæ¸›è³‡ - TPEX HTML"""
        url = "https://www.tpex.org.tw/web/regular_emerging/corporateInfo/capital_reduction.php"
        
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()
            
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')
            
            if tables:
                # æ‰¾åˆ°æ¸›è³‡çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and 'æ¸›è³‡' in table.get_text():
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TPEX_æ¸›è³‡'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… ä¸Šæ«ƒæ¸›è³‡è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"ä¸Šæ«ƒæ¸›è³‡è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_capital_reduction(self) -> pd.DataFrame:
        """3. ä¸Šå¸‚æ¸›è³‡ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB8U.html"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°æ¸›è³‡çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and 'æ¸›è³‡' in table.get_text():
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_æ¸›è³‡'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… ä¸Šå¸‚æ¸›è³‡è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"ä¸Šå¸‚æ¸›è³‡è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_gov_company_info(self, api_key: str = None) -> pd.DataFrame:
        """4. ä¼æ¥­åŸºæœ¬è³‡è¨Š - æ”¿åºœé–‹æ”¾å¹³å° JSON âœ…"""
        if not api_key:
            logger.warning("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            # è¿”å›æ¨¡æ“¬æ•¸æ“šçµæ§‹
            return pd.DataFrame({
                'company_id': ['12345678'],
                'company_name': ['æ¸¬è©¦å…¬å¸ [æ¨¡æ“¬æ•¸æ“š]'],
                'industry': ['é›»å­æ¥­ [æ¨¡æ“¬æ•¸æ“š]'],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_ä¼æ¥­è³‡è¨Š_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now()],
                'data_type': ['MOCK_DATA']
            })

        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000157-053"
        params = {
            'Authorization': api_key,
            'limit': 1000
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_ä¼æ¥­è³‡è¨Š'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… ä¼æ¥­åŸºæœ¬è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"ä¼æ¥­åŸºæœ¬è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_business_info(self) -> pd.DataFrame:
        """5. ä¼æ¥­ä¸»è¦ç¶“ç‡Ÿæ¥­å‹™ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB9U.html"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°ç¶“ç‡Ÿæ¥­å‹™çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('æ¥­å‹™' in table.get_text() or 'ç¶“ç‡Ÿ' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_ç¶“ç‡Ÿæ¥­å‹™'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… ä¼æ¥­ç¶“ç‡Ÿæ¥­å‹™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"ä¼æ¥­ç¶“ç‡Ÿæ¥­å‹™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_monthly_revenue(self, year: int = None, month: int = None) -> pd.DataFrame:
        """6. ä¸Šå¸‚æ«ƒæœˆç‡Ÿæ”¶ - TWSE HTML (å¢å¼·è§£æ)"""
        if not year:
            year = datetime.now().year
        if not month:
            month = datetime.now().month

        # å„ªå…ˆä½¿ç”¨å¢å¼·HTMLè§£æå™¨
        if self.html_parser:
            try:
                url_html = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB4U.html"
                df = self.html_parser.parse_twse_page(url_html, 'æœˆç‡Ÿæ”¶', year=year, month=month)
                if not df.empty:
                    logger.info(f"âœ… æœˆç‡Ÿæ”¶è³‡è¨Šç²å–æˆåŠŸ (å¢å¼·è§£æ): {len(df)} ç­†è¨˜éŒ„")
                    return df
            except Exception as e:
                logger.warning(f"å¢å¼·è§£æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•: {e}")

        # å‚™ç”¨JSON APIæ–¹æ³•
        url = "https://www.twse.com.tw/exchangeReport/TWTB4U"
        params = {
            'response': 'json',
            'date': f"{year}{month:02d}01"
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_æœˆç‡Ÿæ”¶'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… æœˆç‡Ÿæ”¶è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"æœˆç‡Ÿæ”¶è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_finmind_financial_data(self, symbol: str = "2330") -> pd.DataFrame:
        """7. è²¡å‹™æŒ‡æ¨™ - FinMind API JSON"""
        url = "https://api.finmindtrade.com/api/v4/data"
        params = {
            'dataset': 'TaiwanStockFinancialStatements',
            'data_id': symbol,
            'start_date': '2024-01-01'
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'FinMind_è²¡å‹™æŒ‡æ¨™'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… è²¡å‹™æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è²¡å‹™æŒ‡æ¨™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    # ========================================================================
    # ç±Œç¢¼é¢æ•¸æ“šçˆ¬èŸ² (6å€‹æ•¸æ“šæº)
    # ========================================================================

    def crawl_twse_broker_mapping(self) -> pd.DataFrame:
        """1. åˆ¸å•†åˆ†é»åç¨±å°ç…§è¡¨ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB5U.html"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°åˆ¸å•†å°ç…§è¡¨
                for table in tables:
                    if table.find('th') and ('åˆ¸å•†' in table.get_text() or 'åˆ†é»' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_åˆ¸å•†å°ç…§'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… åˆ¸å•†åˆ†é»å°ç…§è¡¨ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"åˆ¸å•†åˆ†é»å°ç…§è¡¨è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_broker_trading(self, symbol: str = "2330") -> pd.DataFrame:
        """2. åˆ¸å•†åˆ†é»è²·è³£è¶…å‰15å¤§åˆ¸å•†æ˜ç´° - TWSE HTML"""
        url = "https://www.twse.com.tw/fund/T86"
        params = {
            'response': 'json',
            'date': datetime.now().strftime('%Y%m%d'),
            'selectType': 'ALLBUT0999'
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_åˆ¸å•†è²·è³£è¶…'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… åˆ¸å•†è²·è³£è¶…æ˜ç´°ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"åˆ¸å•†è²·è³£è¶…æ˜ç´°è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_foreign_holding(self) -> pd.DataFrame:
        """3. å¤–è³‡æŒè‚¡æ¯”ç‡ - TWSE HTML"""
        url = "https://www.twse.com.tw/fund/MI_QFIIS"
        params = {
            'response': 'json',
            'date': datetime.now().strftime('%Y%m%d')
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_å¤–è³‡æŒè‚¡'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… å¤–è³‡æŒè‚¡æ¯”ç‡ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"å¤–è³‡æŒè‚¡æ¯”ç‡è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_taifex_institutional_trading(self) -> pd.DataFrame:
        """4. æœŸè²¨ä¸‰å¤§æ³•äººç›¤å¾Œè³‡è¨Š - TAIFEX CSV"""
        url = "https://www.taifex.com.tw/cht/3/largeTraderFutQry"
        params = {
            'queryDate': datetime.now().strftime('%Y/%m/%d')
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°ä¸‰å¤§æ³•äººçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('æ³•äºº' in table.get_text() or 'æœŸè²¨' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TAIFEX_ä¸‰å¤§æ³•äºº'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… æœŸè²¨ä¸‰å¤§æ³•äººè³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"æœŸè²¨ä¸‰å¤§æ³•äººè³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_margin_trading(self) -> pd.DataFrame:
        """5. èè³‡èåˆ¸ - TWSE HTML"""
        url = "https://www.twse.com.tw/exchangeReport/MI_MARGN"
        params = {
            'response': 'json',
            'date': datetime.now().strftime('%Y%m%d'),
            'selectType': 'ALL'
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_èè³‡èåˆ¸'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… èè³‡èåˆ¸è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"èè³‡èåˆ¸è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_securities_lending(self) -> pd.DataFrame:
        """6. å€Ÿåˆ¸ - TWSE HTML"""
        url = "https://www.twse.com.tw/exchangeReport/TWT38U"
        params = {
            'response': 'json',
            'date': datetime.now().strftime('%Y%m%d')
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'data' in data and data['data']:
                df = pd.DataFrame(data['data'])
                df['data_source'] = 'TWSE_å€Ÿåˆ¸'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… å€Ÿåˆ¸è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"å€Ÿåˆ¸è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    # ========================================================================
    # äº‹ä»¶é¢æ•¸æ“šçˆ¬èŸ² (5å€‹æ•¸æ“šæº)
    # ========================================================================

    def crawl_twse_announcements(self) -> pd.DataFrame:
        """1. é‡è¨Šå…¬å‘Š - TWSE HTML (å¢å¼·è§£æ)"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB6U.html"

        # å„ªå…ˆä½¿ç”¨å¢å¼·HTMLè§£æå™¨
        if self.html_parser:
            try:
                df = self.html_parser.parse_twse_page(url, 'é‡è¨Šå…¬å‘Š')
                if not df.empty:
                    logger.info(f"âœ… é‡è¨Šå…¬å‘Šç²å–æˆåŠŸ (å¢å¼·è§£æ): {len(df)} ç­†è¨˜éŒ„")
                    return df
            except Exception as e:
                logger.warning(f"å¢å¼·è§£æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•: {e}")

        # å‚™ç”¨è§£ææ–¹æ³•
        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°é‡è¨Šå…¬å‘Šçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('å…¬å‘Š' in table.get_text() or 'é‡è¨Š' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_é‡è¨Šå…¬å‘Š'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… é‡è¨Šå…¬å‘Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"é‡è¨Šå…¬å‘Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_investor_conference(self) -> pd.DataFrame:
        """2. æ³•èªªæœƒæœŸç¨‹ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB1U.html"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°æ³•èªªæœƒçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('æ³•èªªæœƒ' in table.get_text() or 'æœŸç¨‹' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_æ³•èªªæœƒ'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… æ³•èªªæœƒæœŸç¨‹ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"æ³•èªªæœƒæœŸç¨‹è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_shareholder_meeting(self) -> pd.DataFrame:
        """3. è‚¡æ±æœƒæœŸç¨‹ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB2U.html"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°è‚¡æ±æœƒçš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and ('è‚¡æ±æœƒ' in table.get_text() or 'æœŸç¨‹' in table.get_text()):
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_è‚¡æ±æœƒ'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… è‚¡æ±æœƒæœŸç¨‹ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"è‚¡æ±æœƒæœŸç¨‹è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_twse_treasury_stock(self) -> pd.DataFrame:
        """4. åº«è—è‚¡ - TWSE HTML"""
        url = "https://www.twse.com.tw/zh/page/trading/exchange/TWTB3U.html"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            tables = soup.find_all('table')

            if tables:
                # æ‰¾åˆ°åº«è—è‚¡çš„è¡¨æ ¼
                for table in tables:
                    if table.find('th') and 'åº«è—è‚¡' in table.get_text():
                        df = pd.read_html(str(table))[0]
                        df['data_source'] = 'TWSE_åº«è—è‚¡'
                        df['crawl_time'] = datetime.now()
                        logger.info(f"âœ… åº«è—è‚¡è³‡è¨Šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                        return df

            return pd.DataFrame()

        except Exception as e:
            logger.error(f"åº«è—è‚¡è³‡è¨Šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_cnyes_news(self) -> pd.DataFrame:
        """5. å°è‚¡æ–°è - cnyes RSS"""
        url = "https://news.cnyes.com/api/v3/news/category/tw_stock"

        response = self._safe_request(url)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'items' in data and 'data' in data['items']:
                news_data = []
                for item in data['items']['data']:
                    news_data.append({
                        'title': item.get('title', ''),
                        'summary': item.get('summary', ''),
                        'publish_at': item.get('publishAt', ''),
                        'url': item.get('url', ''),
                        'data_source': 'cnyes_æ–°è',
                        'crawl_time': datetime.now()
                    })

                df = pd.DataFrame(news_data)
                logger.info(f"âœ… å°è‚¡æ–°èç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"å°è‚¡æ–°èè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    # ========================================================================
    # ç¸½ç¶“é¢æ•¸æ“šçˆ¬èŸ² (5å€‹æ•¸æ“šæº)
    # ========================================================================

    def crawl_gov_economic_indicators(self, api_key: str = None) -> pd.DataFrame:
        """1. å°ç£æ™¯æ°£æŒ‡æ¨™ - æ”¿åºœé–‹æ”¾å¹³å° JSON âœ…"""
        if not api_key:
            logger.warning("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            # è¿”å›æ¨¡æ“¬æ•¸æ“šçµæ§‹
            return pd.DataFrame({
                'indicator_name': ['æ™¯æ°£å°ç­–ä¿¡è™Ÿ [æ¨¡æ“¬æ•¸æ“š]'],
                'value': [25],
                'period': ['2025-01'],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_æ™¯æ°£æŒ‡æ¨™_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now()],
                'data_type': ['MOCK_DATA']
            })

        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000151-001"
        params = {
            'Authorization': api_key,
            'limit': 100
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_æ™¯æ°£æŒ‡æ¨™'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… æ™¯æ°£æŒ‡æ¨™ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"æ™¯æ°£æŒ‡æ¨™è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_gov_manufacturing_pmi(self, api_key: str = None) -> pd.DataFrame:
        """2. å°ç£è£½é€ æ¥­æ¡è³¼ç¶“ç†äººæŒ‡æ•¸ - æ”¿åºœé–‹æ”¾å¹³å° JSON"""
        if not api_key:
            logger.warning("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            return pd.DataFrame({
                'pmi_value': [52.5],
                'period': ['2025-01 [æ¨¡æ“¬æ•¸æ“š]'],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_è£½é€ æ¥­PMI_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now()],
                'data_type': ['MOCK_DATA']
            })

        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000152-001"
        params = {
            'Authorization': api_key,
            'limit': 100
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_è£½é€ æ¥­PMI'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… è£½é€ æ¥­PMIç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è£½é€ æ¥­PMIè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_gov_service_pmi(self, api_key: str = None) -> pd.DataFrame:
        """3. å°ç£éè£½é€ æ¥­æ¡è³¼ç¶“ç†äººæŒ‡æ•¸ - æ”¿åºœé–‹æ”¾å¹³å° JSON"""
        if not api_key:
            logger.warning("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            return pd.DataFrame({
                'pmi_value': [48.5],
                'period': ['2025-01 [æ¨¡æ“¬æ•¸æ“š]'],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_æœå‹™æ¥­PMI_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now()],
                'data_type': ['MOCK_DATA']
            })

        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000153-001"
        params = {
            'Authorization': api_key,
            'limit': 100
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_æœå‹™æ¥­PMI'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… æœå‹™æ¥­PMIç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"æœå‹™æ¥­PMIè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_gov_money_supply(self, api_key: str = None) -> pd.DataFrame:
        """4. è²¨å¹£ç¸½è¨ˆæ•¸å¹´å¢ç‡ - æ”¿åºœé–‹æ”¾å¹³å° JSON"""
        if not api_key:
            logger.warning("æ”¿åºœé–‹æ”¾å¹³å°éœ€è¦APIé‡‘é‘°ï¼Œè¿”å›æ¨¡æ“¬æ•¸æ“š")
            return pd.DataFrame({
                'money_supply_growth': [3.2],
                'period': ['2025-01 [æ¨¡æ“¬æ•¸æ“š]'],
                'data_source': ['æ”¿åºœé–‹æ”¾å¹³å°_è²¨å¹£ä¾›çµ¦_æ¨¡æ“¬æ•¸æ“š'],
                'crawl_time': [datetime.now()],
                'data_type': ['MOCK_DATA']
            })

        url = "https://data.gov.tw/api/v1/rest/datastore/382000000A-000154-001"
        params = {
            'Authorization': api_key,
            'limit': 100
        }

        response = self._safe_request(url, params)
        if not response:
            return pd.DataFrame()

        try:
            data = response.json()
            if 'result' in data and 'records' in data['result']:
                df = pd.DataFrame(data['result']['records'])
                df['data_source'] = 'æ”¿åºœé–‹æ”¾å¹³å°_è²¨å¹£ä¾›çµ¦'
                df['crawl_time'] = datetime.now()
                logger.info(f"âœ… è²¨å¹£ä¾›çµ¦æ•¸æ“šç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"è²¨å¹£ä¾›çµ¦æ•¸æ“šè§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    def crawl_yahoo_world_indices(self) -> pd.DataFrame:
        """5. ä¸–ç•ŒæŒ‡æ•¸ - Yahoo Finance API"""
        try:
            # ä¸»è¦ä¸–ç•ŒæŒ‡æ•¸ä»£ç¢¼
            indices = {
                '^GSPC': 'S&P 500',
                '^DJI': 'Dow Jones',
                '^IXIC': 'NASDAQ',
                '^N225': 'Nikkei 225',
                '^HSI': 'Hang Seng',
                '000001.SS': 'Shanghai Composite'
            }

            world_data = []
            for symbol, name in indices.items():
                try:
                    ticker = yf.Ticker(symbol)
                    info = ticker.info

                    world_data.append({
                        'symbol': symbol,
                        'name': name,
                        'current_price': info.get('regularMarketPrice', 0),
                        'previous_close': info.get('previousClose', 0),
                        'change': info.get('regularMarketChange', 0),
                        'change_percent': info.get('regularMarketChangePercent', 0),
                        'data_source': 'Yahoo_ä¸–ç•ŒæŒ‡æ•¸',
                        'crawl_time': datetime.now()
                    })

                except Exception as e:
                    logger.warning(f"ç²å– {name} æ•¸æ“šå¤±æ•—: {e}")
                    continue

            if world_data:
                df = pd.DataFrame(world_data)
                logger.info(f"âœ… ä¸–ç•ŒæŒ‡æ•¸ç²å–æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                return df
            else:
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"ä¸–ç•ŒæŒ‡æ•¸è§£æå¤±æ•—: {e}")
            return pd.DataFrame()

    # ========================================================================
    # çµ±ä¸€æ¥å£æ–¹æ³•
    # ========================================================================

    def get_all_data_sources(self) -> Dict[str, List[str]]:
        """ç²å–æ‰€æœ‰æ•¸æ“šæºçš„åˆ†é¡æ¸…å–®"""
        return {
            'technical': [
                'crawl_twse_zero_share',
                'crawl_twse_backtest_index',
                'crawl_tpex_cb_trading',
                'crawl_twse_trading_change',
                'crawl_yahoo_adjusted_price',
                'crawl_tpex_disposal_stocks',
                'crawl_twse_market_indicators',
                'crawl_twse_attention_stocks',
                'crawl_taifex_futures_daily',
                'crawl_twse_intraday_zero_share'
            ],
            'fundamental': [
                'crawl_twse_dividend_announcement',
                'crawl_tpex_capital_reduction',
                'crawl_twse_capital_reduction',
                'crawl_gov_company_info',
                'crawl_twse_business_info',
                'crawl_twse_monthly_revenue',
                'crawl_finmind_financial_data'
            ],
            'chip': [
                'crawl_twse_broker_mapping',
                'crawl_twse_broker_trading',
                'crawl_twse_foreign_holding',
                'crawl_taifex_institutional_trading',
                'crawl_twse_margin_trading',
                'crawl_twse_securities_lending'
            ],
            'event': [
                'crawl_twse_announcements',
                'crawl_twse_investor_conference',
                'crawl_twse_shareholder_meeting',
                'crawl_twse_treasury_stock',
                'crawl_cnyes_news'
            ],
            'macro': [
                'crawl_gov_economic_indicators',
                'crawl_gov_manufacturing_pmi',
                'crawl_gov_service_pmi',
                'crawl_gov_money_supply',
                'crawl_yahoo_world_indices'
            ]
        }

    def crawl_by_category(self, category: str, **kwargs) -> Dict[str, pd.DataFrame]:
        """æŒ‰åˆ†é¡çˆ¬å–æ•¸æ“š"""
        all_sources = self.get_all_data_sources()

        if category not in all_sources:
            logger.error(f"æœªçŸ¥çš„æ•¸æ“šåˆ†é¡: {category}")
            return {}

        results = {}
        methods = all_sources[category]

        for method_name in methods:
            try:
                method = getattr(self, method_name)
                logger.info(f"æ­£åœ¨åŸ·è¡Œ: {method_name}")

                # æ ¹æ“šæ–¹æ³•éœ€è¦çš„åƒæ•¸èª¿ç”¨
                if method_name in ['crawl_yahoo_adjusted_price']:
                    if 'symbol' in kwargs and 'start_date' in kwargs and 'end_date' in kwargs:
                        df = method(kwargs['symbol'], kwargs['start_date'], kwargs['end_date'])
                    else:
                        logger.warning(f"{method_name} éœ€è¦ symbol, start_date, end_date åƒæ•¸")
                        continue
                elif method_name in ['crawl_gov_company_info', 'crawl_gov_economic_indicators',
                                   'crawl_gov_manufacturing_pmi', 'crawl_gov_service_pmi',
                                   'crawl_gov_money_supply']:
                    df = method(kwargs.get('api_key'))
                else:
                    df = method()

                if not df.empty:
                    results[method_name] = df
                    logger.info(f"âœ… {method_name} æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„")
                else:
                    logger.warning(f"âš ï¸ {method_name} ç„¡æ•¸æ“š")

            except Exception as e:
                logger.error(f"âŒ {method_name} åŸ·è¡Œå¤±æ•—: {e}")
                continue

        return results

    def crawl_all_data(self, **kwargs) -> Dict[str, Dict[str, pd.DataFrame]]:
        """çˆ¬å–æ‰€æœ‰åˆ†é¡çš„æ•¸æ“š"""
        all_results = {}
        categories = ['technical', 'fundamental', 'chip', 'event', 'macro']

        for category in categories:
            logger.info(f"ğŸ”„ é–‹å§‹çˆ¬å– {category} æ•¸æ“š...")
            category_results = self.crawl_by_category(category, **kwargs)
            all_results[category] = category_results

            success_count = len(category_results)
            total_count = len(self.get_all_data_sources()[category])
            logger.info(f"ğŸ“Š {category} å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")

        return all_results
