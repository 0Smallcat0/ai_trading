#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•¸æ“šæºæ”¹é€²é©—è­‰è…³æœ¬

æ­¤è…³æœ¬é©—è­‰æ•¸æ“šæºé©—è­‰å ±å‘Šæ”¹é€²å»ºè­°çš„å¯¦æ–½æ•ˆæœï¼Œ
å°æ¯”æ”¹é€²å‰å¾Œçš„æ•¸æ“šæºå¯ç”¨æ€§å’Œå“è³ªã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- åŸ·è¡Œæ”¹é€²å¾Œçš„æ•¸æ“šæºé©—è­‰
- ç”Ÿæˆå°æ¯”å ±å‘Š
- è©•ä¼°æ”¹é€²æ•ˆæœ
- æä¾›é€²ä¸€æ­¥å„ªåŒ–å»ºè­°

Usage:
    python scripts/validate_data_source_improvements.py
    
Note:
    æ­¤è…³æœ¬å°ˆé–€é©—è­‰æ•¸æ“šæºé©—è­‰å ±å‘Šæ”¹é€²å»ºè­°çš„å¯¦æ–½æ•ˆæœï¼Œ
    ç¢ºä¿æ”¹é€²å¾Œçš„ç³»çµ±èƒ½å¤ è§£æ±ºåŸæœ‰çš„HTMLè§£æå¤±æ•—å’Œæ•¸æ“šå“è³ªå•é¡Œã€‚
"""

import sys
import os
import logging
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.data_sources.enhanced_html_parser import EnhancedHTMLParser
    from src.data_sources.multi_tier_backup_manager import MultiTierBackupManager
    from src.data_sources.enhanced_data_validator import EnhancedDataValidator
    from src.data_sources.automated_validation_pipeline import AutomatedValidationPipeline
    from src.data_sources.comprehensive_crawler import ComprehensiveCrawler
    from src.data_sources.verified_crawler import VerifiedCrawler
except ImportError as e:
    print(f"âŒ å°å…¥æ¨¡çµ„å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿å·²æ­£ç¢ºå®‰è£æ‰€æœ‰ä¾è³´ä¸¦ä¸”é …ç›®çµæ§‹æ­£ç¢º")
    sys.exit(1)

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/validation_improvements.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DataSourceImprovementValidator:
    """æ•¸æ“šæºæ”¹é€²é©—è­‰å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é©—è­‰å™¨"""
        self.output_dir = Path("logs/improvement_validation")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.html_parser = EnhancedHTMLParser()
        self.backup_manager = MultiTierBackupManager()
        self.data_validator = EnhancedDataValidator()
        self.pipeline = AutomatedValidationPipeline()
        self.comprehensive_crawler = ComprehensiveCrawler()
        self.verified_crawler = VerifiedCrawler()
        
        logger.info("âœ… æ•¸æ“šæºæ”¹é€²é©—è­‰å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def run_comprehensive_validation(self):
        """åŸ·è¡Œå…¨é¢é©—è­‰"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ•¸æ“šæºæ”¹é€²é©—è­‰")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'html_parser_test': self._test_html_parser(),
            'backup_manager_test': self._test_backup_manager(),
            'data_validator_test': self._test_data_validator(),
            'pipeline_test': self._test_validation_pipeline(),
            'crawler_improvements_test': self._test_crawler_improvements(),
            'performance_test': self._test_performance(),
            'overall_assessment': {}
        }
        
        # ç”Ÿæˆæ•´é«”è©•ä¼°
        results['overall_assessment'] = self._generate_overall_assessment(results)
        
        # ä¿å­˜çµæœ
        self._save_validation_results(results)
        
        # ç”Ÿæˆå ±å‘Š
        self._generate_improvement_report(results)
        
        logger.info("âœ… æ•¸æ“šæºæ”¹é€²é©—è­‰å®Œæˆ")
        return results
        
    def _test_html_parser(self):
        """æ¸¬è©¦å¢å¼·HTMLè§£æå™¨"""
        logger.info("ğŸ” æ¸¬è©¦å¢å¼·HTMLè§£æå™¨...")
        
        test_results = {
            'component': 'EnhancedHTMLParser',
            'tests': [],
            'overall_score': 0,
            'status': 'unknown'
        }
        
        try:
            # æ¸¬è©¦1: åˆå§‹åŒ–æ¸¬è©¦
            init_test = {
                'name': 'åˆå§‹åŒ–æ¸¬è©¦',
                'success': self.html_parser is not None,
                'details': 'æª¢æŸ¥è§£æå™¨æ˜¯å¦æ­£ç¢ºåˆå§‹åŒ–'
            }
            test_results['tests'].append(init_test)
            
            # æ¸¬è©¦2: é¸æ“‡å™¨æ˜ å°„æ¸¬è©¦
            mapping_test = {
                'name': 'é¸æ“‡å™¨æ˜ å°„æ¸¬è©¦',
                'success': len(self.html_parser.selector_mapping) >= 5,
                'details': f'æ”¯æ´ {len(self.html_parser.selector_mapping)} ç¨®æ•¸æ“šé¡å‹'
            }
            test_results['tests'].append(mapping_test)
            
            # æ¸¬è©¦3: å¥åº·ç‹€æ…‹æ¸¬è©¦
            health_status = self.html_parser.get_health_status()
            health_test = {
                'name': 'å¥åº·ç‹€æ…‹æ¸¬è©¦',
                'success': isinstance(health_status, dict) and 'parser_name' in health_status,
                'details': f'å¥åº·ç‹€æ…‹: {health_status.get("parser_name", "æœªçŸ¥")}'
            }
            test_results['tests'].append(health_test)
            
            # æ¸¬è©¦4: å‹•æ…‹æ›´æ–°æ¸¬è©¦
            test_config = {
                'table_keywords': ['æ¸¬è©¦'],
                'selectors': ['table.test'],
                'fallback_url_patterns': ['/test']
            }
            self.html_parser.update_selector_mapping('æ¸¬è©¦é¡å‹', test_config)
            update_test = {
                'name': 'å‹•æ…‹æ›´æ–°æ¸¬è©¦',
                'success': 'æ¸¬è©¦é¡å‹' in self.html_parser.selector_mapping,
                'details': 'é¸æ“‡å™¨æ˜ å°„å‹•æ…‹æ›´æ–°åŠŸèƒ½æ­£å¸¸'
            }
            test_results['tests'].append(update_test)
            
            # è¨ˆç®—ç¸½åˆ†
            success_count = sum(1 for test in test_results['tests'] if test['success'])
            test_results['overall_score'] = (success_count / len(test_results['tests'])) * 100
            test_results['status'] = 'passed' if test_results['overall_score'] >= 75 else 'failed'
            
        except Exception as e:
            logger.error(f"HTMLè§£æå™¨æ¸¬è©¦å¤±æ•—: {e}")
            test_results['status'] = 'error'
            test_results['error'] = str(e)
            
        return test_results
        
    def _test_backup_manager(self):
        """æ¸¬è©¦å¤šå±¤å‚™æ´æ©Ÿåˆ¶ç®¡ç†å™¨"""
        logger.info("ğŸ” æ¸¬è©¦å¤šå±¤å‚™æ´æ©Ÿåˆ¶ç®¡ç†å™¨...")
        
        test_results = {
            'component': 'MultiTierBackupManager',
            'tests': [],
            'overall_score': 0,
            'status': 'unknown'
        }
        
        try:
            # æ¸¬è©¦1: åˆå§‹åŒ–æ¸¬è©¦
            init_test = {
                'name': 'åˆå§‹åŒ–æ¸¬è©¦',
                'success': self.backup_manager is not None,
                'details': 'æª¢æŸ¥å‚™æ´ç®¡ç†å™¨æ˜¯å¦æ­£ç¢ºåˆå§‹åŒ–'
            }
            test_results['tests'].append(init_test)
            
            # æ¸¬è©¦2: å‚™æ´é…ç½®æ¸¬è©¦
            registry_test = {
                'name': 'å‚™æ´é…ç½®æ¸¬è©¦',
                'success': len(self.backup_manager.backup_registry) >= 3,
                'details': f'é…ç½®äº† {len(self.backup_manager.backup_registry)} å€‹æ•¸æ“šåˆ†é¡çš„å‚™æ´'
            }
            test_results['tests'].append(registry_test)
            
            # æ¸¬è©¦3: å¥åº·å ±å‘Šæ¸¬è©¦
            health_report = self.backup_manager.get_health_report()
            health_test = {
                'name': 'å¥åº·å ±å‘Šæ¸¬è©¦',
                'success': isinstance(health_report, dict) and 'total_sources' in health_report,
                'details': f'ç›£æ§ {health_report.get("total_sources", 0)} å€‹æ•¸æ“šæº'
            }
            test_results['tests'].append(health_test)
            
            # æ¸¬è©¦4: æ•¸æ“šæºè¨»å†Šæ¸¬è©¦
            from src.data_sources.multi_tier_backup_manager import DataSourceConfig
            test_config = DataSourceConfig(
                name='test_source',
                priority=1,
                crawler_class='TestCrawler',
                method_name='test_method'
            )
            self.backup_manager.register_backup_source('æ¸¬è©¦åˆ†é¡', 'æ¸¬è©¦é¡å‹', test_config)
            
            register_test = {
                'name': 'æ•¸æ“šæºè¨»å†Šæ¸¬è©¦',
                'success': 'æ¸¬è©¦åˆ†é¡' in self.backup_manager.backup_registry,
                'details': 'å‹•æ…‹æ•¸æ“šæºè¨»å†ŠåŠŸèƒ½æ­£å¸¸'
            }
            test_results['tests'].append(register_test)
            
            # è¨ˆç®—ç¸½åˆ†
            success_count = sum(1 for test in test_results['tests'] if test['success'])
            test_results['overall_score'] = (success_count / len(test_results['tests'])) * 100
            test_results['status'] = 'passed' if test_results['overall_score'] >= 75 else 'failed'
            
        except Exception as e:
            logger.error(f"å‚™æ´ç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
            test_results['status'] = 'error'
            test_results['error'] = str(e)
            
        return test_results
        
    def _test_data_validator(self):
        """æ¸¬è©¦å¢å¼·æ•¸æ“šé©—è­‰å™¨"""
        logger.info("ğŸ” æ¸¬è©¦å¢å¼·æ•¸æ“šé©—è­‰å™¨...")
        
        test_results = {
            'component': 'EnhancedDataValidator',
            'tests': [],
            'overall_score': 0,
            'status': 'unknown'
        }
        
        try:
            import pandas as pd
            
            # æ¸¬è©¦1: åˆå§‹åŒ–æ¸¬è©¦
            init_test = {
                'name': 'åˆå§‹åŒ–æ¸¬è©¦',
                'success': self.data_validator is not None,
                'details': 'æª¢æŸ¥æ•¸æ“šé©—è­‰å™¨æ˜¯å¦æ­£ç¢ºåˆå§‹åŒ–'
            }
            test_results['tests'].append(init_test)
            
            # æ¸¬è©¦2: ç©ºæ•¸æ“šé©—è­‰æ¸¬è©¦
            empty_df = pd.DataFrame()
            empty_report = self.data_validator.validate_data_quality(empty_df)
            empty_test = {
                'name': 'ç©ºæ•¸æ“šé©—è­‰æ¸¬è©¦',
                'success': empty_report.quality_score == 0.0,
                'details': f'ç©ºæ•¸æ“šå“è³ªåˆ†æ•¸: {empty_report.quality_score}'
            }
            test_results['tests'].append(empty_test)
            
            # æ¸¬è©¦3: æ­£å¸¸æ•¸æ“šé©—è­‰æ¸¬è©¦
            normal_df = pd.DataFrame({
                'value': [1, 2, 3, 4, 5],
                'category': ['A', 'B', 'C', 'D', 'E']
            })
            normal_report = self.data_validator.validate_data_quality(normal_df)
            normal_test = {
                'name': 'æ­£å¸¸æ•¸æ“šé©—è­‰æ¸¬è©¦',
                'success': normal_report.quality_score > 80.0,
                'details': f'æ­£å¸¸æ•¸æ“šå“è³ªåˆ†æ•¸: {normal_report.quality_score:.1f}'
            }
            test_results['tests'].append(normal_test)
            
            # æ¸¬è©¦4: ç•°å¸¸æª¢æ¸¬æ¸¬è©¦
            outlier_df = pd.DataFrame({
                'value': [1, 2, 3, 4, 100]  # 100æ˜¯ç•°å¸¸å€¼
            })
            from src.data_sources.enhanced_data_validator import AnomalyMethod
            anomalies = self.data_validator._detect_anomalies(outlier_df, AnomalyMethod.ZSCORE, 'general')
            anomaly_test = {
                'name': 'ç•°å¸¸æª¢æ¸¬æ¸¬è©¦',
                'success': len(anomalies) > 0,
                'details': f'æª¢æ¸¬åˆ° {len(anomalies)} å€‹ç•°å¸¸å€¼'
            }
            test_results['tests'].append(anomaly_test)
            
            # è¨ˆç®—ç¸½åˆ†
            success_count = sum(1 for test in test_results['tests'] if test['success'])
            test_results['overall_score'] = (success_count / len(test_results['tests'])) * 100
            test_results['status'] = 'passed' if test_results['overall_score'] >= 75 else 'failed'
            
        except Exception as e:
            logger.error(f"æ•¸æ“šé©—è­‰å™¨æ¸¬è©¦å¤±æ•—: {e}")
            test_results['status'] = 'error'
            test_results['error'] = str(e)
            
        return test_results
        
    def _test_validation_pipeline(self):
        """æ¸¬è©¦è‡ªå‹•åŒ–é©—è­‰ç®¡é“"""
        logger.info("ğŸ” æ¸¬è©¦è‡ªå‹•åŒ–é©—è­‰ç®¡é“...")
        
        test_results = {
            'component': 'AutomatedValidationPipeline',
            'tests': [],
            'overall_score': 0,
            'status': 'unknown'
        }
        
        try:
            # æ¸¬è©¦1: åˆå§‹åŒ–æ¸¬è©¦
            init_test = {
                'name': 'åˆå§‹åŒ–æ¸¬è©¦',
                'success': self.pipeline is not None,
                'details': 'æª¢æŸ¥é©—è­‰ç®¡é“æ˜¯å¦æ­£ç¢ºåˆå§‹åŒ–'
            }
            test_results['tests'].append(init_test)
            
            # æ¸¬è©¦2: é…ç½®æ¸¬è©¦
            config = self.pipeline.validation_config
            config_test = {
                'name': 'é…ç½®æ¸¬è©¦',
                'success': 'verified_sources' in config and 'comprehensive_sources' in config,
                'details': 'é©—è­‰é…ç½®çµæ§‹æ­£ç¢º'
            }
            test_results['tests'].append(config_test)
            
            # æ¸¬è©¦3: çµ„ä»¶æ•´åˆæ¸¬è©¦
            components = ['html_parser', 'backup_manager', 'data_validator']
            integration_test = {
                'name': 'çµ„ä»¶æ•´åˆæ¸¬è©¦',
                'success': all(hasattr(self.pipeline, comp) for comp in components),
                'details': f'æ•´åˆäº† {len(components)} å€‹æ ¸å¿ƒçµ„ä»¶'
            }
            test_results['tests'].append(integration_test)
            
            # è¨ˆç®—ç¸½åˆ†
            success_count = sum(1 for test in test_results['tests'] if test['success'])
            test_results['overall_score'] = (success_count / len(test_results['tests'])) * 100
            test_results['status'] = 'passed' if test_results['overall_score'] >= 75 else 'failed'
            
        except Exception as e:
            logger.error(f"é©—è­‰ç®¡é“æ¸¬è©¦å¤±æ•—: {e}")
            test_results['status'] = 'error'
            test_results['error'] = str(e)
            
        return test_results
        
    def _test_crawler_improvements(self):
        """æ¸¬è©¦çˆ¬èŸ²æ”¹é€²"""
        logger.info("ğŸ” æ¸¬è©¦çˆ¬èŸ²æ”¹é€²...")
        
        test_results = {
            'component': 'CrawlerImprovements',
            'tests': [],
            'overall_score': 0,
            'status': 'unknown'
        }
        
        try:
            # æ¸¬è©¦1: ç¶œåˆçˆ¬èŸ²HTMLè§£æå™¨æ•´åˆ
            html_integration_test = {
                'name': 'HTMLè§£æå™¨æ•´åˆæ¸¬è©¦',
                'success': hasattr(self.comprehensive_crawler, 'html_parser'),
                'details': 'ç¶œåˆçˆ¬èŸ²å·²æ•´åˆå¢å¼·HTMLè§£æå™¨'
            }
            test_results['tests'].append(html_integration_test)
            
            # æ¸¬è©¦2: çˆ¬èŸ²æ–¹æ³•å¯ç”¨æ€§æ¸¬è©¦
            methods_to_test = [
                'crawl_twse_dividend_announcement',
                'crawl_twse_monthly_revenue',
                'crawl_twse_announcements'
            ]
            
            available_methods = 0
            for method_name in methods_to_test:
                if hasattr(self.comprehensive_crawler, method_name):
                    available_methods += 1
                    
            methods_test = {
                'name': 'æ”¹é€²æ–¹æ³•å¯ç”¨æ€§æ¸¬è©¦',
                'success': available_methods == len(methods_to_test),
                'details': f'{available_methods}/{len(methods_to_test)} å€‹æ”¹é€²æ–¹æ³•å¯ç”¨'
            }
            test_results['tests'].append(methods_test)
            
            # è¨ˆç®—ç¸½åˆ†
            success_count = sum(1 for test in test_results['tests'] if test['success'])
            test_results['overall_score'] = (success_count / len(test_results['tests'])) * 100
            test_results['status'] = 'passed' if test_results['overall_score'] >= 75 else 'failed'
            
        except Exception as e:
            logger.error(f"çˆ¬èŸ²æ”¹é€²æ¸¬è©¦å¤±æ•—: {e}")
            test_results['status'] = 'error'
            test_results['error'] = str(e)
            
        return test_results
        
    def _test_performance(self):
        """æ¸¬è©¦æ€§èƒ½"""
        logger.info("ğŸ” æ¸¬è©¦æ€§èƒ½...")
        
        test_results = {
            'component': 'Performance',
            'tests': [],
            'overall_score': 0,
            'status': 'unknown'
        }
        
        try:
            import time
            import pandas as pd
            
            # æ¸¬è©¦1: æ•¸æ“šé©—è­‰æ€§èƒ½
            large_df = pd.DataFrame({
                'value': range(1000),
                'category': ['A'] * 500 + ['B'] * 500
            })
            
            start_time = time.time()
            report = self.data_validator.validate_data_quality(large_df)
            validation_time = time.time() - start_time
            
            validation_perf_test = {
                'name': 'æ•¸æ“šé©—è­‰æ€§èƒ½æ¸¬è©¦',
                'success': validation_time < 5.0,
                'details': f'1000ç­†è¨˜éŒ„é©—è­‰è€—æ™‚: {validation_time:.2f}ç§’'
            }
            test_results['tests'].append(validation_perf_test)
            
            # æ¸¬è©¦2: çµ„ä»¶åˆå§‹åŒ–æ€§èƒ½
            start_time = time.time()
            test_parser = EnhancedHTMLParser()
            init_time = time.time() - start_time
            
            init_perf_test = {
                'name': 'çµ„ä»¶åˆå§‹åŒ–æ€§èƒ½æ¸¬è©¦',
                'success': init_time < 2.0,
                'details': f'HTMLè§£æå™¨åˆå§‹åŒ–è€—æ™‚: {init_time:.2f}ç§’'
            }
            test_results['tests'].append(init_perf_test)
            
            # è¨ˆç®—ç¸½åˆ†
            success_count = sum(1 for test in test_results['tests'] if test['success'])
            test_results['overall_score'] = (success_count / len(test_results['tests'])) * 100
            test_results['status'] = 'passed' if test_results['overall_score'] >= 75 else 'failed'
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            test_results['status'] = 'error'
            test_results['error'] = str(e)
            
        return test_results
        
    def _generate_overall_assessment(self, results):
        """ç”Ÿæˆæ•´é«”è©•ä¼°"""
        components = [
            'html_parser_test',
            'backup_manager_test', 
            'data_validator_test',
            'pipeline_test',
            'crawler_improvements_test',
            'performance_test'
        ]
        
        total_score = 0
        passed_components = 0
        
        for comp in components:
            if comp in results and results[comp].get('status') == 'passed':
                passed_components += 1
                total_score += results[comp].get('overall_score', 0)
                
        avg_score = total_score / len(components) if components else 0
        success_rate = (passed_components / len(components)) * 100 if components else 0
        
        # ç¢ºå®šæ•´é«”ç‹€æ…‹
        if success_rate >= 80 and avg_score >= 75:
            overall_status = 'excellent'
            status_desc = 'å„ªç§€'
        elif success_rate >= 60 and avg_score >= 60:
            overall_status = 'good'
            status_desc = 'è‰¯å¥½'
        elif success_rate >= 40 and avg_score >= 40:
            overall_status = 'fair'
            status_desc = 'ä¸€èˆ¬'
        else:
            overall_status = 'poor'
            status_desc = 'éœ€è¦æ”¹é€²'
            
        return {
            'overall_status': overall_status,
            'status_description': status_desc,
            'average_score': round(avg_score, 1),
            'success_rate': round(success_rate, 1),
            'passed_components': passed_components,
            'total_components': len(components),
            'recommendations': self._generate_improvement_recommendations(results)
        }
        
    def _generate_improvement_recommendations(self, results):
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # æª¢æŸ¥å„çµ„ä»¶ç‹€æ…‹ä¸¦ç”Ÿæˆå»ºè­°
        if results.get('html_parser_test', {}).get('status') != 'passed':
            recommendations.append("å¢å¼·HTMLè§£æå™¨éœ€è¦é€²ä¸€æ­¥å„ªåŒ–ï¼Œå»ºè­°æª¢æŸ¥Seleniumé…ç½®")
            
        if results.get('backup_manager_test', {}).get('status') != 'passed':
            recommendations.append("å¤šå±¤å‚™æ´æ©Ÿåˆ¶éœ€è¦å®Œå–„ï¼Œå»ºè­°å¢åŠ æ›´å¤šå‚™æ´æ•¸æ“šæº")
            
        if results.get('data_validator_test', {}).get('status') != 'passed':
            recommendations.append("æ•¸æ“šé©—è­‰å™¨éœ€è¦æ”¹é€²ï¼Œå»ºè­°å„ªåŒ–ç•°å¸¸æª¢æ¸¬ç®—æ³•")
            
        if results.get('performance_test', {}).get('overall_score', 0) < 75:
            recommendations.append("ç³»çµ±æ€§èƒ½éœ€è¦å„ªåŒ–ï¼Œå»ºè­°å¯¦æ–½ç·©å­˜æ©Ÿåˆ¶å’Œä¸¦è¡Œè™•ç†")
            
        if not recommendations:
            recommendations.append("ç³»çµ±æ”¹é€²æ•ˆæœè‰¯å¥½ï¼Œå»ºè­°ç¹¼çºŒç›£æ§å’Œç¶­è­·")
            
        return recommendations
        
    def _save_validation_results(self, results):
        """ä¿å­˜é©—è­‰çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.output_dir / f"improvement_validation_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        logger.info(f"âœ… é©—è­‰çµæœå·²ä¿å­˜: {results_file}")
        
    def _generate_improvement_report(self, results):
        """ç”Ÿæˆæ”¹é€²å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"improvement_report_{timestamp}.html"
        
        assessment = results['overall_assessment']
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>æ•¸æ“šæºæ”¹é€²é©—è­‰å ±å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background: #f0f8ff; padding: 20px; border-radius: 10px; }}
                .summary {{ background: #f5f5f5; padding: 15px; border-radius: 5px; margin: 20px 0; }}
                .component {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .passed {{ background: #e8f5e8; }}
                .failed {{ background: #ffe8e8; }}
                .error {{ background: #fff3cd; }}
                .score {{ font-size: 24px; font-weight: bold; }}
                .recommendations {{ background: #e3f2fd; padding: 15px; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸš€ æ•¸æ“šæºæ”¹é€²é©—è­‰å ±å‘Š</h1>
                <p>é©—è­‰æ™‚é–“: {results['timestamp']}</p>
                <p>æ•´é«”ç‹€æ…‹: <span class="score">{assessment['status_description']}</span></p>
                <p>å¹³å‡åˆ†æ•¸: <span class="score">{assessment['average_score']}</span></p>
                <p>æˆåŠŸç‡: <span class="score">{assessment['success_rate']}%</span></p>
            </div>
            
            <div class="summary">
                <h2>ğŸ“Š é©—è­‰æ‘˜è¦</h2>
                <p>é€šéçµ„ä»¶: {assessment['passed_components']}/{assessment['total_components']}</p>
                <p>æ•´é«”è©•ä¼°: {assessment['overall_status']}</p>
            </div>
        """
        
        # æ·»åŠ å„çµ„ä»¶è©³ç´°çµæœ
        components = [
            ('html_parser_test', 'å¢å¼·HTMLè§£æå™¨'),
            ('backup_manager_test', 'å¤šå±¤å‚™æ´æ©Ÿåˆ¶ç®¡ç†å™¨'),
            ('data_validator_test', 'å¢å¼·æ•¸æ“šé©—è­‰å™¨'),
            ('pipeline_test', 'è‡ªå‹•åŒ–é©—è­‰ç®¡é“'),
            ('crawler_improvements_test', 'çˆ¬èŸ²æ”¹é€²'),
            ('performance_test', 'æ€§èƒ½æ¸¬è©¦')
        ]
        
        for comp_key, comp_name in components:
            if comp_key in results:
                comp_result = results[comp_key]
                status_class = comp_result.get('status', 'unknown')
                
                html_content += f"""
                <div class="component {status_class}">
                    <h3>{comp_name}</h3>
                    <p>ç‹€æ…‹: {comp_result.get('status', 'æœªçŸ¥')}</p>
                    <p>åˆ†æ•¸: {comp_result.get('overall_score', 0):.1f}</p>
                    <ul>
                """
                
                for test in comp_result.get('tests', []):
                    status_icon = "âœ…" if test['success'] else "âŒ"
                    html_content += f"<li>{status_icon} {test['name']}: {test['details']}</li>"
                    
                html_content += "</ul></div>"
                
        # æ·»åŠ å»ºè­°
        html_content += f"""
            <div class="recommendations">
                <h2>ğŸ’¡ æ”¹é€²å»ºè­°</h2>
                <ul>
        """
        
        for rec in assessment['recommendations']:
            html_content += f"<li>{rec}</li>"
            
        html_content += """
                </ul>
            </div>
        </body>
        </html>
        """
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        logger.info(f"âœ… æ”¹é€²å ±å‘Šå·²ç”Ÿæˆ: {report_file}")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ•¸æ“šæºæ”¹é€²é©—è­‰...")
    
    try:
        # å‰µå»ºé©—è­‰å™¨
        validator = DataSourceImprovementValidator()
        
        # åŸ·è¡Œé©—è­‰
        results = validator.run_comprehensive_validation()
        
        # è¼¸å‡ºçµæœæ‘˜è¦
        assessment = results['overall_assessment']
        print(f"\nğŸ“Š é©—è­‰çµæœæ‘˜è¦:")
        print(f"æ•´é«”ç‹€æ…‹: {assessment['status_description']}")
        print(f"å¹³å‡åˆ†æ•¸: {assessment['average_score']}")
        print(f"æˆåŠŸç‡: {assessment['success_rate']}%")
        print(f"é€šéçµ„ä»¶: {assessment['passed_components']}/{assessment['total_components']}")
        
        print(f"\nğŸ’¡ ä¸»è¦å»ºè­°:")
        for i, rec in enumerate(assessment['recommendations'][:3], 1):
            print(f"{i}. {rec}")
            
        print(f"\nâœ… é©—è­‰å®Œæˆï¼è©³ç´°å ±å‘Šè«‹æŸ¥çœ‹ logs/improvement_validation/ ç›®éŒ„")
        
    except Exception as e:
        logger.error(f"âŒ é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        print(f"âŒ é©—è­‰å¤±æ•—: {e}")
        return 1
        
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
