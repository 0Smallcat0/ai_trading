#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æª”æ¡ˆå¤§å°åˆ†æå·¥å…·

æ­¤è…³æœ¬ç”¨æ–¼åˆ†æå°ˆæ¡ˆä¸­çš„Pythonæª”æ¡ˆå¤§å°ï¼Œè­˜åˆ¥éœ€è¦é‡æ§‹çš„è¶…å¤§æª”æ¡ˆï¼Œ
ä¸¦æŒ‰ç…§å„ªå…ˆç´šç”Ÿæˆé‡æ§‹è¨ˆåŠƒã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/analyze_file_sizes.py
    python scripts/analyze_file_sizes.py --threshold 300
    python scripts/analyze_file_sizes.py --output results/file_analysis.json
"""

import os
import sys
import json
import argparse
from typing import Dict, List, Tuple, Optional
from pathlib import Path
from datetime import datetime


class FileAnalyzer:
    """æª”æ¡ˆå¤§å°åˆ†æå™¨"""
    
    def __init__(self, threshold: int = 300):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            threshold: æª”æ¡ˆè¡Œæ•¸é–¾å€¼ï¼Œè¶…éæ­¤å€¼è¦–ç‚ºéœ€è¦é‡æ§‹
        """
        self.threshold = threshold
        self.priority_mapping = {
            # ç¬¬ä¸€å„ªå…ˆç´šï¼šæ ¸å¿ƒæ¥­å‹™é‚è¼¯
            'src/core/': 1,
            'src/api/': 1, 
            'src/risk_management/': 1,
            
            # ç¬¬äºŒå„ªå…ˆç´šï¼šç”¨æˆ¶ç•Œé¢å’ŒåŸ·è¡Œ
            'src/ui/': 2,
            'src/strategies/': 2,
            'src/execution/': 2,
            
            # ç¬¬ä¸‰å„ªå…ˆç´šï¼šå·¥å…·å’Œæ¸¬è©¦
            'scripts/': 3,
            'tests/': 3,
            'src/utils/': 3,
            
            # å…¶ä»–æ¨¡çµ„
            'src/': 2  # é»˜èªç¬¬äºŒå„ªå…ˆç´š
        }
    
    def count_lines(self, filepath: str) -> int:
        """è¨ˆç®—æª”æ¡ˆè¡Œæ•¸
        
        Args:
            filepath: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            æª”æ¡ˆè¡Œæ•¸
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return len(f.readlines())
        except (UnicodeDecodeError, IOError):
            try:
                with open(filepath, 'r', encoding='gbk') as f:
                    return len(f.readlines())
            except (UnicodeDecodeError, IOError):
                return 0
    
    def get_priority(self, filepath: str) -> int:
        """ç²å–æª”æ¡ˆé‡æ§‹å„ªå…ˆç´š

        Args:
            filepath: æª”æ¡ˆè·¯å¾‘

        Returns:
            å„ªå…ˆç´š (1=æœ€é«˜, 2=ä¸­ç­‰, 3=æœ€ä½)
        """
        # æ¨™æº–åŒ–è·¯å¾‘åˆ†éš”ç¬¦
        normalized_path = filepath.replace('\\', '/').replace('./', '')

        # æŒ‰ç…§æœ€å…·é«”çš„è·¯å¾‘åŒ¹é…
        for path_prefix, priority in sorted(self.priority_mapping.items(), key=len, reverse=True):
            if normalized_path.startswith(path_prefix):
                return priority
        return 3  # é»˜èªæœ€ä½å„ªå…ˆç´š
    
    def analyze_directory(self, directory: str = '.') -> Dict:
        """åˆ†æç›®éŒ„ä¸­çš„æ‰€æœ‰Pythonæª”æ¡ˆ
        
        Args:
            directory: è¦åˆ†æçš„ç›®éŒ„è·¯å¾‘
            
        Returns:
            åˆ†æçµæœå­—å…¸
        """
        results = {
            'timestamp': datetime.now().isoformat(),
            'threshold': self.threshold,
            'total_files': 0,
            'oversized_files': 0,
            'files_by_priority': {1: [], 2: [], 3: []},
            'statistics': {
                'priority_1_count': 0,
                'priority_2_count': 0, 
                'priority_3_count': 0,
                'max_lines': 0,
                'avg_lines': 0
            }
        }
        
        all_files = []
        total_lines = 0
        
        # æƒææ‰€æœ‰Pythonæª”æ¡ˆ
        for root, dirs, files in os.walk(directory):
            # è·³ééš±è—ç›®éŒ„å’Œ__pycache__
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']
            
            for file in files:
                if file.endswith('.py'):
                    filepath = os.path.join(root, file).replace('\\', '/')
                    line_count = self.count_lines(filepath)
                    priority = self.get_priority(filepath)
                    
                    file_info = {
                        'path': filepath,
                        'lines': line_count,
                        'priority': priority,
                        'oversized': line_count > self.threshold,
                        'severity': 'error' if line_count > self.threshold else 'warning' if line_count > 250 else 'ok'
                    }
                    
                    all_files.append(file_info)
                    total_lines += line_count
                    results['total_files'] += 1
                    
                    if line_count > self.threshold:
                        results['oversized_files'] += 1
                        results['files_by_priority'][priority].append(file_info)
                        results['statistics'][f'priority_{priority}_count'] += 1
                    
                    if line_count > results['statistics']['max_lines']:
                        results['statistics']['max_lines'] = line_count
        
        # è¨ˆç®—å¹³å‡è¡Œæ•¸
        if results['total_files'] > 0:
            results['statistics']['avg_lines'] = total_lines / results['total_files']
        
        # æŒ‰è¡Œæ•¸æ’åºæ¯å€‹å„ªå…ˆç´šçš„æª”æ¡ˆ
        for priority in results['files_by_priority']:
            results['files_by_priority'][priority].sort(key=lambda x: x['lines'], reverse=True)
        
        return results
    
    def generate_refactor_plan(self, analysis_results: Dict) -> List[Dict]:
        """ç”Ÿæˆé‡æ§‹è¨ˆåŠƒ
        
        Args:
            analysis_results: åˆ†æçµæœ
            
        Returns:
            é‡æ§‹è¨ˆåŠƒåˆ—è¡¨
        """
        refactor_plan = []
        
        # æŒ‰å„ªå…ˆç´šè™•ç†
        for priority in [1, 2, 3]:
            files = analysis_results['files_by_priority'][priority]
            
            for file_info in files:
                plan_item = {
                    'file': file_info['path'],
                    'lines': file_info['lines'],
                    'priority': priority,
                    'estimated_effort': self._estimate_effort(file_info['lines']),
                    'refactor_strategy': self._suggest_strategy(file_info),
                    'target_files': self._suggest_split(file_info)
                }
                refactor_plan.append(plan_item)
        
        return refactor_plan
    
    def _estimate_effort(self, lines: int) -> str:
        """ä¼°ç®—é‡æ§‹å·¥ä½œé‡
        
        Args:
            lines: æª”æ¡ˆè¡Œæ•¸
            
        Returns:
            å·¥ä½œé‡ä¼°ç®— (small/medium/large/huge)
        """
        if lines <= 400:
            return 'small'
        elif lines <= 600:
            return 'medium'
        elif lines <= 1000:
            return 'large'
        else:
            return 'huge'
    
    def _suggest_strategy(self, file_info: Dict) -> List[str]:
        """å»ºè­°é‡æ§‹ç­–ç•¥
        
        Args:
            file_info: æª”æ¡ˆè³‡è¨Š
            
        Returns:
            é‡æ§‹ç­–ç•¥åˆ—è¡¨
        """
        strategies = []
        lines = file_info['lines']
        
        if lines > 300:
            strategies.append('split_into_modules')
        if lines > 500:
            strategies.append('extract_classes')
        if lines > 800:
            strategies.append('separate_concerns')
        if lines > 1000:
            strategies.append('major_refactoring')
            
        strategies.extend([
            'add_docstrings',
            'improve_error_handling',
            'reduce_complexity'
        ])
        
        return strategies
    
    def _suggest_split(self, file_info: Dict) -> List[str]:
        """å»ºè­°æª”æ¡ˆæ‹†åˆ†æ–¹æ¡ˆ
        
        Args:
            file_info: æª”æ¡ˆè³‡è¨Š
            
        Returns:
            å»ºè­°çš„ç›®æ¨™æª”æ¡ˆåˆ—è¡¨
        """
        filepath = file_info['path']
        lines = file_info['lines']
        
        if lines <= 400:
            return [filepath]  # ä¸éœ€è¦æ‹†åˆ†
        
        # æ ¹æ“šæª”æ¡ˆé¡å‹å’Œå¤§å°å»ºè­°æ‹†åˆ†
        base_name = os.path.splitext(filepath)[0]
        
        if 'service' in filepath:
            return [
                f"{base_name}_core.py",
                f"{base_name}_utils.py",
                f"{base_name}_models.py"
            ]
        elif 'adapter' in filepath:
            return [
                f"{base_name}_base.py", 
                f"{base_name}_impl.py"
            ]
        else:
            # é€šç”¨æ‹†åˆ†æ–¹æ¡ˆ
            num_files = min(4, max(2, lines // 300))
            return [f"{base_name}_part{i+1}.py" for i in range(num_files)]


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='åˆ†æPythonæª”æ¡ˆå¤§å°ä¸¦ç”Ÿæˆé‡æ§‹è¨ˆåŠƒ')
    parser.add_argument('--threshold', type=int, default=300, help='æª”æ¡ˆè¡Œæ•¸é–¾å€¼')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--directory', type=str, default='.', help='è¦åˆ†æçš„ç›®éŒ„')
    
    args = parser.parse_args()
    
    analyzer = FileAnalyzer(threshold=args.threshold)
    
    print(f"ğŸ” åˆ†æç›®éŒ„: {args.directory}")
    print(f"ğŸ“ è¡Œæ•¸é–¾å€¼: {args.threshold}")
    print("=" * 60)
    
    # åŸ·è¡Œåˆ†æ
    results = analyzer.analyze_directory(args.directory)
    
    # é¡¯ç¤ºçµ±è¨ˆçµæœ
    print(f"ğŸ“Š åˆ†æçµæœ:")
    print(f"  ç¸½æª”æ¡ˆæ•¸: {results['total_files']}")
    print(f"  è¶…å¤§æª”æ¡ˆæ•¸: {results['oversized_files']}")
    print(f"  æœ€å¤§æª”æ¡ˆè¡Œæ•¸: {results['statistics']['max_lines']}")
    print(f"  å¹³å‡æª”æ¡ˆè¡Œæ•¸: {results['statistics']['avg_lines']:.1f}")
    print()
    
    print("ğŸ“‹ æŒ‰å„ªå…ˆç´šåˆ†é¡:")
    for priority in [1, 2, 3]:
        count = results['statistics'][f'priority_{priority}_count']
        priority_name = ['', 'æ ¸å¿ƒæ¨¡çµ„', 'ç•Œé¢åŸ·è¡Œ', 'å·¥å…·æ¸¬è©¦'][priority]
        print(f"  å„ªå…ˆç´š {priority} ({priority_name}): {count} å€‹æª”æ¡ˆ")
    print()
    
    # é¡¯ç¤ºå‰10å€‹æœ€å¤§çš„æª”æ¡ˆ
    all_oversized = []
    for priority in [1, 2, 3]:
        all_oversized.extend(results['files_by_priority'][priority])
    
    all_oversized.sort(key=lambda x: x['lines'], reverse=True)
    
    print("ğŸ”¥ å‰10å€‹æœ€å¤§æª”æ¡ˆ:")
    for i, file_info in enumerate(all_oversized[:10], 1):
        priority_name = ['', 'æ ¸å¿ƒ', 'ç•Œé¢', 'å·¥å…·'][file_info['priority']]
        print(f"  {i:2d}. {file_info['path']}: {file_info['lines']} è¡Œ ({priority_name})")
    
    # ç”Ÿæˆé‡æ§‹è¨ˆåŠƒ
    refactor_plan = analyzer.generate_refactor_plan(results)
    
    # è¼¸å‡ºçµæœ
    output_data = {
        'analysis': results,
        'refactor_plan': refactor_plan
    }
    
    if args.output:
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {args.output}")
    
    print(f"\nğŸ¯ å»ºè­°å„ªå…ˆè™•ç† {len(refactor_plan)} å€‹æª”æ¡ˆ")
    print("   ä½¿ç”¨ --output åƒæ•¸ä¿å­˜è©³ç´°çš„é‡æ§‹è¨ˆåŠƒ")


if __name__ == "__main__":
    main()
