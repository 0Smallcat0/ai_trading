#!/usr/bin/env python3
"""
æ€§èƒ½åŸºæº–æ¸¬è©¦å·¥å…·

æ­¤è…³æœ¬ç”¨æ–¼æ¸¬è©¦å’Œæ¯”è¼ƒæ•´åˆå‰å¾Œçš„ç³»çµ±æ€§èƒ½ï¼Œ
ç›£æ§æ¨è–¦æ¨¡çµ„çš„è¨˜æ†¶é«”ä½¿ç”¨å’ŒåŸ·è¡Œæ™‚é–“ã€‚

Usage:
    python scripts/performance_benchmark.py
    python scripts/performance_benchmark.py --module risk_management
    python scripts/performance_benchmark.py --compare --baseline baseline.json
"""

import time
import psutil
import json
import argparse
import tracemalloc
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import sys
import os

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™"""
    module_name: str
    execution_time: float  # ç§’
    memory_usage: float    # MB
    cpu_usage: float       # ç™¾åˆ†æ¯”
    import_time: float     # å°å…¥æ™‚é–“ï¼ˆç§’ï¼‰
    timestamp: str
    version: str = "unknown"


class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.results = []
        self.baseline_data = None
        
    def measure_import_time(self, module_path: str) -> float:
        """æ¸¬é‡æ¨¡çµ„å°å…¥æ™‚é–“"""
        start_time = time.perf_counter()
        try:
            # å‹•æ…‹å°å…¥æ¨¡çµ„
            parts = module_path.split('.')
            module = __import__(module_path)
            for part in parts[1:]:
                module = getattr(module, part)
            end_time = time.perf_counter()
            return end_time - start_time
        except ImportError as e:
            print(f"âš ï¸ ç„¡æ³•å°å…¥æ¨¡çµ„ {module_path}: {e}")
            return -1
    
    def measure_memory_usage(self, func, *args, **kwargs) -> tuple:
        """æ¸¬é‡å‡½æ•¸åŸ·è¡Œçš„è¨˜æ†¶é«”ä½¿ç”¨"""
        tracemalloc.start()
        
        # è¨˜éŒ„åˆå§‹è¨˜æ†¶é«”
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # åŸ·è¡Œå‡½æ•¸
        start_time = time.perf_counter()
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            print(f"âš ï¸ å‡½æ•¸åŸ·è¡Œå¤±æ•—: {e}")
            result = None
            success = False
        end_time = time.perf_counter()
        
        # è¨˜éŒ„æœ€çµ‚è¨˜æ†¶é«”
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_diff = final_memory - initial_memory
        
        # ç²å–è©³ç´°è¨˜æ†¶é«”çµ±è¨ˆ
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        execution_time = end_time - start_time
        
        return {
            'execution_time': execution_time,
            'memory_diff': memory_diff,
            'peak_memory': peak / 1024 / 1024,  # MB
            'success': success,
            'result': result
        }
    
    def benchmark_ui_modules(self) -> List[PerformanceMetrics]:
        """åŸºæº–æ¸¬è©¦ UI æ¨¡çµ„"""
        ui_modules = [
            ('src.ui.web_ui_production', 'æ¨è–¦ç‰ˆæœ¬'),
            ('src.ui.web_ui_production_legacy', 'éæ™‚ç‰ˆæœ¬'),
            ('src.ui.web_ui_redesigned', 'è¨­è¨ˆç‰ˆæœ¬'),
        ]
        
        results = []
        
        for module_path, description in ui_modules:
            print(f"ğŸ” æ¸¬è©¦ UI æ¨¡çµ„: {description}")
            
            # æ¸¬é‡å°å…¥æ™‚é–“
            import_time = self.measure_import_time(module_path)
            
            if import_time >= 0:
                # æ¸¬é‡åŸºæœ¬åŠŸèƒ½ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                try:
                    module = __import__(module_path, fromlist=[''])
                    
                    # æ¸¬é‡è¨˜æ†¶é«”ä½¿ç”¨
                    def dummy_function():
                        # æ¨¡æ“¬åŸºæœ¬æ“ä½œ
                        return hasattr(module, 'main')
                    
                    metrics = self.measure_memory_usage(dummy_function)
                    
                    result = PerformanceMetrics(
                        module_name=f"{module_path} ({description})",
                        execution_time=metrics['execution_time'],
                        memory_usage=metrics['memory_diff'],
                        cpu_usage=psutil.cpu_percent(interval=0.1),
                        import_time=import_time,
                        timestamp=datetime.now().isoformat()
                    )
                    
                    results.append(result)
                    
                except Exception as e:
                    print(f"âš ï¸ æ¸¬è©¦å¤±æ•— {module_path}: {e}")
        
        return results
    
    def benchmark_config_modules(self) -> List[PerformanceMetrics]:
        """åŸºæº–æ¸¬è©¦é…ç½®ç®¡ç†æ¨¡çµ„"""
        config_modules = [
            ('src.utils.config_manager', 'æ¨è–¦ç‰ˆæœ¬'),
            ('src.core.config_manager', 'éæ™‚ç‰ˆæœ¬'),
            ('src.core.config_validator', 'é©—è­‰å™¨'),
        ]
        
        results = []
        
        for module_path, description in config_modules:
            print(f"ğŸ” æ¸¬è©¦é…ç½®æ¨¡çµ„: {description}")
            
            import_time = self.measure_import_time(module_path)
            
            if import_time >= 0:
                try:
                    # æ¸¬é‡é…ç½®å‰µå»ºæ€§èƒ½
                    def test_config_creation():
                        module = __import__(module_path, fromlist=[''])
                        
                        # å˜—è©¦å‰µå»ºé…ç½®ç®¡ç†å™¨
                        if hasattr(module, 'create_default_config_manager'):
                            return module.create_default_config_manager()
                        elif hasattr(module, 'ConfigValidator'):
                            return module.ConfigValidator()
                        else:
                            return True
                    
                    metrics = self.measure_memory_usage(test_config_creation)
                    
                    result = PerformanceMetrics(
                        module_name=f"{module_path} ({description})",
                        execution_time=metrics['execution_time'],
                        memory_usage=metrics['memory_diff'],
                        cpu_usage=psutil.cpu_percent(interval=0.1),
                        import_time=import_time,
                        timestamp=datetime.now().isoformat()
                    )
                    
                    results.append(result)
                    
                except Exception as e:
                    print(f"âš ï¸ æ¸¬è©¦å¤±æ•— {module_path}: {e}")
        
        return results
    
    def benchmark_risk_modules(self) -> List[PerformanceMetrics]:
        """åŸºæº–æ¸¬è©¦é¢¨éšªç®¡ç†æ¨¡çµ„"""
        risk_modules = [
            ('src.risk_management.risk_manager_refactored', 'æ¨è–¦ç‰ˆæœ¬'),
            ('src.core.risk_control', 'éæ™‚ç‰ˆæœ¬'),
        ]
        
        results = []
        
        for module_path, description in risk_modules:
            print(f"ğŸ” æ¸¬è©¦é¢¨éšªç®¡ç†æ¨¡çµ„: {description}")
            
            import_time = self.measure_import_time(module_path)
            
            if import_time >= 0:
                try:
                    def test_risk_calculation():
                        module = __import__(module_path, fromlist=[''])
                        
                        # å˜—è©¦å‰µå»ºé¢¨éšªç®¡ç†å™¨
                        if hasattr(module, 'RiskManager'):
                            risk_manager = module.RiskManager()
                            # æ¨¡æ“¬åŸºæœ¬é¢¨éšªè¨ˆç®—
                            return True
                        return False
                    
                    metrics = self.measure_memory_usage(test_risk_calculation)
                    
                    result = PerformanceMetrics(
                        module_name=f"{module_path} ({description})",
                        execution_time=metrics['execution_time'],
                        memory_usage=metrics['memory_diff'],
                        cpu_usage=psutil.cpu_percent(interval=0.1),
                        import_time=import_time,
                        timestamp=datetime.now().isoformat()
                    )
                    
                    results.append(result)
                    
                except Exception as e:
                    print(f"âš ï¸ æ¸¬è©¦å¤±æ•— {module_path}: {e}")
        
        return results
    
    def benchmark_ib_modules(self) -> List[PerformanceMetrics]:
        """åŸºæº–æ¸¬è©¦ IB é©é…å™¨æ¨¡çµ„"""
        ib_modules = [
            ('src.execution.ib_adapter_refactored', 'æ¨è–¦ç‰ˆæœ¬'),
            ('src.execution.ib_adapter', 'éæ™‚ç‰ˆæœ¬'),
        ]
        
        results = []
        
        for module_path, description in ib_modules:
            print(f"ğŸ” æ¸¬è©¦ IB é©é…å™¨æ¨¡çµ„: {description}")
            
            import_time = self.measure_import_time(module_path)
            
            if import_time >= 0:
                try:
                    def test_ib_adapter():
                        module = __import__(module_path, fromlist=[''])
                        
                        # æª¢æŸ¥æ˜¯å¦æœ‰é©é…å™¨é¡åˆ¥
                        if hasattr(module, 'IBAdapterRefactored'):
                            return True
                        elif hasattr(module, 'IBAdapter'):
                            return True
                        return False
                    
                    metrics = self.measure_memory_usage(test_ib_adapter)
                    
                    result = PerformanceMetrics(
                        module_name=f"{module_path} ({description})",
                        execution_time=metrics['execution_time'],
                        memory_usage=metrics['memory_diff'],
                        cpu_usage=psutil.cpu_percent(interval=0.1),
                        import_time=import_time,
                        timestamp=datetime.now().isoformat()
                    )
                    
                    results.append(result)
                    
                except Exception as e:
                    print(f"âš ï¸ æ¸¬è©¦å¤±æ•— {module_path}: {e}")
                    # å³ä½¿æ¸¬è©¦å¤±æ•—ï¼Œä¹Ÿè¨˜éŒ„åŸºæœ¬çš„å°å…¥æ™‚é–“
                    result = PerformanceMetrics(
                        module_name=f"{module_path} ({description}) - æ¸¬è©¦å¤±æ•—",
                        execution_time=0.0,
                        memory_usage=0.0,
                        cpu_usage=psutil.cpu_percent(interval=0.1),
                        import_time=import_time,
                        timestamp=datetime.now().isoformat()
                    )
                    results.append(result)
            else:
                print(f"âš ï¸ å°å…¥å¤±æ•— {module_path}")

        return results
    
    def run_full_benchmark(self) -> List[PerformanceMetrics]:
        """é‹è¡Œå®Œæ•´çš„åŸºæº–æ¸¬è©¦"""
        all_results = []
        
        print("ğŸš€ é–‹å§‹æ€§èƒ½åŸºæº–æ¸¬è©¦...")
        print("=" * 50)
        
        # æ¸¬è©¦å„å€‹æ¨¡çµ„é¡åˆ¥
        all_results.extend(self.benchmark_ui_modules())
        all_results.extend(self.benchmark_config_modules())
        all_results.extend(self.benchmark_risk_modules())
        all_results.extend(self.benchmark_ib_modules())
        
        self.results = all_results
        return all_results
    
    def save_results(self, filename: str):
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        data = {
            'timestamp': datetime.now().isoformat(),
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
                'python_version': sys.version,
            },
            'results': [asdict(result) for result in self.results]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ çµæœå·²ä¿å­˜åˆ°: {filename}")
    
    def load_baseline(self, filename: str):
        """è¼‰å…¥åŸºæº–æ•¸æ“š"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.baseline_data = json.load(f)
            print(f"ğŸ“Š åŸºæº–æ•¸æ“šå·²è¼‰å…¥: {filename}")
        except FileNotFoundError:
            print(f"âš ï¸ åŸºæº–æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
    
    def compare_with_baseline(self) -> str:
        """èˆ‡åŸºæº–æ•¸æ“šæ¯”è¼ƒ"""
        if not self.baseline_data or not self.results:
            return "âŒ ç¼ºå°‘åŸºæº–æ•¸æ“šæˆ–ç•¶å‰çµæœ"
        
        report = []
        report.append("ğŸ“Š æ€§èƒ½æ¯”è¼ƒå ±å‘Š")
        report.append("=" * 50)
        
        # æŒ‰æ¨¡çµ„é¡å‹åˆ†çµ„æ¯”è¼ƒ
        current_by_name = {r.module_name: r for r in self.results}
        baseline_by_name = {r['module_name']: r for r in self.baseline_data['results']}
        
        for module_name, current in current_by_name.items():
            if module_name in baseline_by_name:
                baseline = baseline_by_name[module_name]
                
                report.append(f"\nğŸ” {module_name}:")
                
                # åŸ·è¡Œæ™‚é–“æ¯”è¼ƒ
                time_diff = current.execution_time - baseline['execution_time']
                time_percent = (time_diff / baseline['execution_time']) * 100 if baseline['execution_time'] > 0 else 0
                
                if time_percent > 10:
                    report.append(f"  âš ï¸ åŸ·è¡Œæ™‚é–“: {current.execution_time:.4f}s (â†‘{time_percent:.1f}%)")
                elif time_percent < -10:
                    report.append(f"  âœ… åŸ·è¡Œæ™‚é–“: {current.execution_time:.4f}s (â†“{abs(time_percent):.1f}%)")
                else:
                    report.append(f"  â¡ï¸ åŸ·è¡Œæ™‚é–“: {current.execution_time:.4f}s (~{time_percent:.1f}%)")
                
                # è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ
                memory_diff = current.memory_usage - baseline['memory_usage']
                if abs(memory_diff) > 1:  # è¶…é 1MB å·®ç•°
                    if memory_diff > 0:
                        report.append(f"  âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨: {current.memory_usage:.2f}MB (â†‘{memory_diff:.2f}MB)")
                    else:
                        report.append(f"  âœ… è¨˜æ†¶é«”ä½¿ç”¨: {current.memory_usage:.2f}MB (â†“{abs(memory_diff):.2f}MB)")
                else:
                    report.append(f"  â¡ï¸ è¨˜æ†¶é«”ä½¿ç”¨: {current.memory_usage:.2f}MB (~{memory_diff:.2f}MB)")
        
        return "\n".join(report)
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½å ±å‘Š"""
        if not self.results:
            return "âŒ æ²’æœ‰æ¸¬è©¦çµæœ"
        
        report = []
        report.append("ğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦å ±å‘Š")
        report.append("=" * 50)
        report.append(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æ¸¬è©¦æ¨¡çµ„æ•¸: {len(self.results)}")
        report.append("")
        
        # æŒ‰é¡åˆ¥åˆ†çµ„é¡¯ç¤º
        ui_results = [r for r in self.results if 'ui' in r.module_name.lower()]
        config_results = [r for r in self.results if 'config' in r.module_name.lower()]
        risk_results = [r for r in self.results if 'risk' in r.module_name.lower()]
        ib_results = [r for r in self.results if 'ib_adapter' in r.module_name.lower()]
        
        for category, results in [
            ("UI æ¨¡çµ„", ui_results),
            ("é…ç½®ç®¡ç†æ¨¡çµ„", config_results),
            ("é¢¨éšªç®¡ç†æ¨¡çµ„", risk_results),
            ("IB é©é…å™¨æ¨¡çµ„", ib_results)
        ]:
            if results:
                report.append(f"### {category}")
                for result in results:
                    report.append(f"  ğŸ“¦ {result.module_name}")
                    report.append(f"     å°å…¥æ™‚é–“: {result.import_time:.4f}s")
                    report.append(f"     åŸ·è¡Œæ™‚é–“: {result.execution_time:.4f}s")
                    report.append(f"     è¨˜æ†¶é«”ä½¿ç”¨: {result.memory_usage:.2f}MB")
                    report.append(f"     CPU ä½¿ç”¨: {result.cpu_usage:.1f}%")
                    report.append("")
        
        # æ€§èƒ½å»ºè­°
        report.append("ğŸ”§ æ€§èƒ½å»ºè­°:")
        
        # æ‰¾å‡ºæœ€æ…¢çš„æ¨¡çµ„
        slowest = max(self.results, key=lambda x: x.execution_time)
        report.append(f"  - æœ€æ…¢æ¨¡çµ„: {slowest.module_name} ({slowest.execution_time:.4f}s)")
        
        # æ‰¾å‡ºè¨˜æ†¶é«”ä½¿ç”¨æœ€å¤šçš„æ¨¡çµ„
        memory_heavy = max(self.results, key=lambda x: x.memory_usage)
        report.append(f"  - è¨˜æ†¶é«”ä½¿ç”¨æœ€å¤š: {memory_heavy.module_name} ({memory_heavy.memory_usage:.2f}MB)")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æ€§èƒ½åŸºæº–æ¸¬è©¦')
    parser.add_argument('--module', choices=['ui', 'config', 'risk', 'ib'], 
                       help='æ¸¬è©¦ç‰¹å®šæ¨¡çµ„é¡å‹')
    parser.add_argument('--output', default='performance_results.json', 
                       help='è¼¸å‡ºæª”æ¡ˆåç¨±')
    parser.add_argument('--baseline', help='åŸºæº–æ•¸æ“šæª”æ¡ˆ')
    parser.add_argument('--compare', action='store_true', help='èˆ‡åŸºæº–æ•¸æ“šæ¯”è¼ƒ')
    
    args = parser.parse_args()
    
    benchmark = PerformanceBenchmark()
    
    # è¼‰å…¥åŸºæº–æ•¸æ“šï¼ˆå¦‚æœæä¾›ï¼‰
    if args.baseline:
        benchmark.load_baseline(args.baseline)
    
    # é‹è¡Œæ¸¬è©¦
    if args.module:
        if args.module == 'ui':
            results = benchmark.benchmark_ui_modules()
        elif args.module == 'config':
            results = benchmark.benchmark_config_modules()
        elif args.module == 'risk':
            results = benchmark.benchmark_risk_modules()
        elif args.module == 'ib':
            results = benchmark.benchmark_ib_modules()
        # è¨­ç½®çµæœåˆ° benchmark å¯¦ä¾‹ä¸­
        benchmark.results = results
    else:
        results = benchmark.run_full_benchmark()
    
    # ç”Ÿæˆå ±å‘Š
    report = benchmark.generate_report()
    print(report)
    
    # æ¯”è¼ƒåŸºæº–ï¼ˆå¦‚æœè¦æ±‚ï¼‰
    if args.compare and args.baseline:
        comparison = benchmark.compare_with_baseline()
        print("\n" + comparison)
    
    # ä¿å­˜çµæœ
    benchmark.save_results(args.output)


if __name__ == '__main__':
    main()
