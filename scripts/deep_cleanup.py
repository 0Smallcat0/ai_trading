#!/usr/bin/env python3
"""
æ·±åº¦æ¸…ç†å·¥å…·

æ­¤è…³æœ¬é€²è¡Œæ›´ç©æ¥µçš„æª”æ¡ˆæ¸…ç†ï¼ŒåŒ…æ‹¬ï¼š
- æ¸…ç†æ‰€æœ‰ __pycache__ ç›®éŒ„
- ç§»é™¤ç©ºç›®éŒ„
- æ¸…ç†é‡è¤‡çš„é…ç½®æª”æ¡ˆ
- æ•´ç†æ—¥èªŒæª”æ¡ˆ

Usage:
    python scripts/deep_cleanup.py --analyze
    python scripts/deep_cleanup.py --execute
"""

import os
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import json


class DeepCleanup:
    """æ·±åº¦æ¸…ç†å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.backup_dir = self.project_root / "deep_cleanup_backups"
        self.cleanup_stats = {
            'pycache_dirs': 0,
            'empty_dirs': 0,
            'duplicate_configs': 0,
            'old_logs': 0,
            'temp_files': 0
        }
    
    def create_backup(self) -> str:
        """å‰µå»ºå‚™ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_id = f"deep_cleanup_{timestamp}"
        backup_path = self.backup_dir / backup_id
        backup_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¦ å‰µå»ºæ·±åº¦æ¸…ç†å‚™ä»½: {backup_path}")
        return backup_id
    
    def cleanup_pycache_dirs(self) -> int:
        """æ¸…ç†æ‰€æœ‰ __pycache__ ç›®éŒ„"""
        print("ğŸ—‘ï¸ æ¸…ç† __pycache__ ç›®éŒ„...")
        
        count = 0
        for pycache_dir in self.project_root.rglob('__pycache__'):
            if pycache_dir.is_dir():
                try:
                    shutil.rmtree(pycache_dir)
                    count += 1
                    print(f"  âŒ åˆªé™¤: {pycache_dir.relative_to(self.project_root)}")
                except Exception as e:
                    print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {pycache_dir}: {e}")
        
        return count
    
    def cleanup_pytest_cache(self) -> int:
        """æ¸…ç† pytest å¿«å–"""
        print("ğŸ—‘ï¸ æ¸…ç† pytest å¿«å–...")
        
        count = 0
        cache_patterns = ['.pytest_cache', '.coverage*', '.mypy_cache']
        
        for pattern in cache_patterns:
            for cache_item in self.project_root.rglob(pattern):
                if cache_item.exists():
                    try:
                        if cache_item.is_dir():
                            shutil.rmtree(cache_item)
                        else:
                            cache_item.unlink()
                        count += 1
                        print(f"  âŒ åˆªé™¤: {cache_item.relative_to(self.project_root)}")
                    except Exception as e:
                        print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {cache_item}: {e}")
        
        return count
    
    def cleanup_empty_directories(self) -> int:
        """æ¸…ç†ç©ºç›®éŒ„"""
        print("ğŸ—‘ï¸ æ¸…ç†ç©ºç›®éŒ„...")
        
        count = 0
        # å¤šæ¬¡æƒæï¼Œå› ç‚ºåˆªé™¤å­ç›®éŒ„å¯èƒ½æœƒè®“çˆ¶ç›®éŒ„è®Šç©º
        for _ in range(3):
            empty_dirs = []
            for dir_path in self.project_root.rglob('*'):
                if (dir_path.is_dir() and 
                    not self.should_skip_directory(dir_path) and
                    self.is_empty_directory(dir_path)):
                    empty_dirs.append(dir_path)
            
            for empty_dir in empty_dirs:
                try:
                    empty_dir.rmdir()
                    count += 1
                    print(f"  âŒ åˆªé™¤ç©ºç›®éŒ„: {empty_dir.relative_to(self.project_root)}")
                except Exception as e:
                    print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {empty_dir}: {e}")
        
        return count
    
    def cleanup_duplicate_configs(self) -> int:
        """æ¸…ç†é‡è¤‡çš„é…ç½®æª”æ¡ˆ"""
        print("ğŸ—‘ï¸ æ¸…ç†é‡è¤‡é…ç½®æª”æ¡ˆ...")
        
        count = 0
        
        # æª¢æŸ¥é‡è¤‡çš„ .env æª”æ¡ˆ
        env_files = list(self.project_root.rglob('.env*'))
        if len(env_files) > 1:
            # ä¿ç•™æ ¹ç›®éŒ„çš„ .env.example
            primary_env = None
            for env_file in env_files:
                if env_file.name == '.env.example' and env_file.parent == self.project_root:
                    primary_env = env_file
                    break
            
            if primary_env:
                for env_file in env_files:
                    if env_file != primary_env and 'config/environments' in str(env_file):
                        try:
                            env_file.unlink()
                            count += 1
                            print(f"  âŒ åˆªé™¤é‡è¤‡é…ç½®: {env_file.relative_to(self.project_root)}")
                        except Exception as e:
                            print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {env_file}: {e}")
        
        return count
    
    def cleanup_old_logs(self) -> int:
        """æ¸…ç†èˆŠæ—¥èªŒæª”æ¡ˆ"""
        print("ğŸ—‘ï¸ æ¸…ç†èˆŠæ—¥èªŒæª”æ¡ˆ...")
        
        count = 0
        logs_dir = self.project_root / 'logs'
        
        if logs_dir.exists():
            for log_file in logs_dir.rglob('*.log*'):
                if log_file.is_file():
                    try:
                        # æª¢æŸ¥æª”æ¡ˆå¤§å°ï¼Œå¦‚æœå¾ˆå°å¯èƒ½æ˜¯ç©ºçš„
                        if log_file.stat().st_size < 100:  # å°æ–¼100å­—ç¯€
                            log_file.unlink()
                            count += 1
                            print(f"  âŒ åˆªé™¤ç©ºæ—¥èªŒ: {log_file.relative_to(self.project_root)}")
                    except Exception as e:
                        print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {log_file}: {e}")
        
        return count
    
    def cleanup_temp_files(self) -> int:
        """æ¸…ç†è‡¨æ™‚æª”æ¡ˆ"""
        print("ğŸ—‘ï¸ æ¸…ç†è‡¨æ™‚æª”æ¡ˆ...")
        
        count = 0
        temp_patterns = [
            '*.tmp', '*.temp', '*.bak', '*.backup', 
            '*~', '*.swp', '*.swo', '*.orig'
        ]
        
        for pattern in temp_patterns:
            for temp_file in self.project_root.rglob(pattern):
                if temp_file.is_file() and not self.should_skip_file(temp_file):
                    try:
                        temp_file.unlink()
                        count += 1
                        print(f"  âŒ åˆªé™¤è‡¨æ™‚æª”æ¡ˆ: {temp_file.relative_to(self.project_root)}")
                    except Exception as e:
                        print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {temp_file}: {e}")
        
        return count
    
    def cleanup_backup_directories(self) -> int:
        """æ¸…ç†èˆŠçš„å‚™ä»½ç›®éŒ„"""
        print("ğŸ—‘ï¸ æ¸…ç†èˆŠå‚™ä»½ç›®éŒ„...")
        
        count = 0
        backup_patterns = [
            'file_cleanup_backups',
            'file_organization_backups'
        ]
        
        for pattern in backup_patterns:
            backup_dir = self.project_root / pattern
            if backup_dir.exists():
                # åªä¿ç•™æœ€æ–°çš„3å€‹å‚™ä»½
                backup_subdirs = sorted([d for d in backup_dir.iterdir() if d.is_dir()], 
                                      key=lambda x: x.stat().st_mtime, reverse=True)
                
                for old_backup in backup_subdirs[3:]:  # ä¿ç•™å‰3å€‹ï¼Œåˆªé™¤å…¶é¤˜
                    try:
                        shutil.rmtree(old_backup)
                        count += 1
                        print(f"  âŒ åˆªé™¤èˆŠå‚™ä»½: {old_backup.relative_to(self.project_root)}")
                    except Exception as e:
                        print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {old_backup}: {e}")
        
        return count
    
    def is_empty_directory(self, dir_path: Path) -> bool:
        """æª¢æŸ¥ç›®éŒ„æ˜¯å¦ç‚ºç©º"""
        try:
            return not any(dir_path.iterdir())
        except Exception:
            return False
    
    def should_skip_directory(self, dir_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è·³éç›®éŒ„"""
        skip_dirs = {
            '.git', '.venv', 'node_modules', 
            'src', 'tests', 'docs', 'scripts', 'config',
            'results', 'logs', 'data', 'models'
        }
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯é‡è¦ç›®éŒ„æˆ–å…¶å­ç›®éŒ„
        for part in dir_path.parts:
            if part in skip_dirs:
                return True
        
        return False
    
    def should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è·³éæª”æ¡ˆ"""
        # ä¸åˆªé™¤é‡è¦æª”æ¡ˆ
        important_files = {
            'pyproject.toml', 'poetry.lock', 'requirements.txt',
            'README.md', 'LICENSE', '.gitignore', '.env.example'
        }
        
        if file_path.name in important_files:
            return True
        
        # ä¸åˆªé™¤ src/, tests/, docs/ ä¸­çš„æª”æ¡ˆ
        relative_path = str(file_path.relative_to(self.project_root))
        protected_dirs = ['src/', 'tests/', 'docs/', 'config/']
        
        return any(relative_path.startswith(protected) for protected in protected_dirs)
    
    def analyze_cleanup_potential(self) -> Dict[str, Any]:
        """åˆ†ææ¸…ç†æ½›åŠ›"""
        print("ğŸ” åˆ†ææ¸…ç†æ½›åŠ›...")
        
        analysis = {
            'pycache_dirs': len(list(self.project_root.rglob('__pycache__'))),
            'pytest_cache': len(list(self.project_root.rglob('.pytest_cache'))),
            'empty_dirs': 0,
            'temp_files': 0,
            'old_logs': 0
        }
        
        # è¨ˆç®—ç©ºç›®éŒ„
        for dir_path in self.project_root.rglob('*'):
            if (dir_path.is_dir() and 
                not self.should_skip_directory(dir_path) and
                self.is_empty_directory(dir_path)):
                analysis['empty_dirs'] += 1
        
        # è¨ˆç®—è‡¨æ™‚æª”æ¡ˆ
        temp_patterns = ['*.tmp', '*.temp', '*.bak', '*.backup']
        for pattern in temp_patterns:
            analysis['temp_files'] += len(list(self.project_root.rglob(pattern)))
        
        # è¨ˆç®—å°æ—¥èªŒæª”æ¡ˆ
        logs_dir = self.project_root / 'logs'
        if logs_dir.exists():
            for log_file in logs_dir.rglob('*.log*'):
                if log_file.is_file() and log_file.stat().st_size < 100:
                    analysis['old_logs'] += 1
        
        return analysis
    
    def execute_deep_cleanup(self) -> Dict[str, int]:
        """åŸ·è¡Œæ·±åº¦æ¸…ç†"""
        print("ğŸš€ é–‹å§‹æ·±åº¦æ¸…ç†...")
        print("=" * 50)
        
        backup_id = self.create_backup()
        
        results = {}
        
        try:
            # 1. æ¸…ç† __pycache__ ç›®éŒ„
            results['pycache_dirs'] = self.cleanup_pycache_dirs()
            
            # 2. æ¸…ç† pytest å¿«å–
            results['pytest_cache'] = self.cleanup_pytest_cache()
            
            # 3. æ¸…ç†ç©ºç›®éŒ„
            results['empty_dirs'] = self.cleanup_empty_directories()
            
            # 4. æ¸…ç†é‡è¤‡é…ç½®
            results['duplicate_configs'] = self.cleanup_duplicate_configs()
            
            # 5. æ¸…ç†èˆŠæ—¥èªŒ
            results['old_logs'] = self.cleanup_old_logs()
            
            # 6. æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            results['temp_files'] = self.cleanup_temp_files()
            
            # 7. æ¸…ç†èˆŠå‚™ä»½
            results['old_backups'] = self.cleanup_backup_directories()
            
            print(f"\nâœ… æ·±åº¦æ¸…ç†å®Œæˆï¼å‚™ä»½ID: {backup_id}")
            
            # é¡¯ç¤ºçµ±è¨ˆ
            total_cleaned = sum(results.values())
            print(f"\nğŸ“Š æ¸…ç†çµ±è¨ˆ:")
            print(f"  __pycache__ ç›®éŒ„: {results['pycache_dirs']}")
            print(f"  pytest å¿«å–: {results['pytest_cache']}")
            print(f"  ç©ºç›®éŒ„: {results['empty_dirs']}")
            print(f"  é‡è¤‡é…ç½®: {results['duplicate_configs']}")
            print(f"  èˆŠæ—¥èªŒ: {results['old_logs']}")
            print(f"  è‡¨æ™‚æª”æ¡ˆ: {results['temp_files']}")
            print(f"  èˆŠå‚™ä»½: {results['old_backups']}")
            print(f"  ç¸½è¨ˆ: {total_cleaned} é …")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ·±åº¦æ¸…ç†éç¨‹ä¸­å‡ºéŒ¯: {e}")
            return {}
    
    def generate_report(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ¸…ç†å ±å‘Š"""
        report = []
        report.append("ğŸ“Š æ·±åº¦æ¸…ç†åˆ†æå ±å‘Š")
        report.append("=" * 30)
        report.append(f"åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        report.append("ğŸ” ç™¼ç¾çš„æ¸…ç†é …ç›®:")
        report.append(f"  __pycache__ ç›®éŒ„: {analysis['pycache_dirs']}")
        report.append(f"  pytest å¿«å–: {analysis['pytest_cache']}")
        report.append(f"  ç©ºç›®éŒ„: {analysis['empty_dirs']}")
        report.append(f"  è‡¨æ™‚æª”æ¡ˆ: {analysis['temp_files']}")
        report.append(f"  å°æ—¥èªŒæª”æ¡ˆ: {analysis['old_logs']}")
        report.append("")
        
        total_items = sum(analysis.values())
        if total_items > 0:
            report.append("ğŸ’¡ å»ºè­°åŸ·è¡Œæ·±åº¦æ¸…ç†")
            report.append(f"  é è¨ˆæ¸…ç†é …ç›®: {total_items}")
        else:
            report.append("âœ… å°ˆæ¡ˆå·²ç¶“å¾ˆä¹¾æ·¨")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æ·±åº¦æ¸…ç†å·¥å…·')
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææ¸…ç†æ½›åŠ›')
    parser.add_argument('--execute', action='store_true', help='åŸ·è¡Œæ·±åº¦æ¸…ç†')
    
    args = parser.parse_args()
    
    cleanup = DeepCleanup()
    
    if args.analyze:
        analysis = cleanup.analyze_cleanup_potential()
        report = cleanup.generate_report(analysis)
        print(report)
        
        # ä¿å­˜åˆ†æçµæœ
        with open('deep_cleanup_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ åˆ†æçµæœå·²ä¿å­˜åˆ°: deep_cleanup_analysis.json")
        
    elif args.execute:
        results = cleanup.execute_deep_cleanup()
        
        # ä¿å­˜æ¸…ç†çµæœ
        with open('deep_cleanup_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ æ¸…ç†çµæœå·²ä¿å­˜åˆ°: deep_cleanup_results.json")
        
    else:
        print("è«‹æŒ‡å®šæ“ä½œæ¨¡å¼:")
        print("  --analyze    åˆ†ææ¸…ç†æ½›åŠ›")
        print("  --execute    åŸ·è¡Œæ·±åº¦æ¸…ç†")


if __name__ == '__main__':
    main()
