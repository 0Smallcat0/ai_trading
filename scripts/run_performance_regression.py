#!/usr/bin/env python3
"""
æ•ˆèƒ½å›æ­¸æ¸¬è©¦è…³æœ¬

æ­¤è…³æœ¬åŸ·è¡Œæ•ˆèƒ½å›æ­¸æ¸¬è©¦ï¼Œæ¯”è¼ƒç•¶å‰æ•ˆèƒ½èˆ‡åŸºæº–æ•ˆèƒ½ï¼Œæª¢æ¸¬æ•ˆèƒ½é€€åŒ–ã€‚
"""

import os
import sys
import json
import argparse
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tests.performance.benchmark_config import (
    API_BENCHMARKS,
    REGRESSION_THRESHOLDS,
    CI_PERFORMANCE_CONFIG
)


class PerformanceRegressionTester:
    """æ•ˆèƒ½å›æ­¸æ¸¬è©¦å™¨"""
    
    def __init__(self, project_root: str = "."):
        """
        åˆå§‹åŒ–æ•ˆèƒ½å›æ­¸æ¸¬è©¦å™¨
        
        Args:
            project_root: é …ç›®æ ¹ç›®éŒ„è·¯å¾‘
        """
        self.project_root = Path(project_root).resolve()
        self.reports_dir = self.project_root / "tests" / "performance" / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        self.baseline_file = self.reports_dir / CI_PERFORMANCE_CONFIG["baseline_file"]
        self.results_file = self.reports_dir / CI_PERFORMANCE_CONFIG["results_file"]
        self.comparison_file = self.reports_dir / CI_PERFORMANCE_CONFIG["comparison_file"]
        
        self.regression_results = {
            "timestamp": datetime.now().isoformat(),
            "baseline_exists": self.baseline_file.exists(),
            "tests": {},
            "regressions": [],
            "summary": {
                "total_tests": 0,
                "regression_count": 0,
                "improvement_count": 0,
                "stable_count": 0
            }
        }
    
    def run_performance_tests(self) -> bool:
        """
        åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦
        
        Returns:
            bool: æ¸¬è©¦æ˜¯å¦æˆåŠŸåŸ·è¡Œ
        """
        print("ğŸš€ åŸ·è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦...")
        
        try:
            # åŸ·è¡Œ pytest-benchmark æ¸¬è©¦
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/performance/test_api_benchmarks.py",
                "--benchmark-only",
                "--benchmark-json=" + str(self.results_file),
                "--benchmark-sort=mean",
                "--benchmark-columns=min,max,mean,stddev,rounds,iterations",
                "-v"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é˜è¶…æ™‚
            )
            
            print(f"æ•ˆèƒ½æ¸¬è©¦å®Œæˆï¼Œè¿”å›ç¢¼: {result.returncode}")
            
            if result.returncode != 0:
                print(f"æ•ˆèƒ½æ¸¬è©¦å¤±æ•—: {result.stderr}")
                return False
            
            return True
            
        except subprocess.TimeoutExpired:
            print("âŒ æ•ˆèƒ½æ¸¬è©¦è¶…æ™‚")
            return False
        except Exception as e:
            print(f"âŒ æ•ˆèƒ½æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
            return False
    
    def load_baseline(self) -> Optional[Dict[str, Any]]:
        """
        è¼‰å…¥åŸºæº–æ•ˆèƒ½æ•¸æ“š
        
        Returns:
            Optional[Dict[str, Any]]: åŸºæº–æ•¸æ“šï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å› None
        """
        if not self.baseline_file.exists():
            print("âš ï¸ åŸºæº–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°‡å‰µå»ºæ–°çš„åŸºæº–")
            return None
        
        try:
            with open(self.baseline_file, 'r', encoding='utf-8') as f:
                baseline = json.load(f)
            print(f"âœ… è¼‰å…¥åŸºæº–æ–‡ä»¶: {self.baseline_file}")
            return baseline
        except Exception as e:
            print(f"âŒ è¼‰å…¥åŸºæº–æ–‡ä»¶å¤±æ•—: {e}")
            return None
    
    def load_current_results(self) -> Optional[Dict[str, Any]]:
        """
        è¼‰å…¥ç•¶å‰æ¸¬è©¦çµæœ
        
        Returns:
            Optional[Dict[str, Any]]: ç•¶å‰æ¸¬è©¦çµæœï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å› None
        """
        if not self.results_file.exists():
            print("âŒ æ¸¬è©¦çµæœæ–‡ä»¶ä¸å­˜åœ¨")
            return None
        
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            print(f"âœ… è¼‰å…¥æ¸¬è©¦çµæœ: {self.results_file}")
            return results
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¸¬è©¦çµæœå¤±æ•—: {e}")
            return None
    
    def compare_performance(
        self, 
        baseline: Dict[str, Any], 
        current: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ¯”è¼ƒæ•ˆèƒ½æ•¸æ“š
        
        Args:
            baseline: åŸºæº–æ•ˆèƒ½æ•¸æ“š
            current: ç•¶å‰æ•ˆèƒ½æ•¸æ“š
            
        Returns:
            Dict[str, Any]: æ¯”è¼ƒçµæœ
        """
        comparison = {
            "timestamp": datetime.now().isoformat(),
            "baseline_timestamp": baseline.get("datetime", "unknown"),
            "current_timestamp": current.get("datetime", "unknown"),
            "comparisons": {},
            "regressions": [],
            "improvements": [],
            "summary": {
                "total_benchmarks": 0,
                "regressions": 0,
                "improvements": 0,
                "stable": 0
            }
        }
        
        baseline_benchmarks = {
            bench["name"]: bench 
            for bench in baseline.get("benchmarks", [])
        }
        
        current_benchmarks = {
            bench["name"]: bench 
            for bench in current.get("benchmarks", [])
        }
        
        # æ¯”è¼ƒå…±åŒçš„åŸºæº–æ¸¬è©¦
        common_benchmarks = set(baseline_benchmarks.keys()) & set(current_benchmarks.keys())
        
        for bench_name in common_benchmarks:
            baseline_bench = baseline_benchmarks[bench_name]
            current_bench = current_benchmarks[bench_name]
            
            baseline_mean = baseline_bench["stats"]["mean"]
            current_mean = current_bench["stats"]["mean"]
            
            # è¨ˆç®—è®ŠåŒ–ç™¾åˆ†æ¯”
            change_percent = ((current_mean - baseline_mean) / baseline_mean) * 100
            
            bench_comparison = {
                "name": bench_name,
                "baseline_mean": baseline_mean,
                "current_mean": current_mean,
                "change_percent": change_percent,
                "change_ms": (current_mean - baseline_mean) * 1000,
                "status": "stable"
            }
            
            # åˆ¤æ–·æ˜¯å¦ç‚ºå›æ­¸
            if change_percent > REGRESSION_THRESHOLDS["response_time_increase_percent"]:
                bench_comparison["status"] = "regression"
                comparison["regressions"].append(bench_comparison)
                comparison["summary"]["regressions"] += 1
            elif change_percent < -10:  # æ”¹å–„è¶…é 10%
                bench_comparison["status"] = "improvement"
                comparison["improvements"].append(bench_comparison)
                comparison["summary"]["improvements"] += 1
            else:
                comparison["summary"]["stable"] += 1
            
            comparison["comparisons"][bench_name] = bench_comparison
            comparison["summary"]["total_benchmarks"] += 1
        
        return comparison
    
    def create_baseline(self, current_results: Dict[str, Any]) -> bool:
        """
        å‰µå»ºæ–°çš„æ•ˆèƒ½åŸºæº–
        
        Args:
            current_results: ç•¶å‰æ¸¬è©¦çµæœ
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå‰µå»ºåŸºæº–
        """
        try:
            with open(self.baseline_file, 'w', encoding='utf-8') as f:
                json.dump(current_results, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å‰µå»ºæ–°çš„æ•ˆèƒ½åŸºæº–: {self.baseline_file}")
            return True
            
        except Exception as e:
            print(f"âŒ å‰µå»ºåŸºæº–å¤±æ•—: {e}")
            return False
    
    def save_comparison_results(self, comparison: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ¯”è¼ƒçµæœ
        
        Args:
            comparison: æ¯”è¼ƒçµæœ
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        try:
            with open(self.comparison_file, 'w', encoding='utf-8') as f:
                json.dump(comparison, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ä¿å­˜æ¯”è¼ƒçµæœ: {self.comparison_file}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¯”è¼ƒçµæœå¤±æ•—: {e}")
            return False
    
    def generate_report(self, comparison: Optional[Dict[str, Any]] = None) -> None:
        """
        ç”Ÿæˆæ•ˆèƒ½å›æ­¸å ±å‘Š
        
        Args:
            comparison: æ¯”è¼ƒçµæœï¼Œå¦‚æœç‚º None å‰‡åªç”Ÿæˆç•¶å‰çµæœå ±å‘Š
        """
        print("\n" + "="*60)
        print("æ•ˆèƒ½å›æ­¸æ¸¬è©¦å ±å‘Š")
        print("="*60)
        
        if comparison is None:
            print("ğŸ“Š ç•¶å‰æ•ˆèƒ½æ¸¬è©¦çµæœï¼ˆç„¡åŸºæº–æ¯”è¼ƒï¼‰")
            current_results = self.load_current_results()
            if current_results and "benchmarks" in current_results:
                for bench in current_results["benchmarks"]:
                    mean_ms = bench["stats"]["mean"] * 1000
                    print(f"   {bench['name']}: {mean_ms:.2f}ms")
            return
        
        summary = comparison["summary"]
        print(f"ğŸ“Š æ•ˆèƒ½æ¯”è¼ƒæ‘˜è¦:")
        print(f"   ç¸½åŸºæº–æ¸¬è©¦æ•¸: {summary['total_benchmarks']}")
        print(f"   æ•ˆèƒ½å›æ­¸: {summary['regressions']}")
        print(f"   æ•ˆèƒ½æ”¹å–„: {summary['improvements']}")
        print(f"   æ•ˆèƒ½ç©©å®š: {summary['stable']}")
        
        if comparison["regressions"]:
            print(f"\nâŒ æ•ˆèƒ½å›æ­¸ ({len(comparison['regressions'])} å€‹):")
            for regression in comparison["regressions"]:
                print(f"   {regression['name']}: "
                      f"{regression['change_percent']:+.1f}% "
                      f"({regression['change_ms']:+.2f}ms)")
        
        if comparison["improvements"]:
            print(f"\nâœ… æ•ˆèƒ½æ”¹å–„ ({len(comparison['improvements'])} å€‹):")
            for improvement in comparison["improvements"]:
                print(f"   {improvement['name']}: "
                      f"{improvement['change_percent']:+.1f}% "
                      f"({improvement['change_ms']:+.2f}ms)")
        
        print("="*60)
    
    def run_regression_test(self, update_baseline: bool = False) -> bool:
        """
        åŸ·è¡Œå®Œæ•´çš„æ•ˆèƒ½å›æ­¸æ¸¬è©¦
        
        Args:
            update_baseline: æ˜¯å¦æ›´æ–°åŸºæº–
            
        Returns:
            bool: æ¸¬è©¦æ˜¯å¦é€šéï¼ˆç„¡å›æ­¸ï¼‰
        """
        print("ğŸ” é–‹å§‹æ•ˆèƒ½å›æ­¸æ¸¬è©¦...")
        
        # 1. åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦
        if not self.run_performance_tests():
            return False
        
        # 2. è¼‰å…¥ç•¶å‰çµæœ
        current_results = self.load_current_results()
        if current_results is None:
            return False
        
        # 3. è¼‰å…¥åŸºæº–
        baseline = self.load_baseline()
        
        if baseline is None or update_baseline:
            # å‰µå»ºæ–°åŸºæº–
            self.create_baseline(current_results)
            self.generate_report()
            return True
        
        # 4. æ¯”è¼ƒæ•ˆèƒ½
        comparison = self.compare_performance(baseline, current_results)
        
        # 5. ä¿å­˜æ¯”è¼ƒçµæœ
        self.save_comparison_results(comparison)
        
        # 6. ç”Ÿæˆå ±å‘Š
        self.generate_report(comparison)
        
        # 7. åˆ¤æ–·æ˜¯å¦æœ‰å›æ­¸
        has_regression = comparison["summary"]["regressions"] > 0
        
        if has_regression and CI_PERFORMANCE_CONFIG["fail_on_regression"]:
            print(f"\nâŒ æª¢æ¸¬åˆ° {comparison['summary']['regressions']} å€‹æ•ˆèƒ½å›æ­¸")
            return False
        
        print(f"\nâœ… æ•ˆèƒ½å›æ­¸æ¸¬è©¦é€šé")
        return True


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="æ•ˆèƒ½å›æ­¸æ¸¬è©¦è…³æœ¬")
    parser.add_argument(
        "--project-root",
        default=".",
        help="é …ç›®æ ¹ç›®éŒ„è·¯å¾‘"
    )
    parser.add_argument(
        "--update-baseline",
        action="store_true",
        help="æ›´æ–°æ•ˆèƒ½åŸºæº–"
    )
    parser.add_argument(
        "--no-fail-on-regression",
        action="store_true",
        help="å³ä½¿æœ‰å›æ­¸ä¹Ÿä¸å¤±æ•—"
    )
    
    args = parser.parse_args()
    
    # å‰µå»ºæ¸¬è©¦å™¨
    tester = PerformanceRegressionTester(args.project_root)
    
    # è¨­ç½®å¤±æ•—ç­–ç•¥
    if args.no_fail_on_regression:
        CI_PERFORMANCE_CONFIG["fail_on_regression"] = False
    
    try:
        # åŸ·è¡Œå›æ­¸æ¸¬è©¦
        success = tester.run_regression_test(args.update_baseline)
        
        # è¨­ç½®é€€å‡ºç¢¼
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\næ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
