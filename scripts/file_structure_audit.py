#!/usr/bin/env python3
"""
æª”æ¡ˆçµæ§‹å¯©æŸ¥å·¥å…·

æ­¤è…³æœ¬å®šæœŸå¯©æŸ¥å°ˆæ¡ˆæª”æ¡ˆçµæ§‹ï¼Œæª¢æ¸¬æ½›åœ¨çš„é‡è¤‡æª”æ¡ˆã€
éæ™‚æ¨¡çµ„å’Œçµæ§‹å•é¡Œï¼Œä¸¦ç”Ÿæˆå¯©æŸ¥å ±å‘Šã€‚

Usage:
    python scripts/file_structure_audit.py
    python scripts/file_structure_audit.py --output audit_report.json
    python scripts/file_structure_audit.py --schedule weekly
"""

import os
import json
import argparse
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Set, Tuple
from collections import defaultdict
import ast
import re


class FileStructureAuditor:
    """æª”æ¡ˆçµæ§‹å¯©æŸ¥å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.audit_results = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'issues': [],
            'recommendations': [],
            'metrics': {}
        }
        
    def calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆé›œæ¹Šå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    def find_duplicate_files(self) -> List[Dict[str, Any]]:
        """å°‹æ‰¾é‡è¤‡æª”æ¡ˆ"""
        print("ğŸ” æª¢æŸ¥é‡è¤‡æª”æ¡ˆ...")
        
        file_hashes = defaultdict(list)
        duplicates = []
        
        # æƒæ Python æª”æ¡ˆ
        for py_file in self.project_root.rglob('*.py'):
            # è·³éç‰¹å®šç›®éŒ„
            if any(part in str(py_file) for part in ['.git', '__pycache__', '.pytest_cache', '.venv']):
                continue
                
            file_hash = self.calculate_file_hash(py_file)
            if file_hash:
                file_hashes[file_hash].append(py_file)
        
        # æ‰¾å‡ºé‡è¤‡æª”æ¡ˆ
        for file_hash, files in file_hashes.items():
            if len(files) > 1:
                duplicates.append({
                    'hash': file_hash,
                    'files': [str(f.relative_to(self.project_root)) for f in files],
                    'size': files[0].stat().st_size if files[0].exists() else 0
                })
        
        return duplicates
    
    def find_similar_functions(self) -> List[Dict[str, Any]]:
        """å°‹æ‰¾ç›¸ä¼¼åŠŸèƒ½çš„æª”æ¡ˆ"""
        print("ğŸ” æª¢æŸ¥ç›¸ä¼¼åŠŸèƒ½...")
        
        similar_functions = []
        function_patterns = {
            'config_management': [
                r'def.*config.*manager',
                r'def.*parse.*args',
                r'def.*validate.*config',
                r'class.*Config.*Manager'
            ],
            'risk_management': [
                r'def.*risk.*manager',
                r'def.*calculate.*var',
                r'def.*stop.*loss',
                r'class.*Risk.*Manager'
            ],
            'data_processing': [
                r'def.*process.*data',
                r'def.*clean.*data',
                r'def.*transform.*data',
                r'class.*Data.*Processor'
            ],
            'ui_components': [
                r'def.*render.*page',
                r'def.*show.*sidebar',
                r'def.*display.*chart',
                r'st\.'  # Streamlit ç›¸é—œ
            ]
        }
        
        file_functions = defaultdict(list)
        
        # æƒæ Python æª”æ¡ˆä¸­çš„å‡½æ•¸
        for py_file in self.project_root.rglob('*.py'):
            if any(part in str(py_file) for part in ['.git', '__pycache__', '.pytest_cache', '.venv']):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                for category, patterns in function_patterns.items():
                    matches = []
                    for pattern in patterns:
                        matches.extend(re.findall(pattern, content, re.IGNORECASE))
                    
                    if matches:
                        file_functions[category].append({
                            'file': str(py_file.relative_to(self.project_root)),
                            'matches': len(matches),
                            'patterns': matches[:5]  # åªä¿ç•™å‰5å€‹åŒ¹é…
                        })
                        
            except Exception as e:
                continue
        
        # æ‰¾å‡ºæ¯å€‹é¡åˆ¥ä¸­æœ‰å¤šå€‹æª”æ¡ˆçš„æƒ…æ³
        for category, files in file_functions.items():
            if len(files) > 1:
                similar_functions.append({
                    'category': category,
                    'files': files,
                    'potential_duplication': len(files) > 2
                })
        
        return similar_functions
    
    def check_file_sizes(self) -> List[Dict[str, Any]]:
        """æª¢æŸ¥æª”æ¡ˆå¤§å°"""
        print("ğŸ” æª¢æŸ¥æª”æ¡ˆå¤§å°...")
        
        large_files = []
        max_lines = 300  # æ ¹æ“šå°ˆæ¡ˆæ¨™æº–
        
        for py_file in self.project_root.rglob('*.py'):
            if any(part in str(py_file) for part in ['.git', '__pycache__', '.pytest_cache', '.venv']):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    line_count = len(lines)
                    
                if line_count > max_lines:
                    large_files.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'lines': line_count,
                        'excess_lines': line_count - max_lines,
                        'severity': 'high' if line_count > max_lines * 2 else 'medium'
                    })
                    
            except Exception:
                continue
        
        return large_files
    
    def check_import_patterns(self) -> List[Dict[str, Any]]:
        """æª¢æŸ¥å°å…¥æ¨¡å¼"""
        print("ğŸ” æª¢æŸ¥å°å…¥æ¨¡å¼...")
        
        import_issues = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„éæ™‚å°å…¥æ¨¡å¼
        deprecated_patterns = [
            r'from\s+src\.ui\.web_ui_production_legacy\s+import',
            r'from\s+src\.core\.config_manager\s+import',
            r'from\s+src\.core\.risk_control\s+import',
            r'from\s+src\.execution\.ib_adapter\s+import\s+IBAdapter(?!\s*#.*å‘å¾Œç›¸å®¹)',
        ]
        
        for py_file in self.project_root.rglob('*.py'):
            if any(part in str(py_file) for part in ['.git', '__pycache__', '.pytest_cache', '.venv']):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')
                    
                for line_num, line in enumerate(lines, 1):
                    for pattern in deprecated_patterns:
                        if re.search(pattern, line):
                            import_issues.append({
                                'file': str(py_file.relative_to(self.project_root)),
                                'line': line_num,
                                'content': line.strip(),
                                'issue': 'deprecated_import',
                                'severity': 'high'
                            })
                            
            except Exception:
                continue
        
        return import_issues
    
    def check_naming_conventions(self) -> List[Dict[str, Any]]:
        """æª¢æŸ¥å‘½åè¦ç¯„"""
        print("ğŸ” æª¢æŸ¥å‘½åè¦ç¯„...")
        
        naming_issues = []
        
        # æª¢æŸ¥æª”æ¡ˆå‘½å
        for py_file in self.project_root.rglob('*.py'):
            if any(part in str(py_file) for part in ['.git', '__pycache__', '.pytest_cache', '.venv']):
                continue
                
            file_name = py_file.stem
            
            # æª¢æŸ¥æ˜¯å¦ç¬¦åˆ snake_case
            if not re.match(r'^[a-z][a-z0-9_]*$', file_name) and file_name != '__init__':
                naming_issues.append({
                    'file': str(py_file.relative_to(self.project_root)),
                    'issue': 'non_snake_case_filename',
                    'current_name': file_name,
                    'severity': 'medium'
                })
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç–‘çš„å‘½åæ¨¡å¼
            suspicious_patterns = [
                r'.*_legacy$',
                r'.*_old$',
                r'.*_backup$',
                r'.*_temp$',
                r'.*_test$',
                r'.*_debug$'
            ]
            
            for pattern in suspicious_patterns:
                if re.match(pattern, file_name):
                    naming_issues.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'issue': 'suspicious_naming',
                        'pattern': pattern,
                        'severity': 'medium'
                    })
        
        return naming_issues
    
    def generate_metrics(self) -> Dict[str, Any]:
        """ç”Ÿæˆå°ˆæ¡ˆæŒ‡æ¨™"""
        print("ğŸ“Š ç”Ÿæˆå°ˆæ¡ˆæŒ‡æ¨™...")
        
        metrics = {
            'total_python_files': 0,
            'total_lines_of_code': 0,
            'average_file_size': 0,
            'files_over_300_lines': 0,
            'directory_structure': {},
            'module_distribution': {}
        }
        
        total_lines = 0
        file_count = 0
        directory_counts = defaultdict(int)
        
        for py_file in self.project_root.rglob('*.py'):
            if any(part in str(py_file) for part in ['.git', '__pycache__', '.pytest_cache', '.venv']):
                continue
                
            file_count += 1
            
            # è¨ˆç®—ç›®éŒ„åˆ†ä½ˆ
            relative_path = py_file.relative_to(self.project_root)
            if len(relative_path.parts) > 1:
                directory_counts[relative_path.parts[0]] += 1
            
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    lines = len(f.readlines())
                    total_lines += lines
                    
                    if lines > 300:
                        metrics['files_over_300_lines'] += 1
                        
            except Exception:
                continue
        
        metrics['total_python_files'] = file_count
        metrics['total_lines_of_code'] = total_lines
        metrics['average_file_size'] = total_lines / file_count if file_count > 0 else 0
        metrics['directory_structure'] = dict(directory_counts)
        
        return metrics
    
    def run_audit(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´å¯©æŸ¥"""
        print("ğŸš€ é–‹å§‹æª”æ¡ˆçµæ§‹å¯©æŸ¥...")
        print("=" * 50)
        
        # åŸ·è¡Œå„é …æª¢æŸ¥
        duplicates = self.find_duplicate_files()
        similar_functions = self.find_similar_functions()
        large_files = self.check_file_sizes()
        import_issues = self.check_import_patterns()
        naming_issues = self.check_naming_conventions()
        metrics = self.generate_metrics()
        
        # å½™ç¸½çµæœ
        self.audit_results.update({
            'summary': {
                'duplicate_files': len(duplicates),
                'similar_function_groups': len(similar_functions),
                'large_files': len(large_files),
                'import_issues': len(import_issues),
                'naming_issues': len(naming_issues)
            },
            'issues': {
                'duplicates': duplicates,
                'similar_functions': similar_functions,
                'large_files': large_files,
                'import_issues': import_issues,
                'naming_issues': naming_issues
            },
            'metrics': metrics
        })
        
        # ç”Ÿæˆå»ºè­°
        self.generate_recommendations()
        
        return self.audit_results
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ç™¼ç¾çš„å•é¡Œç”Ÿæˆå»ºè­°
        if self.audit_results['summary']['duplicate_files'] > 0:
            recommendations.append({
                'priority': 'high',
                'category': 'duplication',
                'description': f"ç™¼ç¾ {self.audit_results['summary']['duplicate_files']} çµ„é‡è¤‡æª”æ¡ˆï¼Œå»ºè­°åˆä½µæˆ–ç§»é™¤"
            })
        
        if self.audit_results['summary']['large_files'] > 0:
            recommendations.append({
                'priority': 'medium',
                'category': 'file_size',
                'description': f"ç™¼ç¾ {self.audit_results['summary']['large_files']} å€‹å¤§å‹æª”æ¡ˆï¼ˆ>300è¡Œï¼‰ï¼Œå»ºè­°æ‹†åˆ†"
            })
        
        if self.audit_results['summary']['import_issues'] > 0:
            recommendations.append({
                'priority': 'high',
                'category': 'imports',
                'description': f"ç™¼ç¾ {self.audit_results['summary']['import_issues']} å€‹å°å…¥å•é¡Œï¼Œéœ€è¦ç«‹å³ä¿®å¾©"
            })
        
        # åŸºæ–¼æŒ‡æ¨™ç”Ÿæˆå»ºè­°
        if self.audit_results['metrics']['files_over_300_lines'] > 5:
            recommendations.append({
                'priority': 'medium',
                'category': 'code_organization',
                'description': "å»ºè­°å»ºç«‹æª”æ¡ˆå¤§å°ç›£æ§æ©Ÿåˆ¶ï¼Œé˜²æ­¢æª”æ¡ˆéå¤§"
            })
        
        self.audit_results['recommendations'] = recommendations
    
    def generate_report(self) -> str:
        """ç”Ÿæˆå¯©æŸ¥å ±å‘Š"""
        results = self.audit_results
        
        report = []
        report.append("ğŸ“Š æª”æ¡ˆçµæ§‹å¯©æŸ¥å ±å‘Š")
        report.append("=" * 50)
        report.append(f"å¯©æŸ¥æ™‚é–“: {results['timestamp'][:19]}")
        report.append("")
        
        # æ‘˜è¦
        report.append("ğŸ“‹ å¯©æŸ¥æ‘˜è¦")
        report.append("-" * 20)
        summary = results['summary']
        report.append(f"é‡è¤‡æª”æ¡ˆçµ„: {summary['duplicate_files']}")
        report.append(f"ç›¸ä¼¼åŠŸèƒ½çµ„: {summary['similar_function_groups']}")
        report.append(f"å¤§å‹æª”æ¡ˆ: {summary['large_files']}")
        report.append(f"å°å…¥å•é¡Œ: {summary['import_issues']}")
        report.append(f"å‘½åå•é¡Œ: {summary['naming_issues']}")
        report.append("")
        
        # å°ˆæ¡ˆæŒ‡æ¨™
        report.append("ğŸ“Š å°ˆæ¡ˆæŒ‡æ¨™")
        report.append("-" * 20)
        metrics = results['metrics']
        report.append(f"Python æª”æ¡ˆç¸½æ•¸: {metrics['total_python_files']}")
        report.append(f"ä»£ç¢¼ç¸½è¡Œæ•¸: {metrics['total_lines_of_code']:,}")
        report.append(f"å¹³å‡æª”æ¡ˆå¤§å°: {metrics['average_file_size']:.1f} è¡Œ")
        report.append(f"è¶…é300è¡Œçš„æª”æ¡ˆ: {metrics['files_over_300_lines']}")
        report.append("")
        
        # ç›®éŒ„åˆ†ä½ˆ
        if metrics['directory_structure']:
            report.append("ğŸ“ ç›®éŒ„åˆ†ä½ˆ")
            report.append("-" * 20)
            for directory, count in sorted(metrics['directory_structure'].items()):
                report.append(f"{directory}: {count} å€‹æª”æ¡ˆ")
            report.append("")
        
        # é‡è¦å•é¡Œ
        if summary['import_issues'] > 0 or summary['duplicate_files'] > 0:
            report.append("ğŸš¨ éœ€è¦ç«‹å³è™•ç†çš„å•é¡Œ")
            report.append("-" * 20)
            
            if summary['import_issues'] > 0:
                report.append(f"âŒ {summary['import_issues']} å€‹å°å…¥å•é¡Œ")
                for issue in results['issues']['import_issues'][:3]:
                    report.append(f"   - {issue['file']}:{issue['line']}")
            
            if summary['duplicate_files'] > 0:
                report.append(f"âŒ {summary['duplicate_files']} çµ„é‡è¤‡æª”æ¡ˆ")
                for dup in results['issues']['duplicates'][:3]:
                    report.append(f"   - {', '.join(dup['files'])}")
            
            report.append("")
        
        # å»ºè­°
        if results['recommendations']:
            report.append("ğŸ’¡ æ”¹é€²å»ºè­°")
            report.append("-" * 20)
            for rec in results['recommendations']:
                priority_icon = "ğŸ”´" if rec['priority'] == 'high' else "ğŸŸ¡"
                report.append(f"{priority_icon} {rec['description']}")
            report.append("")
        
        # å¥åº·è©•åˆ†
        total_issues = sum(summary.values())
        if total_issues == 0:
            health_score = "A+ (å„ªç§€)"
        elif total_issues <= 5:
            health_score = "B+ (è‰¯å¥½)"
        elif total_issues <= 10:
            health_score = "C+ (ä¸€èˆ¬)"
        else:
            health_score = "D (éœ€è¦æ”¹é€²)"
        
        report.append(f"ğŸ† æª”æ¡ˆçµæ§‹å¥åº·è©•åˆ†: {health_score}")
        
        return "\n".join(report)
    
    def save_results(self, filename: str):
        """ä¿å­˜å¯©æŸ¥çµæœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.audit_results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ å¯©æŸ¥çµæœå·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æª”æ¡ˆçµæ§‹å¯©æŸ¥å·¥å…·')
    parser.add_argument('--output', default='file_structure_audit.json', 
                       help='è¼¸å‡ºæª”æ¡ˆåç¨±')
    parser.add_argument('--report', action='store_true', 
                       help='é¡¯ç¤ºè©³ç´°å ±å‘Š')
    
    args = parser.parse_args()
    
    auditor = FileStructureAuditor()
    results = auditor.run_audit()
    
    # ç”Ÿæˆä¸¦é¡¯ç¤ºå ±å‘Š
    report = auditor.generate_report()
    print(report)
    
    # ä¿å­˜çµæœ
    auditor.save_results(args.output)
    
    # æ ¹æ“šå•é¡Œæ•¸é‡è¨­ç½®é€€å‡ºç¢¼
    total_issues = sum(results['summary'].values())
    if total_issues > 10:
        exit(1)  # å•é¡Œå¤ªå¤šï¼Œé€€å‡ºç¢¼ç‚º1
    else:
        exit(0)  # æ­£å¸¸é€€å‡º


if __name__ == '__main__':
    main()
