#!/usr/bin/env python3
"""
å…¨é¢æª”æ¡ˆæ•´ç†å·¥å…·

æ­¤è…³æœ¬å°æ•´å€‹å°ˆæ¡ˆé€²è¡Œå…¨é¢çš„æª”æ¡ˆæ•´ç†å’Œæ¸…ç†ï¼ŒåŒ…æ‹¬ï¼š
- æª”æ¡ˆåˆ†é¡æ•´ç†
- å†—é¤˜æª”æ¡ˆè­˜åˆ¥
- ç›®éŒ„çµæ§‹å„ªåŒ–
- è‡¨æ™‚æª”æ¡ˆæ¸…ç†

Usage:
    python scripts/comprehensive_file_organizer.py --analyze
    python scripts/comprehensive_file_organizer.py --execute
    python scripts/comprehensive_file_organizer.py --restore backup_id
"""

import os
import shutil
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Set, Tuple
import hashlib
import re
from collections import defaultdict


class ComprehensiveFileOrganizer:
    """å…¨é¢æª”æ¡ˆæ•´ç†å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.backup_dir = self.project_root / "file_organization_backups"
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'misplaced_files': [],
            'redundant_files': [],
            'temporary_files': [],
            'empty_files': [],
            'directory_issues': [],
            'recommendations': []
        }
        
        # å®šç¾©æ¨™æº–ç›®éŒ„çµæ§‹
        self.standard_structure = {
            'src/': 'åŸå§‹ç¢¼æª”æ¡ˆ',
            'tests/': 'æ¸¬è©¦æª”æ¡ˆ',
            'docs/': 'æ–‡æª”æª”æ¡ˆ',
            'scripts/': 'è…³æœ¬æª”æ¡ˆ',
            'config/': 'é…ç½®æª”æ¡ˆ',
            'data/': 'è³‡æ–™æª”æ¡ˆ',
            'logs/': 'æ—¥èªŒæª”æ¡ˆ',
            'examples/': 'ç¯„ä¾‹æª”æ¡ˆ',
            'models/': 'æ¨¡å‹æª”æ¡ˆ',
            'results/': 'çµæœæª”æ¡ˆ',
            'exports/': 'åŒ¯å‡ºæª”æ¡ˆ',
            'migrations/': 'è³‡æ–™åº«é·ç§»æª”æ¡ˆ',
            'k8s/': 'Kubernetes é…ç½®',
            'cache/': 'å¿«å–æª”æ¡ˆ',
            'mcp_crawler/': 'MCP çˆ¬èŸ²ç›¸é—œ'
        }
        
        # å®šç¾©éœ€è¦æ¸…ç†çš„æª”æ¡ˆæ¨¡å¼
        self.cleanup_patterns = {
            'temporary': [
                r'.*\.tmp$', r'.*\.temp$', r'.*\.bak$', r'.*\.backup$',
                r'.*~$', r'.*\.swp$', r'.*\.swo$', r'.*\.orig$'
            ],
            'cache': [
                r'__pycache__', r'.*\.pyc$', r'.*\.pyo$', r'.*\.pyd$',
                r'\.pytest_cache', r'\.coverage', r'\.mypy_cache'
            ],
            'logs': [
                r'.*\.log$', r'.*\.log\.\d+$'
            ],
            'results': [
                r'.*_results?\.json$', r'.*_report\.json$', r'.*_audit\.json$',
                r'.*performance.*\.json$', r'.*baseline.*\.json$'
            ]
        }
    
    def analyze_file_placement(self) -> List[Dict[str, Any]]:
        """åˆ†ææª”æ¡ˆæ”¾ç½®æ˜¯å¦æ­£ç¢º"""
        print("ğŸ” åˆ†ææª”æ¡ˆæ”¾ç½®...")
        
        misplaced = []
        
        # æª¢æŸ¥æ ¹ç›®éŒ„ä¸­çš„æª”æ¡ˆ
        for item in self.project_root.iterdir():
            if item.is_file() and item.name not in [
                'README.md', 'pyproject.toml', 'poetry.lock', 'Dockerfile',
                'docker-compose.yml', 'docker-compose.dev.yml', 'docker-compose.prod.yml',
                'requirements-production.txt', 'start.sh', 'start.bat',
                'auto_trading_project.code-workspace', 'security.key',
                'install_dependencies.py', 'launch_ui_comparison.py'
            ]:
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²ç§»å‹•åˆ°å…¶ä»–ç›®éŒ„
                suggested_location = self.suggest_file_location(item)
                if suggested_location:
                    misplaced.append({
                        'file': str(item.relative_to(self.project_root)),
                        'current_location': str(item.parent.relative_to(self.project_root)),
                        'suggested_location': suggested_location,
                        'reason': self.get_relocation_reason(item, suggested_location)
                    })
        
        return misplaced
    
    def suggest_file_location(self, file_path: Path) -> str:
        """å»ºè­°æª”æ¡ˆçš„æ­£ç¢ºä½ç½®"""
        file_name = file_path.name.lower()
        
        # JSON çµæœæª”æ¡ˆ
        if any(pattern in file_name for pattern in ['performance', 'audit', 'report', 'results', 'baseline']):
            if file_name.endswith('.json'):
                return 'results/'
        
        # æ—¥èªŒæª”æ¡ˆ
        if file_name.endswith('.log') or 'log' in file_name:
            return 'logs/'
        
        # è‡¨æ™‚æª”æ¡ˆ
        if any(file_name.endswith(ext) for ext in ['.tmp', '.temp', '.bak']):
            return 'DELETE'  # æ¨™è¨˜ç‚ºåˆªé™¤
        
        # æ¸¬è©¦æª”æ¡ˆ
        if file_name.startswith('test_') and file_name.endswith('.py'):
            return 'tests/'
        
        return None
    
    def get_relocation_reason(self, file_path: Path, suggested_location: str) -> str:
        """ç²å–é‡æ–°å®šä½çš„åŸå› """
        file_name = file_path.name.lower()
        
        if suggested_location == 'results/':
            return 'çµæœå’Œå ±å‘Šæª”æ¡ˆæ‡‰æ”¾åœ¨ results/ ç›®éŒ„'
        elif suggested_location == 'logs/':
            return 'æ—¥èªŒæª”æ¡ˆæ‡‰æ”¾åœ¨ logs/ ç›®éŒ„'
        elif suggested_location == 'tests/':
            return 'æ¸¬è©¦æª”æ¡ˆæ‡‰æ”¾åœ¨ tests/ ç›®éŒ„'
        elif suggested_location == 'DELETE':
            return 'è‡¨æ™‚æª”æ¡ˆï¼Œå»ºè­°åˆªé™¤'
        
        return 'ä¸ç¬¦åˆç›®éŒ„çµæ§‹è¦ç¯„'
    
    def find_redundant_files(self) -> List[Dict[str, Any]]:
        """å°‹æ‰¾å†—é¤˜æª”æ¡ˆ"""
        print("ğŸ” å°‹æ‰¾å†—é¤˜æª”æ¡ˆ...")
        
        redundant = []
        file_hashes = defaultdict(list)
        
        # è¨ˆç®—æ‰€æœ‰æª”æ¡ˆçš„é›œæ¹Šå€¼
        for py_file in self.project_root.rglob('*'):
            if py_file.is_file() and not self.should_skip_file(py_file):
                try:
                    file_hash = self.calculate_file_hash(py_file)
                    if file_hash:
                        file_hashes[file_hash].append(py_file)
                except Exception:
                    continue
        
        # æ‰¾å‡ºé‡è¤‡æª”æ¡ˆ
        for file_hash, files in file_hashes.items():
            if len(files) > 1:
                # æ’åºï¼Œä¿ç•™æœ€é‡è¦çš„æª”æ¡ˆ
                primary_file = self.select_primary_file(files)
                duplicates = [f for f in files if f != primary_file]
                
                redundant.append({
                    'primary_file': str(primary_file.relative_to(self.project_root)),
                    'duplicates': [str(f.relative_to(self.project_root)) for f in duplicates],
                    'size': primary_file.stat().st_size,
                    'hash': file_hash
                })
        
        return redundant
    
    def select_primary_file(self, files: List[Path]) -> Path:
        """é¸æ“‡ä¸»è¦æª”æ¡ˆï¼ˆä¿ç•™çš„æª”æ¡ˆï¼‰"""
        # å„ªå…ˆç´šï¼šsrc/ > tests/ > docs/ > scripts/ > å…¶ä»–
        priority_order = ['src/', 'tests/', 'docs/', 'scripts/']
        
        for priority in priority_order:
            for file in files:
                if str(file.relative_to(self.project_root)).startswith(priority):
                    return file
        
        # å¦‚æœæ²’æœ‰å„ªå…ˆç´šåŒ¹é…ï¼Œé¸æ“‡è·¯å¾‘æœ€çŸ­çš„
        return min(files, key=lambda f: len(str(f)))
    
    def find_temporary_files(self) -> List[Dict[str, Any]]:
        """å°‹æ‰¾è‡¨æ™‚æª”æ¡ˆ"""
        print("ğŸ” å°‹æ‰¾è‡¨æ™‚æª”æ¡ˆ...")
        
        temporary = []
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file() and not self.should_skip_file(file_path):
                file_name = file_path.name
                relative_path = str(file_path.relative_to(self.project_root))
                
                # æª¢æŸ¥å„ç¨®è‡¨æ™‚æª”æ¡ˆæ¨¡å¼
                for category, patterns in self.cleanup_patterns.items():
                    for pattern in patterns:
                        if re.match(pattern, file_name) or re.search(pattern, relative_path):
                            temporary.append({
                                'file': relative_path,
                                'category': category,
                                'pattern': pattern,
                                'size': file_path.stat().st_size,
                                'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                            })
                            break
        
        return temporary
    
    def find_empty_files(self) -> List[Dict[str, Any]]:
        """å°‹æ‰¾ç©ºç™½æª”æ¡ˆ"""
        print("ğŸ” å°‹æ‰¾ç©ºç™½æª”æ¡ˆ...")
        
        empty = []
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file() and not self.should_skip_file(file_path):
                try:
                    if file_path.stat().st_size == 0:
                        empty.append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'directory': str(file_path.parent.relative_to(self.project_root))
                        })
                except Exception:
                    continue
        
        return empty
    
    def analyze_directory_structure(self) -> List[Dict[str, Any]]:
        """åˆ†æç›®éŒ„çµæ§‹å•é¡Œ"""
        print("ğŸ” åˆ†æç›®éŒ„çµæ§‹...")
        
        issues = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ä¸ç¬¦åˆè¦ç¯„çš„ç›®éŒ„
        for item in self.project_root.iterdir():
            if item.is_dir() and item.name not in ['.git', '.venv', 'node_modules']:
                dir_name = item.name + '/'
                if dir_name not in self.standard_structure and not item.name.startswith('.'):
                    issues.append({
                        'directory': dir_name,
                        'issue': 'non_standard_directory',
                        'description': f'ç›®éŒ„ {dir_name} ä¸åœ¨æ¨™æº–çµæ§‹ä¸­',
                        'suggestion': self.suggest_directory_action(item)
                    })
        
        # æª¢æŸ¥ç©ºç›®éŒ„
        for dir_path in self.project_root.rglob('*'):
            if dir_path.is_dir() and not self.should_skip_directory(dir_path):
                try:
                    if not any(dir_path.iterdir()):
                        issues.append({
                            'directory': str(dir_path.relative_to(self.project_root)),
                            'issue': 'empty_directory',
                            'description': 'ç©ºç›®éŒ„',
                            'suggestion': 'consider_removal'
                        })
                except Exception:
                    continue
        
        return issues
    
    def suggest_directory_action(self, dir_path: Path) -> str:
        """å»ºè­°ç›®éŒ„æ“ä½œ"""
        dir_name = dir_path.name.lower()
        
        if 'backup' in dir_name or 'old' in dir_name:
            return 'move_to_archive_or_delete'
        elif 'temp' in dir_name or 'tmp' in dir_name:
            return 'delete'
        elif 'test' in dir_name:
            return 'merge_with_tests'
        else:
            return 'review_contents'
    
    def should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è·³éæª”æ¡ˆ"""
        skip_patterns = [
            '.git/', '__pycache__/', '.pytest_cache/', '.venv/',
            'node_modules/', '.mypy_cache/'
        ]
        
        relative_path = str(file_path.relative_to(self.project_root))
        return any(pattern in relative_path for pattern in skip_patterns)
    
    def should_skip_directory(self, dir_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è·³éç›®éŒ„"""
        skip_dirs = {'.git', '__pycache__', '.pytest_cache', '.venv', 'node_modules', '.mypy_cache'}
        return dir_path.name in skip_dirs
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆé›œæ¹Šå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    def run_analysis(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ é–‹å§‹å…¨é¢æª”æ¡ˆåˆ†æ...")
        print("=" * 50)
        
        self.analysis_results.update({
            'misplaced_files': self.analyze_file_placement(),
            'redundant_files': self.find_redundant_files(),
            'temporary_files': self.find_temporary_files(),
            'empty_files': self.find_empty_files(),
            'directory_issues': self.analyze_directory_structure()
        })
        
        # ç”Ÿæˆå»ºè­°
        self.generate_recommendations()
        
        return self.analysis_results
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ•´ç†å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼åˆ†æçµæœç”Ÿæˆå»ºè­°
        if self.analysis_results['misplaced_files']:
            recommendations.append({
                'priority': 'high',
                'category': 'file_placement',
                'description': f"ç§»å‹• {len(self.analysis_results['misplaced_files'])} å€‹éŒ¯æ”¾çš„æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½®"
            })
        
        if self.analysis_results['redundant_files']:
            recommendations.append({
                'priority': 'medium',
                'category': 'redundancy',
                'description': f"åˆªé™¤ {len(self.analysis_results['redundant_files'])} çµ„é‡è¤‡æª”æ¡ˆ"
            })
        
        if self.analysis_results['temporary_files']:
            temp_count = len(self.analysis_results['temporary_files'])
            recommendations.append({
                'priority': 'high',
                'category': 'cleanup',
                'description': f"æ¸…ç† {temp_count} å€‹è‡¨æ™‚æª”æ¡ˆå’Œå¿«å–æª”æ¡ˆ"
            })
        
        if self.analysis_results['empty_files']:
            recommendations.append({
                'priority': 'low',
                'category': 'empty_files',
                'description': f"è™•ç† {len(self.analysis_results['empty_files'])} å€‹ç©ºç™½æª”æ¡ˆ"
            })
        
        self.analysis_results['recommendations'] = recommendations
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        results = self.analysis_results
        
        report = []
        report.append("ğŸ“Š å…¨é¢æª”æ¡ˆæ•´ç†åˆ†æå ±å‘Š")
        report.append("=" * 50)
        report.append(f"åˆ†ææ™‚é–“: {results['timestamp'][:19]}")
        report.append("")
        
        # æ‘˜è¦çµ±è¨ˆ
        report.append("ğŸ“‹ å•é¡Œæ‘˜è¦")
        report.append("-" * 20)
        report.append(f"éŒ¯æ”¾æª”æ¡ˆ: {len(results['misplaced_files'])}")
        report.append(f"é‡è¤‡æª”æ¡ˆçµ„: {len(results['redundant_files'])}")
        report.append(f"è‡¨æ™‚æª”æ¡ˆ: {len(results['temporary_files'])}")
        report.append(f"ç©ºç™½æª”æ¡ˆ: {len(results['empty_files'])}")
        report.append(f"ç›®éŒ„å•é¡Œ: {len(results['directory_issues'])}")
        report.append("")
        
        # è©³ç´°å•é¡Œ
        if results['misplaced_files']:
            report.append("ğŸ“ éŒ¯æ”¾æª”æ¡ˆ")
            report.append("-" * 20)
            for item in results['misplaced_files'][:10]:  # åªé¡¯ç¤ºå‰10å€‹
                report.append(f"  ğŸ“„ {item['file']}")
                report.append(f"     å»ºè­°ç§»å‹•åˆ°: {item['suggested_location']}")
                report.append(f"     åŸå› : {item['reason']}")
            if len(results['misplaced_files']) > 10:
                report.append(f"  ... é‚„æœ‰ {len(results['misplaced_files']) - 10} å€‹æª”æ¡ˆ")
            report.append("")
        
        if results['temporary_files']:
            report.append("ğŸ—‘ï¸ è‡¨æ™‚æª”æ¡ˆ")
            report.append("-" * 20)
            temp_by_category = defaultdict(list)
            for item in results['temporary_files']:
                temp_by_category[item['category']].append(item)
            
            for category, files in temp_by_category.items():
                report.append(f"  {category.upper()}: {len(files)} å€‹æª”æ¡ˆ")
                for file in files[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    report.append(f"    - {file['file']}")
                if len(files) > 3:
                    report.append(f"    - ... é‚„æœ‰ {len(files) - 3} å€‹")
            report.append("")
        
        if results['redundant_files']:
            report.append("ğŸ”„ é‡è¤‡æª”æ¡ˆ")
            report.append("-" * 20)
            for item in results['redundant_files'][:5]:  # åªé¡¯ç¤ºå‰5çµ„
                report.append(f"  ä¸»æª”æ¡ˆ: {item['primary_file']}")
                report.append(f"  é‡è¤‡æª”æ¡ˆ: {', '.join(item['duplicates'])}")
            if len(results['redundant_files']) > 5:
                report.append(f"  ... é‚„æœ‰ {len(results['redundant_files']) - 5} çµ„")
            report.append("")
        
        # å»ºè­°è¡Œå‹•
        if results['recommendations']:
            report.append("ğŸ’¡ å»ºè­°è¡Œå‹•")
            report.append("-" * 20)
            for rec in results['recommendations']:
                priority_icon = "ğŸ”´" if rec['priority'] == 'high' else "ğŸŸ¡" if rec['priority'] == 'medium' else "ğŸŸ¢"
                report.append(f"{priority_icon} {rec['description']}")
            report.append("")
        
        # æ•´ç†è©•åˆ†
        total_issues = (len(results['misplaced_files']) + 
                       len(results['redundant_files']) + 
                       len(results['temporary_files']) + 
                       len(results['directory_issues']))
        
        if total_issues == 0:
            score = "A+ (å„ªç§€)"
        elif total_issues <= 10:
            score = "B+ (è‰¯å¥½)"
        elif total_issues <= 30:
            score = "C+ (ä¸€èˆ¬)"
        else:
            score = "D (éœ€è¦æ•´ç†)"
        
        report.append(f"ğŸ† æª”æ¡ˆæ•´ç†è©•åˆ†: {score}")
        
        return "\n".join(report)
    
    def save_analysis(self, filename: str):
        """ä¿å­˜åˆ†æçµæœ"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ åˆ†æçµæœå·²ä¿å­˜åˆ°: {filename}")


    def create_backup(self) -> str:
        """å‰µå»ºæ•´ç†å‰çš„å‚™ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_id = f"organization_{timestamp}"
        backup_path = self.backup_dir / backup_id
        backup_path.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ“¦ å‰µå»ºå‚™ä»½: {backup_path}")
        return backup_id

    def execute_organization(self, analysis_file: str) -> bool:
        """åŸ·è¡Œæª”æ¡ˆæ•´ç†"""
        print("ğŸš€ é–‹å§‹åŸ·è¡Œæª”æ¡ˆæ•´ç†...")

        # è¼‰å…¥åˆ†æçµæœ
        try:
            with open(analysis_file, 'r', encoding='utf-8') as f:
                analysis = json.load(f)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥åˆ†æçµæœ: {e}")
            return False

        # å‰µå»ºå‚™ä»½
        backup_id = self.create_backup()

        try:
            # 1. ç§»å‹•éŒ¯æ”¾çš„æª”æ¡ˆ
            self.move_misplaced_files(analysis['misplaced_files'])

            # 2. æ¸…ç†è‡¨æ™‚æª”æ¡ˆï¼ˆåªæ¸…ç†å®‰å…¨çš„é¡å‹ï¼‰
            self.cleanup_safe_temporary_files(analysis['temporary_files'])

            # 3. è™•ç†é‡è¤‡æª”æ¡ˆï¼ˆåªè™•ç†æ˜é¡¯çš„é‡è¤‡ï¼‰
            self.handle_safe_duplicates(analysis['redundant_files'])

            print(f"âœ… æª”æ¡ˆæ•´ç†å®Œæˆï¼å‚™ä»½ID: {backup_id}")
            return True

        except Exception as e:
            print(f"âŒ æ•´ç†éç¨‹ä¸­å‡ºéŒ¯: {e}")
            return False

    def move_misplaced_files(self, misplaced_files: List[Dict[str, Any]]):
        """ç§»å‹•éŒ¯æ”¾çš„æª”æ¡ˆ"""
        print("ğŸ“ ç§»å‹•éŒ¯æ”¾çš„æª”æ¡ˆ...")

        for item in misplaced_files:
            source = self.project_root / item['file']

            if item['suggested_location'] == 'DELETE':
                print(f"  ğŸ—‘ï¸ åˆªé™¤è‡¨æ™‚æª”æ¡ˆ: {item['file']}")
                if source.exists():
                    source.unlink()
                continue

            target_dir = self.project_root / item['suggested_location']
            target_dir.mkdir(parents=True, exist_ok=True)
            target = target_dir / source.name

            if source.exists() and not target.exists():
                shutil.move(str(source), str(target))
                print(f"  âœ… ç§»å‹•: {item['file']} â†’ {item['suggested_location']}")

    def cleanup_safe_temporary_files(self, temporary_files: List[Dict[str, Any]]):
        """æ¸…ç†å®‰å…¨çš„è‡¨æ™‚æª”æ¡ˆ"""
        print("ğŸ—‘ï¸ æ¸…ç†è‡¨æ™‚æª”æ¡ˆ...")

        safe_categories = ['cache', 'logs']  # åªæ¸…ç†å¿«å–å’ŒèˆŠæ—¥èªŒ

        for item in temporary_files:
            if item['category'] in safe_categories:
                file_path = self.project_root / item['file']

                # é¡å¤–å®‰å…¨æª¢æŸ¥
                if self.is_safe_to_delete(file_path):
                    try:
                        if file_path.exists():
                            if file_path.is_file():
                                file_path.unlink()
                            elif file_path.is_dir():
                                shutil.rmtree(file_path)
                            print(f"  ğŸ—‘ï¸ åˆªé™¤: {item['file']}")
                    except Exception as e:
                        print(f"  âš ï¸ ç„¡æ³•åˆªé™¤ {item['file']}: {e}")

    def is_safe_to_delete(self, file_path: Path) -> bool:
        """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å®‰å…¨åˆªé™¤"""
        relative_path = str(file_path.relative_to(self.project_root))

        # çµ•å°ä¸åˆªé™¤çš„è·¯å¾‘
        protected_patterns = [
            'src/', 'tests/', 'docs/', 'config/', 'scripts/',
            'pyproject.toml', 'poetry.lock', 'README.md'
        ]

        for pattern in protected_patterns:
            if relative_path.startswith(pattern) or pattern in relative_path:
                return False

        # åªåˆªé™¤æ˜ç¢ºçš„å¿«å–å’Œè‡¨æ™‚æª”æ¡ˆ
        safe_patterns = [
            '__pycache__', '.pytest_cache', '.coverage', '.mypy_cache',
            '.log', '.tmp', '.temp'
        ]

        return any(pattern in relative_path for pattern in safe_patterns)

    def handle_safe_duplicates(self, redundant_files: List[Dict[str, Any]]):
        """è™•ç†å®‰å…¨çš„é‡è¤‡æª”æ¡ˆ"""
        print("ğŸ”„ è™•ç†é‡è¤‡æª”æ¡ˆ...")

        for item in redundant_files:
            # åªè™•ç†æ˜é¡¯çš„å‚™ä»½æª”æ¡ˆ
            for duplicate in item['duplicates']:
                if 'backup' in duplicate.lower() or duplicate.endswith('.bak'):
                    dup_path = self.project_root / duplicate
                    if dup_path.exists():
                        dup_path.unlink()
                        print(f"  ğŸ—‘ï¸ åˆªé™¤é‡è¤‡æª”æ¡ˆ: {duplicate}")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å…¨é¢æª”æ¡ˆæ•´ç†å·¥å…·')
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææª”æ¡ˆçµæ§‹')
    parser.add_argument('--execute', help='åŸ·è¡Œæ•´ç†ï¼ˆéœ€è¦åˆ†ææª”æ¡ˆï¼‰')
    parser.add_argument('--output', default='file_organization_analysis.json',
                       help='è¼¸å‡ºæª”æ¡ˆåç¨±')

    args = parser.parse_args()

    organizer = ComprehensiveFileOrganizer()

    if args.analyze:
        results = organizer.run_analysis()

        # ç”Ÿæˆä¸¦é¡¯ç¤ºå ±å‘Š
        report = organizer.generate_report()
        print(report)

        # ä¿å­˜åˆ†æçµæœ
        organizer.save_analysis(args.output)

        # æ ¹æ“šå•é¡Œæ•¸é‡è¨­ç½®é€€å‡ºç¢¼
        total_issues = (len(results['misplaced_files']) +
                       len(results['redundant_files']) +
                       len(results['temporary_files']) +
                       len(results['directory_issues']))

        if total_issues > 30:
            exit(1)
        else:
            exit(0)

    elif args.execute:
        success = organizer.execute_organization(args.execute)
        exit(0 if success else 1)

    else:
        print("è«‹æŒ‡å®šæ“ä½œæ¨¡å¼:")
        print("  --analyze              åˆ†ææª”æ¡ˆçµæ§‹")
        print("  --execute <analysis>   åŸ·è¡Œæ•´ç†")


if __name__ == '__main__':
    main()
