#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""URL çˆ¬èŸ²åŠŸèƒ½æ¸¬è©¦è…³æœ¬

æ­¤è…³æœ¬ç”¨æ–¼æ¸¬è©¦åŸºæ–¼æä¾›çš„ URL æ¸…å–®å¯¦ç¾çš„çˆ¬èŸ²åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- URL é…ç½®ç®¡ç†å™¨æ¸¬è©¦
- å„ç¨®çˆ¬èŸ²å¯¦ç¾æ¸¬è©¦
- è³‡æ–™ä¾†æºå¯ç”¨æ€§æ¸¬è©¦
- å¯¦éš›è³‡æ–™çˆ¬å–æ¸¬è©¦

Usage:
    python scripts/test_url_crawlers.py
"""

import logging
import sys
from datetime import date, timedelta
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.data_source_urls import DataSourceURLManager, DataCategory
from src.data.web_crawler_implementations import TWSECrawler, YahooFinanceCrawler, CrawlerFactory
from src.data.data_integration_system import DataIntegrationSystem

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/url_crawler_test.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


def test_url_manager():
    """æ¸¬è©¦ URL é…ç½®ç®¡ç†å™¨"""
    logger.info("=== æ¸¬è©¦ URL é…ç½®ç®¡ç†å™¨ ===")
    
    try:
        manager = DataSourceURLManager()
        logger.info("âœ… URL ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æ¸¬è©¦ç²å–æ‰€æœ‰åˆ†é¡
        categories = manager.get_all_categories()
        logger.info("æ”¯æ´çš„è³‡æ–™åˆ†é¡: %s", categories)
        
        # æ¸¬è©¦æŒ‰åˆ†é¡ç²å–è³‡æ–™ä¾†æº
        for category in categories:
            sources = manager.get_sources_by_category(category)
            logger.info("åˆ†é¡ %s: %d å€‹è³‡æ–™ä¾†æº", category, len(sources))
            
            # é¡¯ç¤ºå‰3å€‹è³‡æ–™ä¾†æº
            for i, source in enumerate(sources[:3]):
                logger.info("  %d. %s - %s", i+1, source.name, source.description)
        
        # æ¸¬è©¦ç²å–å®˜æ–¹è³‡æ–™ä¾†æº
        official_sources = manager.get_official_sources()
        logger.info("å®˜æ–¹è³‡æ–™ä¾†æº: %d å€‹", len(official_sources))
        
        # æ¸¬è©¦ç²å–é«˜å¯é æ€§è³‡æ–™ä¾†æº
        reliable_sources = manager.get_high_reliability_sources(min_reliability=4)
        logger.info("é«˜å¯é æ€§è³‡æ–™ä¾†æº: %d å€‹", len(reliable_sources))
        
        # æ¸¬è©¦æœå°‹åŠŸèƒ½
        search_results = manager.search_sources("é›¶è‚¡")
        logger.info("æœå°‹ 'é›¶è‚¡' çµæœ: %d å€‹", len(search_results))
        
        return True
        
    except Exception as e:
        logger.error("âŒ URL ç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: %s", e)
        return False


def test_twse_crawler():
    """æ¸¬è©¦è­‰äº¤æ‰€çˆ¬èŸ²"""
    logger.info("=== æ¸¬è©¦è­‰äº¤æ‰€çˆ¬èŸ² ===")
    
    try:
        crawler = TWSECrawler(request_delay=1.0)  # æ¸¬è©¦æ™‚ä½¿ç”¨è¼ƒçŸ­å»¶é²
        logger.info("âœ… TWSE çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
        
        # æ¸¬è©¦æ—¥æœŸï¼ˆä½¿ç”¨æœ€è¿‘çš„äº¤æ˜“æ—¥ï¼‰
        today = date.today()
        test_date = today - timedelta(days=1)  # æ˜¨å¤©
        
        # å¦‚æœæ˜¯é€±æœ«ï¼Œå¾€å‰æ¨åˆ°é€±äº”
        while test_date.weekday() >= 5:
            test_date -= timedelta(days=1)
        
        test_date_str = test_date.strftime("%Y-%m-%d")
        logger.info("ä½¿ç”¨æ¸¬è©¦æ—¥æœŸ: %s", test_date_str)
        
        # æ¸¬è©¦é›¶è‚¡æˆäº¤è³‡æ–™çˆ¬å–
        logger.info("æ¸¬è©¦é›¶è‚¡æˆäº¤è³‡æ–™çˆ¬å–...")
        try:
            odd_lot_data = crawler.crawl_odd_lot_trading(test_date_str)
            logger.info("âœ… é›¶è‚¡æˆäº¤è³‡æ–™çˆ¬å–æˆåŠŸ: %d ç­†", len(odd_lot_data))
            
            if not odd_lot_data.empty:
                logger.info("è³‡æ–™æ¬„ä½: %s", list(odd_lot_data.columns))
                logger.info("å‰3ç­†è³‡æ–™:")
                logger.info("\n%s", odd_lot_data.head(3).to_string())
            
        except Exception as e:
            logger.warning("âš ï¸ é›¶è‚¡æˆäº¤è³‡æ–™çˆ¬å–å¤±æ•—: %s", e)
        
        # æ¸¬è©¦åŠ æ¬ŠæŒ‡æ•¸è³‡æ–™çˆ¬å–
        logger.info("æ¸¬è©¦åŠ æ¬ŠæŒ‡æ•¸è³‡æ–™çˆ¬å–...")
        try:
            index_data = crawler.crawl_taiex_index(test_date_str)
            logger.info("âœ… åŠ æ¬ŠæŒ‡æ•¸è³‡æ–™çˆ¬å–æˆåŠŸ: %d ç­†", len(index_data))
            
            if not index_data.empty:
                logger.info("æŒ‡æ•¸è³‡æ–™æ¬„ä½: %s", list(index_data.columns))
        
        except Exception as e:
            logger.warning("âš ï¸ åŠ æ¬ŠæŒ‡æ•¸è³‡æ–™çˆ¬å–å¤±æ•—: %s", e)
        
        # æ¸¬è©¦æœˆç‡Ÿæ”¶è³‡æ–™çˆ¬å–
        logger.info("æ¸¬è©¦æœˆç‡Ÿæ”¶è³‡æ–™çˆ¬å–...")
        try:
            current_year = today.year
            current_month = today.month - 1 if today.month > 1 else 12
            if current_month == 12:
                current_year -= 1
            
            revenue_data = crawler.crawl_monthly_revenue(current_year, current_month)
            logger.info("âœ… æœˆç‡Ÿæ”¶è³‡æ–™çˆ¬å–æˆåŠŸ: %d ç­†", len(revenue_data))
            
        except Exception as e:
            logger.warning("âš ï¸ æœˆç‡Ÿæ”¶è³‡æ–™çˆ¬å–å¤±æ•—: %s", e)
        
        return True
        
    except Exception as e:
        logger.error("âŒ TWSE çˆ¬èŸ²æ¸¬è©¦å¤±æ•—: %s", e)
        return False


def test_yahoo_crawler():
    """æ¸¬è©¦ Yahoo Finance çˆ¬èŸ²"""
    logger.info("=== æ¸¬è©¦ Yahoo Finance çˆ¬èŸ² ===")
    
    try:
        crawler = YahooFinanceCrawler()
        logger.info("âœ… Yahoo Finance çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
        
        # æ¸¬è©¦è‚¡ç¥¨æ­·å²è³‡æ–™çˆ¬å–
        logger.info("æ¸¬è©¦è‚¡ç¥¨æ­·å²è³‡æ–™çˆ¬å–...")
        
        end_date = date.today()
        start_date = end_date - timedelta(days=7)  # æœ€è¿‘ä¸€é€±
        
        try:
            stock_data = crawler.crawl_stock_history(
                "2330.TW",  # å°ç©é›»
                start_date.strftime("%Y-%m-%d"),
                end_date.strftime("%Y-%m-%d")
            )
            
            logger.info("âœ… è‚¡ç¥¨æ­·å²è³‡æ–™çˆ¬å–æˆåŠŸ: %d ç­†", len(stock_data))
            
            if not stock_data.empty:
                logger.info("è‚¡ç¥¨è³‡æ–™æ¬„ä½: %s", list(stock_data.columns))
                logger.info("æœ€æ–°è³‡æ–™:")
                logger.info("\n%s", stock_data.tail(3).to_string())
            
        except Exception as e:
            logger.warning("âš ï¸ è‚¡ç¥¨æ­·å²è³‡æ–™çˆ¬å–å¤±æ•—: %s", e)
        
        return True
        
    except Exception as e:
        logger.error("âŒ Yahoo Finance çˆ¬èŸ²æ¸¬è©¦å¤±æ•—: %s", e)
        return False


def test_crawler_factory():
    """æ¸¬è©¦çˆ¬èŸ²å·¥å» """
    logger.info("=== æ¸¬è©¦çˆ¬èŸ²å·¥å»  ===")
    
    try:
        # æ¸¬è©¦å‰µå»ºä¸åŒé¡å‹çš„çˆ¬èŸ²
        crawler_types = ['twse', 'yahoo', 'government']
        
        for crawler_type in crawler_types:
            try:
                crawler = CrawlerFactory.create_crawler(crawler_type, request_delay=1.0)
                logger.info("âœ… æˆåŠŸå‰µå»º %s çˆ¬èŸ²: %s", crawler_type, type(crawler).__name__)
                
            except Exception as e:
                logger.warning("âš ï¸ å‰µå»º %s çˆ¬èŸ²å¤±æ•—: %s", crawler_type, e)
        
        # æ¸¬è©¦ä¸æ”¯æ´çš„çˆ¬èŸ²é¡å‹
        try:
            CrawlerFactory.create_crawler('unsupported')
            logger.error("âŒ æ‡‰è©²æ‹‹å‡ºéŒ¯èª¤ä½†æ²’æœ‰")
            return False
        except ValueError:
            logger.info("âœ… æ­£ç¢ºè™•ç†ä¸æ”¯æ´çš„çˆ¬èŸ²é¡å‹")
        
        return True
        
    except Exception as e:
        logger.error("âŒ çˆ¬èŸ²å·¥å» æ¸¬è©¦å¤±æ•—: %s", e)
        return False


def test_integration_with_system():
    """æ¸¬è©¦èˆ‡è³‡æ–™æ•´åˆç³»çµ±çš„æ•´åˆ"""
    logger.info("=== æ¸¬è©¦èˆ‡è³‡æ–™æ•´åˆç³»çµ±çš„æ•´åˆ ===")
    
    try:
        system = DataIntegrationSystem()
        logger.info("âœ… è³‡æ–™æ•´åˆç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
        
        # æª¢æŸ¥ URL ç®¡ç†å™¨æ˜¯å¦æ­£ç¢ºæ•´åˆ
        if hasattr(system, 'url_manager'):
            logger.info("âœ… URL ç®¡ç†å™¨å·²æ•´åˆåˆ°ç³»çµ±ä¸­")
            
            # æ¸¬è©¦ç²å–æ”¯æ´çš„è³‡æ–™é¡å‹
            supported_types = system.get_supported_data_types()
            logger.info("ç³»çµ±æ”¯æ´çš„è³‡æ–™é¡å‹:")
            for category, types in supported_types.items():
                logger.info("  %s: %s", category, types)
        else:
            logger.warning("âš ï¸ URL ç®¡ç†å™¨æœªæ•´åˆåˆ°ç³»çµ±ä¸­")
        
        # æ¸¬è©¦åŸºæœ¬é¢ä¸‹è¼‰å™¨æ˜¯å¦å¯ç”¨
        if 'fundamental' in system.downloaders:
            logger.info("âœ… åŸºæœ¬é¢ä¸‹è¼‰å™¨å·²æ•´åˆ")
            
            fundamental_downloader = system.downloaders['fundamental']
            supported_fundamental_types = fundamental_downloader.get_supported_data_types()
            logger.info("åŸºæœ¬é¢æ”¯æ´çš„è³‡æ–™é¡å‹: %s", list(supported_fundamental_types.keys()))
        else:
            logger.warning("âš ï¸ åŸºæœ¬é¢ä¸‹è¼‰å™¨æœªæ•´åˆ")
        
        # æ¸¬è©¦ç³»çµ±ç‹€æ…‹
        status = system.get_system_status()
        logger.info("ç³»çµ±ç‹€æ…‹: å¯ç”¨ä¸‹è¼‰å™¨ %d å€‹", len(status['available_downloaders']))
        
        return True
        
    except Exception as e:
        logger.error("âŒ ç³»çµ±æ•´åˆæ¸¬è©¦å¤±æ•—: %s", e)
        return False


def test_url_accessibility():
    """æ¸¬è©¦ URL å¯è¨ªå•æ€§"""
    logger.info("=== æ¸¬è©¦ URL å¯è¨ªå•æ€§ ===")
    
    try:
        manager = DataSourceURLManager()
        
        # æ¸¬è©¦å¹¾å€‹é‡è¦çš„ URL
        test_urls = [
            manager.get_url_config("odd_lot_trading"),
            manager.get_url_config("benchmark_index"),
            manager.get_url_config("monthly_revenue")
        ]
        
        accessible_count = 0
        total_count = len([url for url in test_urls if url is not None])
        
        for url_config in test_urls:
            if url_config is None:
                continue
                
            try:
                import requests
                
                # ç°¡å–®çš„ HEAD è«‹æ±‚æ¸¬è©¦å¯è¨ªå•æ€§
                response = requests.head(
                    url_config.url, 
                    headers=url_config.headers,
                    timeout=10
                )
                
                if response.status_code < 400:
                    logger.info("âœ… %s å¯è¨ªå• (ç‹€æ…‹ç¢¼: %d)", url_config.name, response.status_code)
                    accessible_count += 1
                else:
                    logger.warning("âš ï¸ %s ä¸å¯è¨ªå• (ç‹€æ…‹ç¢¼: %d)", url_config.name, response.status_code)
                    
            except Exception as e:
                logger.warning("âš ï¸ %s è¨ªå•æ¸¬è©¦å¤±æ•—: %s", url_config.name, e)
        
        logger.info("URL å¯è¨ªå•æ€§æ¸¬è©¦å®Œæˆ: %d/%d å¯è¨ªå•", accessible_count, total_count)
        
        return accessible_count > 0
        
    except Exception as e:
        logger.error("âŒ URL å¯è¨ªå•æ€§æ¸¬è©¦å¤±æ•—: %s", e)
        return False


def main():
    """ä¸»å‡½æ•¸"""
    logger.info("é–‹å§‹ URL çˆ¬èŸ²åŠŸèƒ½æ¸¬è©¦")
    
    # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
    Path("logs").mkdir(exist_ok=True)
    
    test_results = []
    
    # åŸ·è¡Œå„é …æ¸¬è©¦
    test_results.append(("URL é…ç½®ç®¡ç†å™¨æ¸¬è©¦", test_url_manager()))
    test_results.append(("TWSE çˆ¬èŸ²æ¸¬è©¦", test_twse_crawler()))
    test_results.append(("Yahoo Finance çˆ¬èŸ²æ¸¬è©¦", test_yahoo_crawler()))
    test_results.append(("çˆ¬èŸ²å·¥å» æ¸¬è©¦", test_crawler_factory()))
    test_results.append(("ç³»çµ±æ•´åˆæ¸¬è©¦", test_integration_with_system()))
    test_results.append(("URL å¯è¨ªå•æ€§æ¸¬è©¦", test_url_accessibility()))
    
    # è¼¸å‡ºæ¸¬è©¦çµæœ
    logger.info("=== æ¸¬è©¦çµæœæ‘˜è¦ ===")
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        logger.info("%s: %s", test_name, status)
        if result:
            passed += 1
    
    logger.info("ç¸½è¨ˆ: %d/%d æ¸¬è©¦é€šé", passed, total)
    
    if passed >= total * 0.8:  # 80% é€šéç‡è¦–ç‚ºæˆåŠŸ
        logger.info("ğŸ‰ URL çˆ¬èŸ²åŠŸèƒ½æ¸¬è©¦åŸºæœ¬é€šéï¼")
        return 0
    else:
        logger.error("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
