# 資料處理模組說明

## 目錄

- [模組概述](#模組概述)
- [架構設計](#架構設計)
- [主要功能](#主要功能)
- [資料處理流程](#資料處理流程)
- [使用方式](#使用方式)
- [配置選項](#配置選項)
- [擴展指南](#擴展指南)
- [資料庫模組](#資料庫模組)
- [常見問題](#常見問題)

## 模組概述

資料處理模組負責對從各種來源獲取的原始數據進行清理、轉換、特徵工程和存儲，為後續的策略生成和模型訓練提供高質量的數據。該模組支持處理各種類型的數據，包括時間序列價格數據、財務報表數據、新聞文本數據等。

### 主要職責

- 數據清理：處理缺失值、異常值和重複數據
- 數據轉換：標準化、歸一化和格式轉換
- 特徵工程：生成技術指標、財務指標和情緒指標
- 數據存儲：將處理後的數據存儲到數據庫或文件系統
- 數據驗證：確保數據質量和完整性

## 架構設計

資料處理模組採用管道（Pipeline）設計模式，將數據處理過程分解為多個獨立的步驟，每個步驟負責特定的處理任務。這種設計使得系統可以靈活地組合不同的處理步驟，以適應不同的數據處理需求。

### 類圖

```
DataProcessor (主處理器)
├── DataCleaner (數據清理器)
│   ├── MissingValueHandler
│   ├── OutlierDetector
│   └── DuplicateRemover
├── DataTransformer (數據轉換器)
│   ├── Normalizer
│   ├── Standardizer
│   └── FormatConverter
├── FeatureEngineer (特徵工程器)
│   ├── TechnicalIndicatorGenerator
│   ├── FinancialIndicatorGenerator
│   └── SentimentIndicatorGenerator
└── DataValidator (數據驗證器)
    ├── CompletionChecker
    ├── ConsistencyChecker
    └── QualityChecker
```

### 核心類和接口

- **DataProcessor**：主處理器，協調各個處理步驟
- **DataPipeline**：數據處理管道，組合多個處理步驟
- **DataCleaner**：負責數據清理任務
- **DataTransformer**：負責數據轉換任務
- **FeatureEngineer**：負責特徵工程任務
- **DataValidator**：負責數據驗證任務
- **DataStorage**：負責數據存儲任務

## 主要功能

### 數據清理

```python
# 創建數據清理器
cleaner = DataCleaner()

# 處理缺失值
cleaned_data = cleaner.handle_missing_values(data, method="ffill")

# 檢測和處理異常值
cleaned_data = cleaner.handle_outliers(cleaned_data, method="winsorize", limits=[0.05, 0.05])

# 移除重複數據
cleaned_data = cleaner.remove_duplicates(cleaned_data)
```

### 數據轉換

```python
# 創建數據轉換器
transformer = DataTransformer()

# 標準化數據
standardized_data = transformer.standardize(data, columns=["close", "volume"])

# 歸一化數據
normalized_data = transformer.normalize(data, columns=["close", "volume"], method="minmax")

# 轉換數據格式
transformed_data = transformer.convert_format(data, target_format="dataframe")
```

### 特徵工程

```python
# 創建特徵工程器
feature_engineer = FeatureEngineer()

# 生成技術指標
data_with_indicators = feature_engineer.generate_technical_indicators(
    data,
    indicators=["SMA", "EMA", "RSI", "MACD", "Bollinger Bands"],
    parameters={"SMA": {"window": 20}, "RSI": {"window": 14}}
)

# 生成財務指標
data_with_financials = feature_engineer.generate_financial_indicators(
    data,
    financial_data,
    indicators=["PE", "PB", "ROE", "ROA", "Debt to Equity"]
)

# 生成情緒指標
data_with_sentiment = feature_engineer.generate_sentiment_indicators(
    data,
    news_data,
    indicators=["Sentiment Score", "News Volume", "Sentiment Change"]
)
```

### 數據驗證

```python
# 創建數據驗證器
validator = DataValidator()

# 檢查數據完整性
validation_result = validator.check_completeness(data, required_columns=["date", "open", "high", "low", "close", "volume"])

# 檢查數據一致性
validation_result = validator.check_consistency(data, rules=[
    "high >= low",
    "high >= open",
    "high >= close",
    "low <= open",
    "low <= close",
    "volume >= 0"
])

# 檢查數據質量
validation_result = validator.check_quality(data, metrics=["missing_ratio", "outlier_ratio", "duplicate_ratio"])
```

## 資料處理流程

資料處理模組的典型處理流程如下：

1. **數據加載**：從數據源加載原始數據
2. **數據清理**：處理缺失值、異常值和重複數據
3. **數據轉換**：標準化、歸一化和格式轉換
4. **特徵工程**：生成技術指標、財務指標和情緒指標
5. **數據驗證**：確保數據質量和完整性
6. **數據存儲**：將處理後的數據存儲到數據庫或文件系統

### 流程圖

```
+----------------+     +----------------+     +----------------+
|                |     |                |     |                |
|  數據加載       | --> |  數據清理       | --> |  數據轉換       |
|                |     |                |     |                |
+----------------+     +----------------+     +----------------+
                                                      |
                                                      v
+----------------+     +----------------+     +----------------+
|                |     |                |     |                |
|  數據存儲       | <-- |  數據驗證       | <-- |  特徵工程       |
|                |     |                |     |                |
+----------------+     +----------------+     +----------------+
```

## 使用方式

### 基本使用

```python
from src.data_processing.processor import DataProcessor
from src.data_processing.pipeline import DataPipeline
from src.data_processing.cleaner import DataCleaner
from src.data_processing.transformer import DataTransformer
from src.data_processing.feature_engineer import FeatureEngineer
from src.data_processing.validator import DataValidator

# 創建數據處理器
processor = DataProcessor()

# 處理數據
processed_data = processor.process(
    data,
    steps=[
        ("clean", {"methods": ["missing_values", "outliers", "duplicates"]}),
        ("transform", {"methods": ["standardize", "normalize"]}),
        ("feature_engineer", {"indicators": ["SMA", "RSI", "MACD"]}),
        ("validate", {"checks": ["completeness", "consistency", "quality"]})
    ]
)

# 保存處理後的數據
processor.save(processed_data, path="data/processed/stock_data.csv")
```

### 使用管道

```python
# 創建數據處理管道
pipeline = DataPipeline([
    ("cleaner", DataCleaner(methods=["missing_values", "outliers", "duplicates"])),
    ("transformer", DataTransformer(methods=["standardize", "normalize"])),
    ("feature_engineer", FeatureEngineer(indicators=["SMA", "RSI", "MACD"])),
    ("validator", DataValidator(checks=["completeness", "consistency", "quality"]))
])

# 使用管道處理數據
processed_data = pipeline.process(data)
```

### 批量處理

```python
# 批量處理多個股票的數據
symbols = ["2330.TW", "2317.TW", "2454.TW", "2412.TW", "2308.TW"]
processed_data = {}

for symbol in symbols:
    # 加載數據
    data = data_loader.load(symbol, start_date="2023-01-01", end_date="2023-12-31")

    # 處理數據
    processed_data[symbol] = processor.process(
        data,
        steps=[
            ("clean", {"methods": ["missing_values", "outliers", "duplicates"]}),
            ("transform", {"methods": ["standardize", "normalize"]}),
            ("feature_engineer", {"indicators": ["SMA", "RSI", "MACD"]}),
            ("validate", {"checks": ["completeness", "consistency", "quality"]})
        ]
    )

    # 保存處理後的數據
    processor.save(processed_data[symbol], path=f"data/processed/{symbol}.csv")
```

## 配置選項

資料處理模組的配置選項可以在 `.envs/.env.{env}` 文件中設置，或者通過代碼直接設置。

### 環境變數配置

```
# 數據清理配置
MISSING_VALUE_HANDLING_METHOD=ffill
OUTLIER_DETECTION_METHOD=zscore
OUTLIER_THRESHOLD=3.0
DUPLICATE_REMOVAL_ENABLED=true

# 數據轉換配置
STANDARDIZATION_METHOD=zscore
NORMALIZATION_METHOD=minmax
FORMAT_CONVERSION_TARGET=dataframe

# 特徵工程配置
TECHNICAL_INDICATORS=SMA,EMA,RSI,MACD,BB
FINANCIAL_INDICATORS=PE,PB,ROE,ROA,DE
SENTIMENT_INDICATORS=SentimentScore,NewsVolume,SentimentChange

# 數據驗證配置
COMPLETENESS_CHECK_ENABLED=true
CONSISTENCY_CHECK_ENABLED=true
QUALITY_CHECK_ENABLED=true
```

### 代碼配置

```python
from src.data_processing.config import DataProcessingConfig

# 創建配置對象
config = DataProcessingConfig()

# 設置數據清理配置
config.set_missing_value_handling_method("ffill")
config.set_outlier_detection_method("zscore")
config.set_outlier_threshold(3.0)
config.set_duplicate_removal_enabled(True)

# 設置數據轉換配置
config.set_standardization_method("zscore")
config.set_normalization_method("minmax")
config.set_format_conversion_target("dataframe")

# 設置特徵工程配置
config.set_technical_indicators(["SMA", "EMA", "RSI", "MACD", "BB"])
config.set_financial_indicators(["PE", "PB", "ROE", "ROA", "DE"])
config.set_sentiment_indicators(["SentimentScore", "NewsVolume", "SentimentChange"])

# 設置數據驗證配置
config.set_completeness_check_enabled(True)
config.set_consistency_check_enabled(True)
config.set_quality_check_enabled(True)

# 創建數據處理器
processor = DataProcessor(config)
```

## 擴展指南

### 添加新的處理步驟

要添加新的處理步驟，需要創建一個新的處理器類，實現必要的方法，然後將其集成到數據處理管道中。

```python
from src.data_processing.base import BaseProcessor

class NewProcessor(BaseProcessor):
    """新處理器"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # 初始化特定於此處理器的屬性

    def process(self, data, **kwargs):
        """處理數據"""
        # 實現數據處理邏輯
        return processed_data
```

然後，將新的處理器類添加到數據處理管道中：

```python
from src.data_processing.pipeline import DataPipeline
from src.data_processing.processors.new_processor import NewProcessor

# 創建數據處理管道
pipeline = DataPipeline([
    ("cleaner", DataCleaner()),
    ("transformer", DataTransformer()),
    ("new_processor", NewProcessor()),  # 添加新的處理器
    ("feature_engineer", FeatureEngineer()),
    ("validator", DataValidator())
])
```

### 添加新的特徵

要添加新的特徵，需要擴展 `FeatureEngineer` 類，添加新的特徵生成方法。

```python
from src.data_processing.feature_engineer import FeatureEngineer

class ExtendedFeatureEngineer(FeatureEngineer):
    """擴展的特徵工程器"""

    def generate_new_feature(self, data, **kwargs):
        """生成新特徵"""
        # 實現新特徵生成邏輯
        return data_with_new_feature
```

然後，使用擴展的特徵工程器：

```python
# 創建擴展的特徵工程器
feature_engineer = ExtendedFeatureEngineer()

# 生成新特徵
data_with_new_feature = feature_engineer.generate_new_feature(data, **kwargs)
```

## 常見問題

### 數據質量問題

**問題**：處理後的數據仍然存在質量問題
**解決方案**：
- 加強數據驗證機制，設置更嚴格的驗證規則
- 使用多種數據清理方法組合，提高清理效果
- 實現數據質量報告，及時發現和解決問題
- 考慮使用更高級的數據清理算法，如機器學習方法

### 性能問題

**問題**：數據處理過程耗時較長
**解決方案**：
- 優化數據處理算法，減少計算複雜度
- 使用並行處理，加速數據處理
- 實現增量處理，只處理新增或變更的數據
- 使用緩存機制，避免重複處理
- 考慮使用分佈式數據處理系統

### 擴展性問題

**問題**：難以添加新的數據處理功能
**解決方案**：
- 遵循模塊化設計原則，確保各個處理步驟的獨立性
- 使用插件機制，支持動態加載新的處理器
- 提供清晰的接口文檔，方便開發者理解和擴展
- 實現配置驅動的處理流程，減少硬編碼

## 資料庫模組

資料庫模組負責資料的儲存、驗證、備份和版本控制，確保系統能夠高效地存取和管理各種類型的資料。

### 模組概述

資料庫模組包含多個子模組，每個子模組負責特定的資料庫相關功能：

#### schema.py

資料庫結構定義，包含所有資料表的結構和關係。

**主要類別**：
- `MarketDataMixin`: 市場資料基礎欄位 Mixin
- `MarketTick`: 市場 Tick 資料表
- `MarketMinute`: 市場分鐘資料表
- `MarketDaily`: 市場日線資料表
- `Fundamental`: 基本面資料表
- `TechnicalIndicator`: 技術指標資料表
- `NewsSentiment`: 新聞情緒資料表
- `TradeRecord`: 交易記錄表
- `SystemLog`: 系統日誌表
- `DataShard`: 資料分片表
- `DataChecksum`: 資料校驗表
- `DatabaseVersion`: 資料庫版本表

**主要函數**：
- `init_db(engine)`: 初始化資料庫
- `create_data_shard()`: 創建資料分片

#### parquet_utils.py

Parquet/Arrow 格式工具模組，用於壓縮歷史資料並提高查詢效能。

**主要函數**：
- `query_to_dataframe()`: 將 SQLAlchemy 查詢結果轉換為 Pandas DataFrame
- `save_to_parquet()`: 將 DataFrame 儲存為 Parquet 格式
- `read_from_parquet()`: 從 Parquet 檔案讀取資料
- `create_market_data_shard()`: 創建市場資料分片並儲存為 Parquet 格式
- `load_from_shard()`: 從資料分片讀取資料

#### data_validation.py

資料驗證模組，用於確保資料的品質和完整性。

**主要類別**：
- `DataValidator`: 資料驗證器

**主要函數**：
- `validate_time_series_continuity()`: 驗證時間序列的連續性
- `check_missing_values()`: 檢查缺失值
- `detect_outliers()`: 檢測異常值
- `verify_data_integrity()`: 驗證資料完整性

#### data_backup.py

資料備份與還原模組，確保資料的安全性和可恢復性。

**主要類別**：
- `DatabaseBackup`: 資料庫備份類

**主要函數**：
- `create_backup()`: 創建資料庫備份
- `restore_backup()`: 還原資料庫備份
- `schedule_backup()`: 排程備份

#### data_versioning.py

資料版本控制模組，用於追蹤資料庫結構和資料的變更。

**主要類別**：
- `DataVersionManager`: 資料版本管理器

**主要函數**：
- `get_current_version()`: 獲取當前資料庫版本
- `update_version()`: 更新資料庫版本
- `compare_schema_with_models()`: 比較資料庫結構與模型定義
- `track_data_change()`: 追蹤資料變更
- `get_change_history()`: 獲取變更歷史

#### data_pipeline.py

資料管道模組，整合資料擷取、驗證、儲存和備份等功能。

**主要類別**：
- `DataPipeline`: 資料管道類

**主要函數**：
- `ingest_data()`: 擷取資料
- `validate_data_quality()`: 驗證資料品質
- `backup_data()`: 備份資料
- `restore_data()`: 還原資料
- `update_schema_version()`: 更新資料庫結構版本
- `create_data_shard()`: 創建資料分片
- `load_from_shard()`: 從資料分片讀取資料

### 資料庫結構

系統使用 SQLAlchemy ORM 框架，支援多種資料庫後端（SQLite、PostgreSQL、InfluxDB 等）。

主要資料表包括：

1. **市場資料表**：
   - `market_tick`: 市場 Tick 資料表，記錄每筆交易的詳細資訊
   - `market_minute`: 市場分鐘資料表，記錄分鐘級別的 K 線資料
   - `market_daily`: 市場日線資料表，記錄日線級別的 K 線資料

2. **基本面資料表**：
   - `fundamental`: 基本面資料表，記錄公司的基本面資料

3. **技術指標資料表**：
   - `technical_indicator`: 技術指標資料表，記錄各種技術指標的計算結果

4. **新聞情緒資料表**：
   - `news_sentiment`: 新聞情緒資料表，記錄與特定股票相關的新聞情緒分析結果

5. **交易記錄表**：
   - `trade_record`: 交易記錄表，記錄系統執行的所有交易

6. **系統管理表**：
   - `system_log`: 系統日誌表，記錄系統運行過程中的各種日誌
   - `data_shard`: 資料分片表，記錄資料分片的相關資訊
   - `data_checksum`: 資料校驗表，記錄資料的校驗碼
   - `database_version`: 資料庫版本表，記錄資料庫結構的版本資訊

### 資料完整性機制

系統實現了多層次的資料完整性機制：

1. **校驗碼**：每筆資料都會計算校驗碼，用於驗證資料的完整性
2. **資料分片**：大型資料集會分片儲存，提高查詢效能並便於管理
3. **資料備份**：定期備份資料庫，確保資料的安全性
4. **版本控制**：追蹤資料庫結構和重要資料的變更歷史

### 資料品質檢查

系統提供多種資料品質檢查機制：

1. **時間序列連續性檢查**：檢查時間序列資料是否有缺失的時間點
2. **缺失值檢查**：檢查資料中的缺失值並提供報告
3. **異常值檢測**：使用統計方法檢測異常值
4. **資料類型檢查**：確保資料類型符合預期

### 使用範例

#### 初始化資料管道

```python
from src.database.data_pipeline import DataPipeline

# 初始化資料管道
pipeline = DataPipeline()
```

#### 擷取資料

```python
import pandas as pd
from datetime import date
from src.database.schema import MarketDaily, MarketType

# 擷取資料
data = pd.DataFrame({
    "symbol": ["2330.TW", "2330.TW", "2330.TW"],
    "date": [date(2023, 1, 1), date(2023, 1, 2), date(2023, 1, 3)],
    "open": [500.0, 505.0, 510.0],
    "high": [510.0, 515.0, 520.0],
    "low": [495.0, 500.0, 505.0],
    "close": [505.0, 510.0, 515.0],
    "volume": [10000, 12000, 15000]
})

# 驗證資料品質
validation_result = pipeline.validate_data_quality(data)

# 儲存資料
if validation_result["is_valid"]:
    pipeline.ingest_data(data, table_class=MarketDaily, market_type=MarketType.STOCK)
```

#### 查詢資料

```python
from src.database.schema import MarketDaily
from sqlalchemy.orm import Session
from sqlalchemy import select

# 創建會話
with Session(engine) as session:
    # 查詢資料
    stmt = select(MarketDaily).where(
        MarketDaily.symbol == "2330.TW",
        MarketDaily.date >= date(2023, 1, 1),
        MarketDaily.date <= date(2023, 1, 3)
    )
    result = session.execute(stmt).scalars().all()

    # 轉換為 DataFrame
    from src.database.parquet_utils import query_to_dataframe
    df = query_to_dataframe(session, stmt)
```

#### 備份資料

```python
from src.database.data_backup import DatabaseBackup

# 創建備份
backup = DatabaseBackup()
backup_file = backup.create_backup()

# 還原備份
backup.restore_backup(backup_file)
```
