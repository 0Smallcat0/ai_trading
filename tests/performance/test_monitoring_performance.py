"""ç›£æ§ç³»çµ±æ•ˆèƒ½æ¸¬è©¦

æ­¤æ¨¡çµ„åŒ…å«ç›£æ§ç³»çµ±å„çµ„ä»¶çš„æ•ˆèƒ½åŸºæº–æ¸¬è©¦ã€‚
"""

import time
import threading
import psutil
import gc
from typing import Dict, List, Any
from unittest.mock import MagicMock, patch
import sys
import os
from pathlib import Path

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))


class PerformanceTestBase:
    """æ•ˆèƒ½æ¸¬è©¦åŸºç¤é¡åˆ¥"""
    
    def __init__(self):
        self.results: Dict[str, Any] = {}
        self.process = psutil.Process()
    
    def measure_time(self, func, *args, **kwargs):
        """æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“"""
        start_time = time.perf_counter()
        result = func(*args, **kwargs)
        end_time = time.perf_counter()
        execution_time = (end_time - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
        return result, execution_time
    
    def measure_memory(self, func, *args, **kwargs):
        """æ¸¬é‡å‡½æ•¸è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        gc.collect()  # å¼·åˆ¶åƒåœ¾å›æ”¶
        
        memory_before = self.process.memory_info().rss / 1024 / 1024  # MB
        result = func(*args, **kwargs)
        memory_after = self.process.memory_info().rss / 1024 / 1024  # MB
        
        memory_used = memory_after - memory_before
        return result, memory_used
    
    def run_concurrent_test(self, func, num_threads=10, iterations=100):
        """åŸ·è¡Œä¸¦ç™¼æ¸¬è©¦"""
        results = []
        threads = []
        
        def worker():
            for _ in range(iterations):
                start_time = time.perf_counter()
                try:
                    func()
                    end_time = time.perf_counter()
                    results.append((end_time - start_time) * 1000)
                except Exception as e:
                    results.append(f"Error: {e}")
        
        # å•Ÿå‹•ç·šç¨‹
        start_time = time.perf_counter()
        for _ in range(num_threads):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰ç·šç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        total_time = time.perf_counter() - start_time
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        valid_results = [r for r in results if isinstance(r, (int, float))]
        error_count = len(results) - len(valid_results)
        
        if valid_results:
            avg_time = sum(valid_results) / len(valid_results)
            max_time = max(valid_results)
            min_time = min(valid_results)
            throughput = len(valid_results) / total_time
        else:
            avg_time = max_time = min_time = throughput = 0
        
        return {
            'total_requests': len(results),
            'successful_requests': len(valid_results),
            'error_count': error_count,
            'total_time': total_time,
            'avg_response_time': avg_time,
            'max_response_time': max_time,
            'min_response_time': min_time,
            'throughput': throughput,
            'requests_per_second': throughput
        }


class PrometheusCollectorPerformanceTest(PerformanceTestBase):
    """Prometheus æ”¶é›†å™¨æ•ˆèƒ½æ¸¬è©¦"""
    
    def setUp(self):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        # æ¨¡æ“¬ä¾è³´
        with patch('src.monitoring.prometheus_collector.CollectorRegistry'), \
             patch('src.monitoring.prometheus_collector.generate_latest'), \
             patch('src.monitoring.prometheus_collector.SystemMetricsCollector'), \
             patch('src.monitoring.prometheus_collector.TradingMetricsCollector'), \
             patch('src.monitoring.prometheus_collector.APIMetricsCollector'), \
             patch('src.monitoring.prometheus_collector.BusinessMetricsCollector'):
            
            from src.monitoring.prometheus_collector import PrometheusCollector
            self.collector = PrometheusCollector(collection_interval=1)
    
    def test_initialization_performance(self):
        """æ¸¬è©¦åˆå§‹åŒ–æ•ˆèƒ½"""
        def init_collector():
            with patch('src.monitoring.prometheus_collector.CollectorRegistry'), \
                 patch('src.monitoring.prometheus_collector.SystemMetricsCollector'), \
                 patch('src.monitoring.prometheus_collector.TradingMetricsCollector'), \
                 patch('src.monitoring.prometheus_collector.APIMetricsCollector'), \
                 patch('src.monitoring.prometheus_collector.BusinessMetricsCollector'):
                
                from src.monitoring.prometheus_collector import PrometheusCollector
                return PrometheusCollector(collection_interval=1)
        
        # æ¸¬é‡åˆå§‹åŒ–æ™‚é–“
        _, init_time = self.measure_time(init_collector)
        
        # æ¸¬é‡åˆå§‹åŒ–è¨˜æ†¶é«”ä½¿ç”¨
        _, memory_used = self.measure_memory(init_collector)
        
        return {
            'initialization_time_ms': init_time,
            'memory_used_mb': memory_used,
            'target_init_time_ms': 100,  # ç›®æ¨™ <100ms
            'target_memory_mb': 50,      # ç›®æ¨™ <50MB
            'init_time_ok': init_time < 100,
            'memory_ok': memory_used < 50
        }
    
    def test_metrics_collection_performance(self):
        """æ¸¬è©¦æŒ‡æ¨™æ”¶é›†æ•ˆèƒ½"""
        self.setUp()
        
        # æ¨¡æ“¬æŒ‡æ¨™æ”¶é›†
        def collect_metrics():
            return self.collector.get_metrics()
        
        # æ¸¬é‡å–®æ¬¡æ”¶é›†æ™‚é–“
        _, collection_time = self.measure_time(collect_metrics)
        
        # æ¸¬é‡ä¸¦ç™¼æ”¶é›†æ•ˆèƒ½
        concurrent_results = self.run_concurrent_test(
            collect_metrics, 
            num_threads=5, 
            iterations=20
        )
        
        return {
            'single_collection_time_ms': collection_time,
            'concurrent_results': concurrent_results,
            'target_collection_time_ms': 50,  # ç›®æ¨™ <50ms
            'target_throughput_rps': 100,     # ç›®æ¨™ >100 RPS
            'collection_time_ok': collection_time < 50,
            'throughput_ok': concurrent_results['requests_per_second'] > 100
        }


class GrafanaConfigPerformanceTest(PerformanceTestBase):
    """Grafana é…ç½®ç®¡ç†å™¨æ•ˆèƒ½æ¸¬è©¦"""
    
    def setUp(self):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        with patch('src.monitoring.grafana_config.GrafanaFace'), \
             patch('src.monitoring.grafana_config.DashboardManager'), \
             patch('src.monitoring.grafana_config.DatasourceManager'), \
             patch('src.monitoring.grafana_config.TemplateGenerator'):
            
            from src.monitoring.grafana_config import GrafanaConfigManager
            self.manager = GrafanaConfigManager()
    
    def test_dashboard_creation_performance(self):
        """æ¸¬è©¦å„€è¡¨æ¿å‰µå»ºæ•ˆèƒ½"""
        self.setUp()
        
        def create_dashboard():
            panel_configs = [
                {
                    "title": f"Test Panel {i}",
                    "type": "stat",
                    "metric": f"test_metric_{i}",
                    "grid_pos": {"h": 8, "w": 6, "x": 0, "y": i*8}
                }
                for i in range(10)
            ]
            
            return self.manager.create_custom_dashboard(
                "Performance Test Dashboard",
                ["test", "performance"],
                panel_configs
            )
        
        # æ¸¬é‡å‰µå»ºæ™‚é–“
        _, creation_time = self.measure_time(create_dashboard)
        
        # æ¸¬é‡è¨˜æ†¶é«”ä½¿ç”¨
        _, memory_used = self.measure_memory(create_dashboard)
        
        return {
            'dashboard_creation_time_ms': creation_time,
            'memory_used_mb': memory_used,
            'target_creation_time_ms': 200,  # ç›®æ¨™ <200ms
            'target_memory_mb': 100,         # ç›®æ¨™ <100MB
            'creation_time_ok': creation_time < 200,
            'memory_ok': memory_used < 100
        }
    
    def test_configuration_export_performance(self):
        """æ¸¬è©¦é…ç½®åŒ¯å‡ºæ•ˆèƒ½"""
        self.setUp()
        
        def export_config():
            return self.manager.export_configuration()
        
        # æ¸¬é‡åŒ¯å‡ºæ™‚é–“
        _, export_time = self.measure_time(export_config)
        
        return {
            'export_time_ms': export_time,
            'target_export_time_ms': 500,  # ç›®æ¨™ <500ms
            'export_time_ok': export_time < 500
        }


class MonitorSystemPerformanceTest(PerformanceTestBase):
    """ç›£æ§ç³»çµ±æ•ˆèƒ½æ¸¬è©¦"""
    
    def setUp(self):
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        # æ¨¡æ“¬æ‰€æœ‰ä¾è³´
        with patch('src.monitoring.monitor_system.AlertHandler'), \
             patch('src.monitoring.monitor_system.SystemMonitor'), \
             patch('src.monitoring.monitor_system.ThresholdChecker'), \
             patch('src.monitoring.monitor_system.prometheus_exporter'), \
             patch('src.monitoring.monitor_system.alert_manager'), \
             patch('src.monitoring.monitor_system.get_logger'):
            
            from src.monitoring.monitor_system import MonitorSystem
            self.system = MonitorSystem()
    
    def test_system_startup_performance(self):
        """æ¸¬è©¦ç³»çµ±å•Ÿå‹•æ•ˆèƒ½"""
        def startup_system():
            self.setUp()
            return self.system.start()
        
        # æ¸¬é‡å•Ÿå‹•æ™‚é–“
        _, startup_time = self.measure_time(startup_system)
        
        # æ¸¬é‡å•Ÿå‹•è¨˜æ†¶é«”ä½¿ç”¨
        _, memory_used = self.measure_memory(startup_system)
        
        return {
            'startup_time_ms': startup_time,
            'memory_used_mb': memory_used,
            'target_startup_time_ms': 1000,  # ç›®æ¨™ <1000ms
            'target_memory_mb': 200,         # ç›®æ¨™ <200MB
            'startup_time_ok': startup_time < 1000,
            'memory_ok': memory_used < 200
        }
    
    def test_status_check_performance(self):
        """æ¸¬è©¦ç‹€æ…‹æª¢æŸ¥æ•ˆèƒ½"""
        self.setUp()
        
        def check_status():
            return self.system.get_status()
        
        # æ¸¬é‡ç‹€æ…‹æª¢æŸ¥æ™‚é–“
        _, check_time = self.measure_time(check_status)
        
        # æ¸¬é‡ä¸¦ç™¼ç‹€æ…‹æª¢æŸ¥
        concurrent_results = self.run_concurrent_test(
            check_status,
            num_threads=10,
            iterations=50
        )
        
        return {
            'status_check_time_ms': check_time,
            'concurrent_results': concurrent_results,
            'target_check_time_ms': 100,  # ç›®æ¨™ <100ms
            'target_throughput_rps': 50,  # ç›®æ¨™ >50 RPS
            'check_time_ok': check_time < 100,
            'throughput_ok': concurrent_results['requests_per_second'] > 50
        }


def run_performance_tests():
    """åŸ·è¡Œæ‰€æœ‰æ•ˆèƒ½æ¸¬è©¦"""
    print("ğŸš€ é–‹å§‹ç›£æ§ç³»çµ±æ•ˆèƒ½æ¸¬è©¦\n")
    
    # æ¸¬è©¦é¡åˆ¥åˆ—è¡¨
    test_classes = [
        ("Prometheus æ”¶é›†å™¨", PrometheusCollectorPerformanceTest),
        ("Grafana é…ç½®ç®¡ç†å™¨", GrafanaConfigPerformanceTest),
        ("ç›£æ§ç³»çµ±", MonitorSystemPerformanceTest),
    ]
    
    all_results = {}
    
    for test_name, test_class in test_classes:
        print(f"ğŸ“Š åŸ·è¡Œ {test_name} æ•ˆèƒ½æ¸¬è©¦:")
        
        try:
            tester = test_class()
            test_methods = [method for method in dir(tester) 
                          if method.startswith('test_') and callable(getattr(tester, method))]
            
            class_results = {}
            
            for method_name in test_methods:
                method = getattr(tester, method_name)
                print(f"  ğŸ” {method_name}...")
                
                try:
                    result = method()
                    class_results[method_name] = result
                    
                    # é¡¯ç¤ºé—œéµæŒ‡æ¨™
                    if 'time_ms' in str(result):
                        for key, value in result.items():
                            if 'time_ms' in key and isinstance(value, (int, float)):
                                print(f"    â±ï¸  {key}: {value:.2f} ms")
                            elif 'memory_mb' in key and isinstance(value, (int, float)):
                                print(f"    ğŸ’¾ {key}: {value:.2f} MB")
                            elif 'throughput' in key and isinstance(value, (int, float)):
                                print(f"    ğŸš€ {key}: {value:.2f} RPS")
                    
                except Exception as e:
                    print(f"    âŒ æ¸¬è©¦å¤±æ•—: {e}")
                    class_results[method_name] = {"error": str(e)}
            
            all_results[test_name] = class_results
            
        except Exception as e:
            print(f"  âŒ {test_name} æ¸¬è©¦é¡åˆ¥åˆå§‹åŒ–å¤±æ•—: {e}")
            all_results[test_name] = {"error": str(e)}
        
        print()
    
    # ç”Ÿæˆæ•ˆèƒ½å ±å‘Š
    print("="*60)
    print("ğŸ“‹ æ•ˆèƒ½æ¸¬è©¦ç¸½çµå ±å‘Š")
    print("="*60)
    
    for test_name, results in all_results.items():
        print(f"\nğŸ”§ {test_name}:")
        
        if "error" in results:
            print(f"  âŒ æ¸¬è©¦å¤±æ•—: {results['error']}")
            continue
        
        for method_name, result in results.items():
            if "error" in result:
                print(f"  âŒ {method_name}: {result['error']}")
            else:
                # æª¢æŸ¥æ˜¯å¦é”åˆ°æ•ˆèƒ½ç›®æ¨™
                passed_checks = []
                failed_checks = []
                
                for key, value in result.items():
                    if key.endswith('_ok') and isinstance(value, bool):
                        if value:
                            passed_checks.append(key.replace('_ok', ''))
                        else:
                            failed_checks.append(key.replace('_ok', ''))
                
                status = "âœ…" if not failed_checks else "âš ï¸"
                print(f"  {status} {method_name}")
                
                if passed_checks:
                    print(f"    âœ… é€šé: {', '.join(passed_checks)}")
                if failed_checks:
                    print(f"    âŒ æœªé”æ¨™: {', '.join(failed_checks)}")
    
    return all_results


if __name__ == "__main__":
    results = run_performance_tests()
    
    # å„²å­˜çµæœåˆ°æª”æ¡ˆ
    import json
    with open("performance_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æ•ˆèƒ½æ¸¬è©¦çµæœå·²å„²å­˜åˆ° performance_results.json")
    print("ğŸ¯ æ•ˆèƒ½åŸºæº–æ¸¬è©¦å®Œæˆï¼")
