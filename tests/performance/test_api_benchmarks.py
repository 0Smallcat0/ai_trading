"""
API æ•ˆèƒ½åŸºæº–æ¸¬è©¦

æ­¤æ¨¡çµ„æ¸¬è©¦é—œéµ API ç«¯é»çš„æ•ˆèƒ½æ˜¯å¦ç¬¦åˆé è¨­åŸºæº–ã€‚
"""

import pytest
import time
from typing import Dict, Any
from fastapi.testclient import TestClient

from tests.performance.benchmark_config import API_BENCHMARKS


class TestAPIBenchmarks:
    """API æ•ˆèƒ½åŸºæº–æ¸¬è©¦é¡"""

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_health_check_benchmark(
        self, test_client: TestClient, benchmark, benchmark_validator
    ):
        """æ¸¬è©¦å¥åº·æª¢æŸ¥ç«¯é»æ•ˆèƒ½åŸºæº–"""

        def health_check_request():
            return test_client.get("/health")

        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        result = benchmark(health_check_request)

        # é©—è­‰æ•ˆèƒ½
        actual_time_ms = result.stats.mean * 1000  # è½‰æ›ç‚ºæ¯«ç§’
        validation = benchmark_validator("/health", "GET", actual_time_ms)

        # æ–·è¨€æ•ˆèƒ½ç¬¦åˆåŸºæº–
        assert validation["is_acceptable"], (
            f"å¥åº·æª¢æŸ¥ç«¯é»æ•ˆèƒ½ä¸ç¬¦åˆåŸºæº–: "
            f"{actual_time_ms:.2f}ms > {validation['benchmark']['max_response_time_ms']}ms"
        )

        print(f"âœ… å¥åº·æª¢æŸ¥æ•ˆèƒ½: {actual_time_ms:.2f}ms ({validation['status']})")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_auth_login_benchmark(
        self, test_client: TestClient, benchmark, benchmark_validator
    ):
        """æ¸¬è©¦ç™»å…¥ç«¯é»æ•ˆèƒ½åŸºæº–"""

        def login_request():
            return test_client.post(
                "/api/v1/auth/login", json={"username": "admin", "password": "admin123"}
            )

        result = benchmark(login_request)
        actual_time_ms = result.stats.mean * 1000
        validation = benchmark_validator("/api/v1/auth/login", "POST", actual_time_ms)

        assert validation["is_acceptable"], (
            f"ç™»å…¥ç«¯é»æ•ˆèƒ½ä¸ç¬¦åˆåŸºæº–: "
            f"{actual_time_ms:.2f}ms > {validation['benchmark']['max_response_time_ms']}ms"
        )

        print(f"âœ… ç™»å…¥æ•ˆèƒ½: {actual_time_ms:.2f}ms ({validation['status']})")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_data_list_benchmark(
        self,
        test_client: TestClient,
        auth_headers: Dict[str, str],
        benchmark,
        benchmark_validator,
    ):
        """æ¸¬è©¦æ•¸æ“šåˆ—è¡¨ç«¯é»æ•ˆèƒ½åŸºæº–"""

        def data_list_request():
            return test_client.get("/api/v1/data/", headers=auth_headers)

        result = benchmark(data_list_request)
        actual_time_ms = result.stats.mean * 1000
        validation = benchmark_validator("/api/v1/data/", "GET", actual_time_ms)

        # ç”±æ–¼å¯èƒ½æœ‰é€Ÿç‡é™åˆ¶ï¼Œæˆ‘å€‘æ”¾å¯¬æª¢æŸ¥æ¢ä»¶
        if validation["actual_time_ms"] > 1000:  # å¦‚æœè¶…é 1 ç§’ï¼Œå¯èƒ½æ˜¯é€Ÿç‡é™åˆ¶
            pytest.skip("è·³éæ¸¬è©¦ï¼Œå¯èƒ½å—åˆ°é€Ÿç‡é™åˆ¶å½±éŸ¿")

        assert validation["is_acceptable"], (
            f"æ•¸æ“šåˆ—è¡¨ç«¯é»æ•ˆèƒ½ä¸ç¬¦åˆåŸºæº–: "
            f"{actual_time_ms:.2f}ms > {validation['benchmark']['max_response_time_ms']}ms"
        )

        print(f"âœ… æ•¸æ“šåˆ—è¡¨æ•ˆèƒ½: {actual_time_ms:.2f}ms ({validation['status']})")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_strategies_list_benchmark(
        self,
        test_client: TestClient,
        auth_headers: Dict[str, str],
        benchmark,
        benchmark_validator,
    ):
        """æ¸¬è©¦ç­–ç•¥åˆ—è¡¨ç«¯é»æ•ˆèƒ½åŸºæº–"""

        def strategies_list_request():
            return test_client.get("/api/v1/strategies/", headers=auth_headers)

        result = benchmark(strategies_list_request)
        actual_time_ms = result.stats.mean * 1000
        validation = benchmark_validator("/api/v1/strategies/", "GET", actual_time_ms)

        if validation["actual_time_ms"] > 1000:
            pytest.skip("è·³éæ¸¬è©¦ï¼Œå¯èƒ½å—åˆ°é€Ÿç‡é™åˆ¶å½±éŸ¿")

        assert validation["is_acceptable"], (
            f"ç­–ç•¥åˆ—è¡¨ç«¯é»æ•ˆèƒ½ä¸ç¬¦åˆåŸºæº–: "
            f"{actual_time_ms:.2f}ms > {validation['benchmark']['max_response_time_ms']}ms"
        )

        print(f"âœ… ç­–ç•¥åˆ—è¡¨æ•ˆèƒ½: {actual_time_ms:.2f}ms ({validation['status']})")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_models_list_benchmark(
        self,
        test_client: TestClient,
        auth_headers: Dict[str, str],
        benchmark,
        benchmark_validator,
    ):
        """æ¸¬è©¦æ¨¡å‹åˆ—è¡¨ç«¯é»æ•ˆèƒ½åŸºæº–"""

        def models_list_request():
            return test_client.get("/api/v1/models/", headers=auth_headers)

        result = benchmark(models_list_request)
        actual_time_ms = result.stats.mean * 1000
        validation = benchmark_validator("/api/v1/models/", "GET", actual_time_ms)

        if validation["actual_time_ms"] > 1000:
            pytest.skip("è·³éæ¸¬è©¦ï¼Œå¯èƒ½å—åˆ°é€Ÿç‡é™åˆ¶å½±éŸ¿")

        assert validation["is_acceptable"], (
            f"æ¨¡å‹åˆ—è¡¨ç«¯é»æ•ˆèƒ½ä¸ç¬¦åˆåŸºæº–: "
            f"{actual_time_ms:.2f}ms > {validation['benchmark']['max_response_time_ms']}ms"
        )

        print(f"âœ… æ¨¡å‹åˆ—è¡¨æ•ˆèƒ½: {actual_time_ms:.2f}ms ({validation['status']})")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_portfolio_overview_benchmark(
        self,
        test_client: TestClient,
        auth_headers: Dict[str, str],
        benchmark,
        benchmark_validator,
    ):
        """æ¸¬è©¦æŠ•è³‡çµ„åˆæ¦‚è¦½ç«¯é»æ•ˆèƒ½åŸºæº–"""

        def portfolio_overview_request():
            return test_client.get("/api/v1/portfolio/", headers=auth_headers)

        result = benchmark(portfolio_overview_request)
        actual_time_ms = result.stats.mean * 1000
        validation = benchmark_validator("/api/v1/portfolio/", "GET", actual_time_ms)

        if validation["actual_time_ms"] > 1000:
            pytest.skip("è·³éæ¸¬è©¦ï¼Œå¯èƒ½å—åˆ°é€Ÿç‡é™åˆ¶å½±éŸ¿")

        assert validation["is_acceptable"], (
            f"æŠ•è³‡çµ„åˆæ¦‚è¦½ç«¯é»æ•ˆèƒ½ä¸ç¬¦åˆåŸºæº–: "
            f"{actual_time_ms:.2f}ms > {validation['benchmark']['max_response_time_ms']}ms"
        )

        print(f"âœ… æŠ•è³‡çµ„åˆæ¦‚è¦½æ•ˆèƒ½: {actual_time_ms:.2f}ms ({validation['status']})")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_comprehensive_api_benchmarks(
        self, test_client: TestClient, auth_headers: Dict[str, str], benchmark_validator
    ):
        """ç¶œåˆ API æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""

        # å®šç¾©è¦æ¸¬è©¦çš„ç«¯é»
        test_endpoints = [
            ("/health", "GET", None),
            (
                "/api/v1/auth/login",
                "POST",
                {"username": "admin", "password": "admin123"},
            ),
            ("/api/v1/data/", "GET", auth_headers),
            ("/api/v1/strategies/", "GET", auth_headers),
            ("/api/v1/models/", "GET", auth_headers),
            ("/api/v1/portfolio/", "GET", auth_headers),
        ]

        results = []

        for endpoint, method, headers_or_data in test_endpoints:
            try:
                start_time = time.perf_counter()

                if method == "GET":
                    response = test_client.get(endpoint, headers=headers_or_data)
                elif method == "POST":
                    if endpoint == "/api/v1/auth/login":
                        response = test_client.post(endpoint, json=headers_or_data)
                    else:
                        response = test_client.post(endpoint, headers=headers_or_data)

                end_time = time.perf_counter()
                actual_time_ms = (end_time - start_time) * 1000

                # è·³éé€Ÿç‡é™åˆ¶çš„éŸ¿æ‡‰
                if response.status_code == 429:
                    continue

                validation = benchmark_validator(endpoint, method, actual_time_ms)
                results.append(validation)

                print(
                    f"ğŸ“Š {endpoint} ({method}): {actual_time_ms:.2f}ms ({validation['status']})"
                )

            except Exception as e:
                print(f"âŒ {endpoint} ({method}): æ¸¬è©¦å¤±æ•— - {e}")
                continue

        # çµ±è¨ˆçµæœ
        if results:
            total_tests = len(results)
            passed_tests = sum(1 for r in results if r["is_acceptable"])
            pass_rate = (passed_tests / total_tests) * 100

            print(f"\nğŸ“ˆ ç¶œåˆæ•ˆèƒ½æ¸¬è©¦çµæœ:")
            print(f"   ç¸½æ¸¬è©¦æ•¸: {total_tests}")
            print(f"   é€šéæ¸¬è©¦: {passed_tests}")
            print(f"   é€šéç‡: {pass_rate:.1f}%")

            # å¦‚æœé€šéç‡ä½æ–¼ 80%ï¼Œç™¼å‡ºè­¦å‘Šä½†ä¸å¤±æ•—
            if pass_rate < 80:
                print(f"âš ï¸ æ•ˆèƒ½é€šéç‡ ({pass_rate:.1f}%) ä½æ–¼å»ºè­°å€¼ (80%)")
            else:
                print(f"âœ… æ•ˆèƒ½é€šéç‡ç¬¦åˆè¦æ±‚")
        else:
            pytest.skip("æ‰€æœ‰æ¸¬è©¦éƒ½è¢«è·³éï¼Œå¯èƒ½å—åˆ°é€Ÿç‡é™åˆ¶å½±éŸ¿")

    @pytest.mark.benchmark
    @pytest.mark.performance
    def test_performance_regression_check(
        self, test_client: TestClient, benchmark_validator
    ):
        """æ•ˆèƒ½å›æ­¸æª¢æŸ¥"""

        # æ¸¬è©¦é—œéµç«¯é»çš„æ•ˆèƒ½å›æ­¸
        critical_endpoints = [
            ("/health", "GET"),
        ]

        for endpoint, method in critical_endpoints:
            start_time = time.perf_counter()

            if method == "GET":
                response = test_client.get(endpoint)

            end_time = time.perf_counter()
            actual_time_ms = (end_time - start_time) * 1000

            validation = benchmark_validator(endpoint, method, actual_time_ms)

            # å°æ–¼é—œéµç«¯é»ï¼Œæˆ‘å€‘è¦æ±‚æ›´åš´æ ¼çš„æ•ˆèƒ½æ¨™æº–
            assert validation["performance_ratio"] <= 1.5, (
                f"é—œéµç«¯é» {endpoint} æ•ˆèƒ½å›æ­¸: "
                f"å¯¦éš›æ™‚é–“ {actual_time_ms:.2f}ms è¶…éåŸºæº– 50%"
            )

            print(f"âœ… {endpoint} æ•ˆèƒ½å›æ­¸æª¢æŸ¥é€šé: {actual_time_ms:.2f}ms")
